{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GAN Strcuture (GRU)\n"
      ],
      "metadata": {
        "id": "20rMhzbyCODl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install torchtext==0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0KWc4m-A-2z",
        "outputId": "bc262e8f-5f99-40d9-ea60-18930cbde812"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp39-cp39-linux_x86_64.whl (1982.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m645.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0+cu111) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0+cu111) (1.22.4)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.8.0+cu111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.9\n",
            "  Downloading torchtext-0.9.0-cp39-cp39-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (4.65.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (1.8.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0->torchtext==0.9) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (1.26.15)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "Successfully installed torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "C8R9vyY0CNfu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchtext\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import json\n",
        "import ast\n",
        "import glob\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "from torchtext.legacy import data\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,hidden_size,n_layers=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.emb = torch.eye(vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.GRU(vocab_size,hidden_size,batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size,50)\n",
        "        self.fc2 = nn.Linear(50,1)\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        \n",
        "        x = self.emb[x]\n",
        "        out, hidden = self.rnn(x,hidden)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,hidden_size,n_layers=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.emb = torch.eye(vocab_size)\n",
        "        self.rnn = nn.GRU(vocab_size,hidden_size, n_layers,batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        x = self.emb[x]\n",
        "        out, hidden = self.rnn(x,hidden)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous Training (Example From Tutorial)\n"
      ],
      "metadata": {
        "id": "ZazC4fmCCSsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train (trainDS,G,D,lr=0.002,batch_size=1,num_epochs=10):\n",
        "  d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "  g_optimizer = optim.Adam(G.parameters(), lr)\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  train_data = trainDS\n",
        "  #train_loader = train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  samples = []\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    D.train()\n",
        "    G.train()\n",
        "\n",
        "    for batch_i in range(len(train_data)):\n",
        "   # for batch_i,  real_recipes,ingredients) in enumerate(train_data['directions'],train_data['ingredients']):\n",
        "            real_recipes = train_data['directions'][batch_i]\n",
        "            ingredients = train_data['ingredients'][batch_i]\n",
        "\n",
        "            # batch_size = real_recipes.size(0)\n",
        "\n",
        "            # === Train the Discriminator ===\n",
        "            \n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # discriminator losses on real images \n",
        "          \n",
        "            D_real = D(real_recipes)\n",
        "            labels = torch.ones(batch_size)\n",
        "\n",
        "           \n",
        "            D_real= sum(D_real)/D_real.shape[0]\n",
        "            d_real_loss = criterion(D_real, labels)\n",
        "\n",
        "            \n",
        "            # discriminator losses on fake images\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "            \n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = G(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "            D_fake = D(fake_recipes)\n",
        "            \n",
        "            #labels = torch.zeros(batch_size) # fake labels = 0\n",
        "            labelsD = torch.zeros(1)\n",
        "            labelsD = torch.diag(labelsD,0)\n",
        "            d_fake_loss = criterion(D_fake, labelsD)\n",
        "            \n",
        "            # add up losses and update parameters\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "\n",
        "             # === Train the Generator ===\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # generator losses on fake images\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "\n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = G(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "\n",
        "            D_fake = D(fake_recipes)\n",
        "            #labels = torch.ones(batch_size) #flipped labels\n",
        "\n",
        "\n",
        "            labels = torch.ones(batch_size)\n",
        "            # compute loss and update parameters\n",
        "            g_loss = criterion(D_fake,labels)\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "                % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # append discriminator loss and generator loss\n",
        "            losses.append((d_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "vrz3-cmNCXMP"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Training"
      ],
      "metadata": {
        "id": "eX9T17Z9vvbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r2Ujs4rAaMt",
        "outputId": "41974933-795a-44d7-cff0-bf2ce5057040"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting HyperParameters"
      ],
      "metadata": {
        "id": "VvxXBAhGv6hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4\n",
        "batch_size = 12\n",
        "num_epochs = 2\n"
      ],
      "metadata": {
        "id": "Qr43CaQpv15Y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting Dataset (From Baseline)"
      ],
      "metadata": {
        "id": "4rATo3vpd0bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [r'\\'', r'\\\"', r'\\.', r'<br \\/>', r',', r'\\(', r'\\)', r'\\!', r'\\?', r'\\:', r'\\s+']\n",
        "replacements = [' \\'  ', '', ' . ', ' ', ' , ', ' ( ', ' ) ', ' ! ', ' ? ', ' ', ' ']\n",
        "patterns_dict = list((re.compile(p), r) for p, r in zip(patterns, replacements))\n",
        "\n",
        "def basic_english_normalize(line):\n",
        "    line = line.lower()\n",
        "    for pattern_re, replaced_str in patterns_dict:\n",
        "        line = pattern_re.sub(replaced_str, line)\n",
        "    return line.split()\n",
        "\n",
        "directions_field = torchtext.legacy.data.Field(sequential=True,\n",
        "                                  tokenize=basic_english_normalize,\n",
        "                                  include_lengths=True,\n",
        "                                  batch_first=False,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\")\n",
        "\n",
        "fields = [('directions', directions_field), ('ingredients', None)]\n",
        "baseline_data = torchtext.legacy.data.TabularDataset(\"drive/MyDrive/data/char_nlg_df.csv\", \"csv\", fields)\n",
        "\n",
        "directions_field.build_vocab(baseline_data)\n",
        "directions_field.vocab = torchtext.vocab.Vocab(directions_field.vocab.freqs, specials=['<unk>','<pad>', '<BOS>', '<EOS>', ';'])\n",
        "baseline_vocab_stoi = directions_field.vocab.stoi\n",
        "baseline_vocab_itos = directions_field.vocab.itos\n",
        "baseline_vocab_size = len(directions_field.vocab.itos)\n",
        "print(baseline_vocab_size)\n",
        "print(directions_field.vocab.itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDU7SvPPAdYa",
        "outputId": "04b1eec2-de0e-4cb6-a36e-f8567490497e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3502\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ';', 'and', 'in', 'to', 'add', 'with', 'minutes', 'until', 'a', 'mix', 'for', '1', 'the', 'bake', 'at', 'of', 'into', '2', 'on', 'ingredients', 'sugar', 'over', 'or', 'pour', 'stir', 'pan', 'mixture', '350', 'well', 'cook', 'butter', 'cheese', 'heat', 'water', 'together', 'top', 'inch', 'combine', 'salt', 'flour', 'cream', 'all', 'place', 'oven', 'bowl', 'brown', 'cool', 'milk', 'beat', 'baking', 'x', '4', '3', 'chicken', 'eggs', 'cover', 'put', 'onion', 'about', 'dish', 'sprinkle', 'pepper', 'then', 'cup', 'large', '9', 'serve', '10', 'cut', 'greased', 'drain', '30', 'remaining', 'is', 'remove', 'hours', 'boil', 'from', 'sauce', 'let', 'spread', 'hour', 'egg', 'vanilla', '8', '5', 'cake', 'oil', 'makes', 'set', '6', 'hot', '13', '20', 'nuts', '15', 'meat', 'margarine', 'dry', 'simmer', 'roll', 'stirring', 'casserole', 'medium', 'chill', 'each', 'blend', 'soup', 'juice', 'tender', 'pie', 'melt', 'layer', 'potatoes', 'skillet', 'sheet', 'if', 'small', '45', 'refrigerate', 'melted', 'it', 'onions', 'before', 'cookie', 'out', 'saucepan', 'are', 'bread', 'as', 'garlic', 'be', 'powder', 'bring', 'half', 'smooth', 'cups', 'dough', 'rice', 'crumbs', 'low', 'servings', 'beef', 'bottom', '375', 'can', 'chocolate', 'aside', 'make', '12', 'serving', 'spoon', 'one', 'whip', 'slice', 'pineapple', 'boiling', 'lightly', 'soda', 'not', 'first', '25', 'pieces', 'done', 'serves', 'fold', 'when', '325', 'time', 'quart', 'crust', 'up', 'preheat', 'paper', 'grease', 'use', 'batter', 'lemon', 'you', 'chips', 'refrigerator', 'sour', 'other', 'tomatoes', 'vegetables', 'bacon', 'sift', 'saute', 'this', '40', 'by', 'cinnamon', 'high', 'thoroughly', 'corn', 'green', 'celery', 'desired', 'overnight', 'balls', '35', 'mixing', 'stand', 'warm', 'shortening', 'onto', '400', 'tomato', 'do', 'thick', 'beans', 'golden', 'will', 'an', 'chopped', 'press', 'slices', 'turn', 'crackers', 'pudding', 'vinegar', 'floured', 'more', 'drop', 'broccoli', 'off', 'pans', 'except', 'after', 'constantly', 'microwave', 'broth', 'just', 'loaf', 'foil', 'gradually', 'uncovered', 'pot', 'fat', 'pecans', 'cold', 'covered', 'noodles', 'blended', 'coconut', 'fruit', 'may', 'cooked', 'fry', 'center', 'very', 'mushrooms', 'package', 'liquid', 'reduce', 'tablespoons', 'apples', 'dissolve', 'browned', 'shape', 'size', 'whipped', 'according', 'shell', 'toss', 'while', 'garnish', 'parsley', 'teaspoon', 'directions', 'firm', 'ground', 'ice', 'minute', 'slightly', 'syrup', 'powdered', 'taste', 'carrots', 'cooking', 'filling', 'rest', 'sides', 'little', 'mixer', 'evenly', '50', 'dip', 'dressing', 'enough', 'least', 'whites', 'side', 'oleo', 'peel', 'buttered', 'peanut', 'ball', 'blender', 'form', '7', 'mayonnaise', 'layers', 'slowly', 'occasionally', 'fork', 'mustard', 'salad', 'speed', 'peppers', 'plate', 'ungreased', 'arrange', 'light', 'prepare', 'keep', 'longer', 'approximately', 'container', 'jello', 'mixed', 'sausage', 'coat', 'next', 'spray', 'through', 'wrap', 'cabbage', 'your', 'orange', 'soft', 'yields', 'cookies', 'cornstarch', 'marshmallows', 'topping', 'fluffy', 'raisins', 'ready', 'several', 'using', 'squares', 'cracker', 'beaten', 'beating', 'cocoa', 'tablespoon', 'crushed', 'muffin', 'store', 'take', 'clean', 'thin', 'but', 'completely', 'fill', 'good', 'rolls', 'spaghetti', 'them', 'thickened', 'two', 'chop', 'freeze', 'yolks', 'macaroni', 'shallow', 'that', 'wax', 'buttermilk', 'tube', 'yeast', 'down', 'gently', 'graham', 'hamburger', 'heavy', 'once', 'bananas', 'comes', 'dozen', 'few', 'gelatin', 'bubbly', 'double', 'grated', 'separate', 'white', 'last', 'stuffing', '425', 'another', 'inches', 'rack', 'seasoning', 'stiff', 'glass', 'directed', 'temperature', 'alternately', 'both', 'electric', 'immediately', 'strawberries', 'has', 'parmesan', 'repeat', 'return', 'bag', 'ham', 'lettuce', 'line', 'frozen', 'sliced', 'strips', 'cubes', 'fish', 'season', '300', 'continue', 'jell', 'rise', 'seasonings', 'spices', 'creamed', 'paprika', 'square', 'ginger', 'pork', 'brush', 'dissolved', 'drained', 'food', 'have', 'wash', 'waxed', 'heated', 'o', 'pat', 'soy', 'knife', '450', 'like', 'again', 'break', 'cherries', 'crisp', 'adding', 'chops', 'creamy', 'plastic', 's', 'been', 'box', 'bundt', 'cooled', 'room', 'seal', 'shake', '24', 'apple', 'around', 'walnuts', 'baked', 'nutmeg', 'shells', 'so', '60', 'addition', 'chili', 'deep', 'melts', 'mushroom', 'preheated', 'stick', 'also', 'biscuits', 'crumble', 'full', 'pasta', 'i', 'jars', 'pumpkin', 'seed', 'should', 'shrimp', 'yield', 'allow', 'apart', 'olives', 'peaches', 'any', 'oranges', 'pastry', 'vegetable', 'boiler', 'coated', 'knead', 'lined', 'rinse', 'steak', 'used', 'bouillon', 'crisco', 'jar', 'some', 'amount', 'cheddar', 'drizzle', 'freezer', 'fresh', 'unbaked', 'worcestershire', 'zucchini', 'confectioners', 'mash', 'oats', 'punch', 'round', 'sheets', 'wire', 'degrees', 'marinate', 'squash', 'bars', 'crock', 'hands', 'they', 'additional', 'flakes', 'hand', 'jelly', 'lay', 'mold', 'reserve', 'spinach', 'wine', '55', 'edges', 'nonstick', 'potato', 'prepared', 'sugars', 'toothpick', 'bell', 'cereal', 'day', 'divide', 'measure', 'needed', 'paste', 'peas', 'pound', 'skin', 'tortilla', '11', 'consistency', 'leaf', 'olive', 'roast', 'shredded', 'back', 'pizza', 'red', 'soften', 'soups', 'whisk', 'bite', 'cucumbers', 'moistened', 'process', 'recipe', 'times', 'too', 'between', 'board', 'breasts', 'lid', 'teaspoons', 'whole', 'bay', 'candy', 'inserted', 'meal', 'order', 'oregano', 'sure', 'turkey', 'almonds', 'cherry', 'crush', 'gravy', 'honey', 'leave', 'leaves', 'peanuts', 'soak', 'thicken', 'cheeses', 'chunks', 'clear', 'dice', 'extract', 'hard', 'no', 'pam', 'stove', 'frying', 'oatmeal', 'piece', 'salmon', 't', 'tightly', 'added', 'cottage', 'fine', 'frequently', 'long', 'reserved', 'save', 'surface', 'sweet', 'turning', 'calories', 'chilled', 'days', 'dutch', 'flavoring', 'halves', 'icing', 'juices', 'softened', 'ale', 'cans', 'cutting', 'frost', 'glaze', 'need', 'often', 'rind', 'salted', 'still', 'thickens', 'transfer', 'which', 'yogurt', '16', 'breast', 'catsup', 'crumbly', 'drippings', 'flatten', 'mozzarella', 'seconds', 'served', 'three', 'tortillas', '250', 'airtight', 'bone', 'ending', 'enjoy', 'forms', 'frosting', 'great', 'removing', 'sprayed', 'condensed', 'excess', 'per', 'taco', 'thickness', '18', 'berries', 'biscuit', 'broil', 'carefully', 'dill', 'dot', 'lengthwise', 'mashed', 'patties', 'processor', 'teaspoonfuls', 'evaporated', 'every', 'flat', 'foamy', 'marinade', 'moderate', 'quickly', 'salsa', 'slow', 'towel', 'wedges', 'applesauce', 'don', 'ends', 'made', 'necessary', 'steam', 'tartar', 'best', 'bits', 'cornmeal', 'cranberries', 'easy', 'grill', 'peaks', 'platter', 'quarts', 'remainder', 'tops', 'ahead', 'black', 'cloves', 'crescent', 'crumb', 'everything', 'loaves', 'muffins', 'oblong', 'only', 'open', 'pack', 'rings', 'uncover', 'under', 'almost', 'alternate', 'broiler', 'chilies', 'dates', 'kettle', 'sifted', 'spoonfuls', 'tuna', 'along', 'basil', 'cauliflower', 'combined', 'cornbread', 'edge', 'favorite', 'four', 'grate', 'ketchup', 'lukewarm', 'middle', 'pink', 'reserving', 'same', 'toast', 'towels', 'wafers', 'c', 'color', 'delicious', 'diced', 'end', 'freezes', 'french', 'italian', 'refrigerated', 'second', 'seeds', 'steaks', 'stems', 'almond', 'banana', 'barbecue', 'begins', 'better', 'chestnuts', 'crusts', 'cube', 'dust', 'even', 'finely', 'fruits', 'm', 'pyrex', 'rolling', 'scoop', 'sherbet', 'sit', 'thaw', 'weeks', 'above', 'cocktail', 'coffee', 'coloring', 'during', 'leaving', 'molasses', 'point', 'single', 'strawberry', 'want', 'wings', '14', 'bean', 'buns', 'cucumber', 'dash', 'eat', 'holes', 'lime', 'meanwhile', 'optional', 'ounce', 'pecan', 'preserves', 'rounded', 'strain', 'these', 'asparagus', 'big', 'blueberries', 'brand', 'curry', 'doritos', 'eagle', 'flavor', 'granulated', 'grind', 'iron', 'making', 'moist', 'puree', 'raw', 'ritz', 'stage', 'stuff', 'bubbles', 'etc', 'heaping', 'lower', 'morning', 'now', 'part', 'pears', 'pies', 'spatula', 'tins', 'trim', 'way', 'work', 'aluminum', 'beater', 'bisquick', 'boils', 'choice', 'cornflakes', 'five', 'get', 'greens', 'keeps', 'meatballs', 'oiled', 'partially', 'pepperoni', 'pimento', 'racks', 'seasoned', 'spring', 'tin', 'whipping', 'wooden', 'yellow', 'yolk', '0', '275', 'blending', 'cakes', 'come', 'entire', 'herbs', 'instant', 'measuring', 'moisten', 'prevent', 'real', 'sized', 'start', 'stock', 'tear', 'thyme', 'wet', 'avocado', 'baste', 'butterscotch', 'close', 'cranberry', 'crumbled', 'equal', 'extra', 'gallon', 'hole', 'less', 'marshmallow', 'much', 'pinch', 'poke', 'prick', 'rhubarb', 'roasting', 'sesame', 'sharp', 'sherry', 'thinly', 'tight', 'twice', 'without', 'across', 'away', 'bones', 'caramel', 'carrot', 'clams', 'core', 'cutter', 'dinner', 'discard', 'easily', 'finish', 'firmly', 'given', 'halfway', 'individual', 'inside', 'instead', 'invert', 'leftover', 'oysters', 'parts', 'pint', 'popcorn', 'possible', 'pull', 'pulp', 'quarters', 'ricotta', 'rub', 'shapes', 'shred', 'sterilized', 'strip', 'alternating', 'basting', 'canned', 'cayenne', 'cider', 'cooker', 'crab', 'croutons', 'cubed', 'debone', 'dissolves', 'doubled', 'fingers', 'flavors', 'following', 'gone', 'handle', 'its', 'logs', 'mandarin', 'meringue', 'paraffin', 'people', 'plain', 're', 'rectangle', 'skins', 'smaller', 'spice', 'stew', 'teaspoonful', 'than', 'toasted', 'veggies', 'velveeta', 'wheat', 'allspice', 'colored', 'custard', 'dream', 'dried', 'eggplant', 'empty', 'fillets', 'flavorings', 'generously', 'glasses', 'glazed', 'grits', 'jam', 'lard', 'listed', 'log', 'looks', 'minced', 'mixtures', 'morsels', 'night', 'plus', 'pounds', 'quick', 'ring', 'rye', 'sandwiches', 'spreading', 'squeeze', 'sticky', 'sweetened', 'swiss', 'tabasco', 'takes', 'transparent', 'uncooked', 'undrained', 'weed', 'wide', 'begin', 'circle', 'crabmeat', 'crack', 'cumin', 'dishes', 'dredge', 'drops', 'finger', 'fit', 'fried', 'g', 'gets', 'hold', 'leftovers', 'level', 'nut', 'overbake', 'poppy', 'regular', 'right', 'sauerkraut', 'scallops', 'scrape', 'setting', 'skim', 'unbeaten', '100', '36', 'absorbed', 'beer', 'beets', 'bit', 'brownies', 'coarse', 'couple', 'crosswise', 'dipped', 'filled', 'flake', 'hamburg', 'heating', 'karo', 'krispies', 'look', 'meats', 'miracle', 'note', 'overlapping', 'pancakes', 'pickles', 'quarter', 'ranch', 'rolled', 'rosemary', 'rounds', 'rum', 'run', 'simmering', 'slaw', 'split', 'sprouts', 'sticks', 'topped', 'walnut', 'wok', 'adjust', 'aid', 'american', 'being', 'blueberry', 'boiled', 'bourbon', 'browns', 'cheesecloth', 'cloth', 'coating', 'colander', 'cooling', 'covering', 'crispy', 'decorate', 'depending', 'dump', 'either', 'follow', 'frypan', 'fudge', 'grapes', 'hershey', 'kool', 'lemonade', 'lumpy', 'mini', 'oreos', 'ovenproof', 'overcook', 'packet', 'picante', 'pints', 'pop', 'portions', 'rectangular', 'sandwich', 'sauteed', 'scald', 'seam', 'separately', 'sprinkled', 'stem', 'stiffly', 'sweetener', 'tater', 'tea', 'tester', 'tests', 'thawed', 'there', 'tots', 'vigorously', '200', '22', '9x13', 'bed', 'beginning', 'bottoms', 'careful', 'chives', 'corned', 'cupcake', 'diameter', 'dogs', 'fashion', 'fire', 'fridge', 'fritos', 'grain', 'hens', 'later', 'liquids', 'loosen', 'lumps', 'mallet', 'mayo', 'miniature', 'must', 'near', 'outer', 'outside', 'own', 'power', 'ribs', 'ro', 'safe', 'sections', 'slicing', 'soon', 'spoonful', 'starting', 'starts', 'sticking', 'tbsp', 'tel', 'thermometer', 'tie', 'till', 'translucent', 'wait', 'watch', '48', '70', 'amounts', 'bags', 'bar', 'barely', 'bath', 'blade', 'bottle', 'bowls', 'broken', 'caramels', 'chipped', 'chuck', 'cilantro', 'clove', 'coals', 'coarsely', 'concentrate', 'containers', 'cutlets', 'diagonally', 'dropped', 'duty', 'enchilada', 'filo', 'griddle', 'harden', 'head', 'instructions', 'kidney', 'left', 'lift', 'maraschino', 'metal', 'molds', 'most', 'my', 'n', 'nearly', 'nice', 'oreo', 'packages', 'pancake', 'parboil', 'pickle', 'placing', 'portion', 'poultry', 'pressing', 'pretzels', 'reaches', 'refried', 'sage', 'scraping', 'sealed', 'skewers', 'slotted', 'substitute', 'such', 'tablespoonfuls', 'tastes', 'third', 'toothpicks', 'touch', 'unroll', 'wish', 'would', '17', '65', 'angel', 'apricot', 'artichoke', 'assorted', 'avocados', 'becomes', 'bran', 'breaking', 'briefly', 'browning', 'bubbling', 'bulk', 'check', 'chiles', 'chip', 'congeal', 'creme', 'crusty', 'dark', 'dessert', 'does', 'doneness', 'drink', 'excellent', 'fitting', 'frothy', 'gelatine', 'go', 'hearts', 'heath', 'herb', 'increase', 'limp', 'loses', 'main', 'min', 'mint', 'moderately', 'nicely', 'okra', 'old', 'pimentos', 'pin', 'pitcher', 'poured', 'prefer', 'puffed', 'push', 'relish', 'removed', 'rich', 'running', 'semi', 'six', 'slit', 'strainer', 'style', 'tarragon', 'test', 'their', 'thread', 'tortellini', 'tossing', 'tough', 'triangles', 'undiluted', 'upside', 'usually', 'v', 'wafer', 'week', 'whatever', 'whiz', '120', '32', 'air', 'always', 'among', 'baby', 'brandy', 'breadcrumbs', 'brushing', 'caps', 'centers', 'chafing', 'cheez', 'cholesterol', 'cob', 'contains', 'continually', 'cools', 'counter', 'cupcakes', 'degree', 'depression', 'dipping', 'disappears', 'distribute', 'doesn', 'door', 'finished', 'fully', 'gel', 'give', 'glossy', 'halved', 'hash', 'hominy', 'horseradish', 'including', 'keeping', 'kind', 'lamb', 'layering', 'lemons', 'liberally', 'lids', 'lot', 'many', 'marble', 'months', 'narrow', 'needs', 'omit', 'overmix', 'party', 'patty', 'pepperidge', 'pick', 'placed', 'plates', 'preparation', 'pretty', 'proof', 'qt', 'radishes', 'ragu', 'raisin', 'raspberries', 'roaster', 'rotary', 'saut', 'secure', 'slits', 'soaked', 'solid', 'stream', 'table', 'tapioca', 'texture', 'touched', 'try', 'turmeric', 'twinkies', 'veal', '130', '475', '75', '80', 'advance', 'alum', 'appetizer', 'atop', 'available', 'bagels', 'baker', 'bark', 'basket', 'beaters', 'become', 'below', 'burn', 'burner', 'caraway', 'cereals', 'children', 'chilling', 'combination', 'complete', 'crawfish', 'crepes', 'cuts', 'cutters', 'damp', 'diluted', 'directs', 'easier', 'elastic', 'extracts', 'farm', 'finally', 'find', 'flowers', 'foam', 'followed', 'forming', 'franks', 'free', 'frosted', 'fryer', 'funnel', 'generous', 'granules', 'jalapeno', 'kept', 'kiss', 'kraut', 'label', 'ladle', 'lasagna', 'liver', 'love', 'mango', 'maple', 'melting', 'mg', 'move', 'mushy', 'nectar', 'non', 'noodle', 'oat', 'ones', 'ounces', 'pare', 'parties', 'peach', 'peeled', 'pimientos', 'preferably', 'preference', 'preparing', 'prior', 'prunes', 'rectangles', 'reduced', 'resembles', 'roni', 'rotate', 'roux', 'rubber', 'scallions', 'sealing', 'seems', 'shallots', 'shelf', 'snack', 'souffle', 'spears', 'sprigs', 'springs', 'stack', 'stuffed', 'sunflower', 'swirl', 'tablespoonful', 'tamales', 'thicker', 'throughout', 'tinfoil', 'tiny', 'tray', 'tupperware', 'unmold', 'washed', 'wipe', 'working', 'wrapper', 'yams', '10x', '23', '370', 'absorb', 'altogether', 'anise', 'apply', 'avoid', 'base', 'bird', 'blot', 'blue', 'boned', 'brisket', 'brownie', 'bubble', 'burning', 'called', 'candies', 'canning', 'cast', 'chopper', 'christmas', 'circles', 'club', 'coats', 'coca', 'cola', 'contents', 'cooks', 'corners', 'cornflake', 'cracked', 'cross', 'crowd', 'crumbles', 'diagonal', 'different', 'dumplings', 'eaten', 'english', 'envelope', 'equals', 'evaporates', 'f', 'feeds', 'fettuccine', 'fingertips', 'fitted', 'fix', 'flavored', 'float', 'flute', 'folding', 'follows', 'formed', 'gallons', 'germ', 'grams', 'guacamole', 'halve', 'headspace', 'holds', 'hollow', 'icebox', 'important', 'itself', 'jack', 'kisses', 'knives', 'ladyfingers', 'larger', 'lawry', 'lentils', 'life', 'liners', 'marmalade', 'measured', 'microwavable', 'milnot', 'mixes', 'moisture', 'monterey', 'mugs', 'never', 'omitted', 'paint', 'peppercorns', 'pimiento', 'piping', 'points', 'position', 'pouring', 'pressure', 'pretzel', 'pulled', 'pulls', 'pureed', 'purpose', 'quantity', 'quite', 'raise', 'ramen', 'really', 'reheated', 'remains', 'rinsed', 'rises', 'rows', 'sauces', 'sausages', 'saving', 'scant', 'sear', 'section', 'sets', 'shoulder', 'sieve', 'simply', 'skewer', 'slide', 'snow', 'sort', 'soupy', 'special', 'spoons', 'sprinkling', 'squirrel', 'stalks', 'starter', 'stays', 'step', 'stewed', 'stone', 'stored', 'tart', 'thirds', 'throw', 'thumb', 'tint', 'total', 'turned', 'turns', 'twist', 'type', 'usual', 'wesson', 'what', 'works', 'wrapped', 'ziploc', 'ziti', '150', '240', '33', '46', '500', '64', '90', 'absorbent', 'against', 'al', 'alone', 'angle', 'anything', 'arugula', 'balance', 'batches', 'because', 'blackberries', 'blanch', 'blanched', 'blossoms', 'braise', 'breading', 'breads', 'breakfast', 'brine', 'bringing', 'brussels', 'bun', 'burgers', 'burritos', 'butterfly', 'capers', 'cashews', 'change', 'chex', 'chinese', 'chow', 'chunk', 'chutney', 'clam', 'closed', 'closely', 'clusters', 'congealed', 'could', 'countertop', 'couscous', 'cracklins', 'creaming', 'create', 'crepe', 'crimp', 'crockpot', 'crunch', 'crunchy', 'curdle', 'daily', 'de', 'deboned', 'decoration', 'decorative', 'deer', 'dente', 'depth', 'devein', 'diet', 'dippers', 'directly', 'dollop', 'early', 'eating', 'eight', 'elephant', 'enclose', 'everyone', 'exchange', 'family', 'fasten', 'feed', 'flaked', 'flaky', 'florets', 'flower', 'frankfurters', 'fun', 'garnished', 'gather', 'gentle', 'goes', 'grapefruit', 'grilled', 'grinder', 'halibut', 'hamburgers', 'handful', 'heatproof', 'holding', 'home', 'homemade', 'horizontally', 'how', 'hr', 'indefinitely', 'indention', 'ingredient', 'insert', 'insides', 'inverted', 'items', 'jackets', 'jellyroll', 'kahlua', 'kids', 'leeks', 'lengths', 'liquor', 'lite', 'loose', 'lots', 'machine', 'masher', 'mein', 'mill', 'milligrams', 'mine', 'mostaccioli', 'mound', 'mug', 'nacho', 'nine', 'number', 'nutmeats', 'overbeat', 'oyster', 'oz', 'packaged', 'palms', 'papers', 'particles', 'partly', 'pats', 'peek', 'peeling', 'penne', 'person', 'pet', 'pile', 'pinto', 'pit', 'pith', 'please', 'popped', 'pops', 'prebaked', 'preferred', 'pressed', 'procedure', 'puffs', 'puffy', 'pumpernickel', 'purchased', 'putting', 'rabbit', 'rapidly', 'release', 'remain', 'replace', 'requires', 'reynolds', 'rim', 'rinsing', 'ripe', 'roasted', 'romaine', 'rotel', 'rotini', 'row', 'runs', 'russian', 'saltwater', 'saucepot', 'saucer', 'scatter', 'scorching', 'seafood', 'seams', 'see', 'shaped', 'shows', 'sifting', 'slash', 'smoke', 'smoked', 'soaking', 'sodium', 'something', 'spiral', 'spooned', 'sprinkles', 'squeezing', 'standing', 'star', 'stay', 'steep', 'sterile', 'stirred', 'stockpot', 'stop', 'storage', 'strings', 'strokes', 'substituted', 'sun', 'surround', 'tails', 'taken', 'tall', 'tears', 'teriyaki', 'themselves', 'thickening', 'thinner', 'tip', 'tongs', 'toward', 'trays', 'triangle', 'trimmings', 'tsp', 'tuck', 'underside', 'upright', 'variety', 'various', 'vegies', 'vent', 'vermicelli', 'warmed', 'was', 'we', 'weather', 'whiskey', 'wieners', 'wild', 'wilted', 'within', 'yum', '112', '13x9', '1st', '234', '2nd', '365', '85', '86', '8ths', '9x9', 'able', 'accent', 'actually', 'age', 'alcohol', 'allowed', 'allowing', 'already', 'amaretto', 'amber', 'anchor', 'appear', 'appearance', 'approx', 'apricots', 'artichokes', 'artificial', 'average', 'awful', 'awhile', 'bakes', 'bakon', 'bamboo', 'basic', 'batch', 'beet', 'beverages', 'bigger', 'birds', 'birthday', 'blintzes', 'block', 'bombs', 'boneless', 'bottles', 'building', 'bundles', 'butterfingers', 'butterflied', 'button', 'buy', 'cajun', 'campbell', 'candied', 'candle', 'cantaloupe', 'carton', 'catalina', 'champagne', 'changes', 'checking', 'chestnut', 'chewy', 'child', 'chitterlings', 'chocolates', 'choose', 'chowder', 'chunked', 'church', 'circular', 'cleaned', 'clings', 'clumping', 'cluster', 'coke', 'cokes', 'cole', 'collards', 'colors', 'confectioner', 'consomme', 'containing', 'continuing', 'continuously', 'control', 'cores', 'corns', 'correct', 'country', 'covers', 'cracks', 'crank', 'crescents', 'criss', 'crumbling', 'crystals', 'd', 'dab', 'dampen', 'dashes', 'decorating', 'delicately', 'delightful', 'desire', 'deviled', 'dew', 'dilute', 'dirt', 'disappear', 'discarding', 'dividing', 'dog', 'doing', 'dome', 'dots', 'draft', 'draw', 'drinks', 'dropping', 'drying', 'dunk', 'eggbeater', 'elbow', 'emulsify', 'enchiladas', 'ensure', 'envelopes', 'equally', 'escape', 'espresso', 'evaporate', 'exactly', 'exchanges', 'extremely', 'fairly', 'faith', 'fall', 'fashioned', 'fast', 'fats', 'feta', 'field', 'figs', 'firms', 'fist', 'fits', 'flame', 'flank', 'flattened', 'flatter', 'floating', 'flours', 'flowerets', 'fluted', 'fondant', 'freezing', 'freshly', 'fries', 'future', 'game', 'garbanzo', 'garnishes', 'gas', 'genesis', 'giblets', 'gives', 'gloss', 'going', 'golf', 'gr', 'grains', 'grape', 'grocery', 'groundhog', 'group', 'gruyere', 'guests', 'gummy', 'had', 'haddock', 'handling', 'hang', 'hardened', 'having', 'help', 'helpings', 'helps', 'here', 'hi', 'holidays', 'hollowed', 'hors', 'hotter', 'human', 'hunger', 'incorporate', 'indirect', 'individually', 'intact', 'island', 'jelled', 'jicama', 'joy', 'juicy', 'kettles', 'kielbasa', 'kindly', 'kindness', 'kitchen', 'kiwi', 'kneading', 'knox', 'kraft', 'largest', 'lasagne', 'lastly', 'lasts', 'leak', 'lean', 'legs', 'letting', 'lighten', 'liking', 'lima', 'limburger', 'linguine', 'linguini', 'liqueur', 'll', 'loosely', 'lowest', 'luke', 'lunch', 'luncheon', 'mace', 'manufacturer', 'marjoram', 'marsala', 'maximum', 'maybe', 'melba', 'membranes', 'method', 'mexican', 'microcook', 'mince', 'mints', 'mocha', 'month', 'motion', 'mounds', 'mountain', 'mouth', 'mush', 'navy', 'neck', 'new', 'newspaper', 'nonfat', 'normally', 'oeuvre', 'oleomargarine', 'opaque', 'opening', 'option', 'original', 'orzo', 'oval', 'p', 'pale', 'palm', 'parchment', 'pattern', 'pea', 'peak', 'peaked', 'pear', 'pectin', 'perforations', 'perhaps', 'perk', 'persimmon', 'personal', 'pheasant', 'phyllo', 'pickling', 'pig', 'pineapples', 'pistachio', 'pizzas', 'pizzelle', 'pkg', 'pliable', 'plump', 'pocket', 'pointed', 'prawns', 'prayer', 'precooked', 'preheating', 'prep', 'probably', 'proceed', 'pronged', 'protein', 'provolone', 'puddings', 'puff', 'pulling', 'purple', 'quiche', 'raspberry', 'rather', 'realemon', 'regularly', 'reheat', 'remember', 'resemble', 'resistant', 'results', 'retain', 'reused', 'richer', 'rims', 'rivels', 'rock', 'roses', 'rotating', 'rotelle', 'runny', 'salads', 'saltine', 'salty', 'samuel', 'saran', 'satisfy', 'saved', 'savory', 'says', 'scalded', 'scissors', 'scoops', 'score', 'scramble', 'scrambled', 'scrub', 'seasons', 'securely', 'seem', 'select', 'self', 'separated', 'separates', 'service', 'settling', 'shaker', 'shaking', 'shears', 'shellfish', 'shiny', 'showers', 'shoyu', 'shut', 'similar', 'sink', 'sizzling', 'skillets', 'skinless', 'slab', 'slight', 'slush', 'smash', 'smearing', 'smile', 'smoothness', 'solids', 'solution', 'sooner', 'sorrow', 'spanish', 'spiced', 'spins', 'splashing', 'splenda', 'spoke', 'spots', 'spreadable', 'sprite', 'stands', 'starch', 'stars', 'steadily', 'steady', 'steamed', 'steel', 'sterilize', 'stiffen', 'stops', 'stretch', 'string', 'stuck', 'sugared', 'suit', 'supper', 'supreme', 'sweat', 'sweeten', 'sweeter', 'sympathy', 'syrupy', 'taffy', 'tail', 'taking', 'tan', 'tap', 'tapped', 'tasty', 'teflon', 'thickest', 'thighs', 'those', 'thousand', 'tied', 'tines', 'tips', 'tofu', 'toppings', 'tossed', 'tostitos', 'touching', 'toy', 'treat', 'trifle', 'truss', 'tvp', 'undercooked', 'unpeeled', 'unsweetened', 'unwrap', 'upon', 'uv', 'venison', 'vermouth', 'vessel', 'volume', 'waffles', 'wanted', 'warmer', 'watercress', 'wedge', 'whirl', 'width', 'winter', 'won', 'worms', 'wrappers', 'xxxx', 'yummy', 'zest', '01', '111', '118', '125', '12x3', '160', '175', '199', '21', '212', '225', '245', '249', '255', '26', '260', '265', '27', '270', '28', '2x15', '310', '315', '340', '344', '360', '37', '377', '405', '41', '43', '435', '44', '450f', '465', '4th', '4x', '520', '5th', '61', '633', '66', '6655', '68', '73', '800', '8x9', '93', '96', '9x1', '9x13x2', 'accompanied', 'accomplished', 'accordance', 'accordion', 'accumulate', 'accumulates', 'achieve', 'achiote', 'activate', 'active', 'acts', 'adds', 'advice', 'afford', 'afternoon', 'agent', 'alex', 'alfalfa', 'alongside', 'alot', 'alternated', 'altitude', 'ample', 'anchovies', 'anchovy', 'angling', 'angostura', 'animal', 'appears', 'appetizers', 'appropriate', 'aroma', 'arranging', 'arrowroot', 'assemble', 'assembled', 'attached', 'attachment', 'attractive', 'awareness', 'backbone', 'bagel', 'baguette', 'baller', 'bands', 'barbecued', 'barbecuing', 'baskets', 'bathroom', 'bathtub', 'battered', 'batters', 'beads', 'bearing', 'beau', 'beautiful', 'bends', 'bermuda', 'berry', 'beware', 'bias', 'bible', 'birthdays', 'bitter', 'bitterness', 'blackberry', 'blackened', 'blades', 'bland', 'blenders', 'bleu', 'bologna', 'bonnet', 'book', 'bother', 'boulder', 'bow', 'boxes', 'boys', 'braising', 'breaded', 'breadstick', 'brickle', 'bricks', 'brittle', 'broiled', 'broiling', 'brook', 'broths', 'brought', 'brownish', 'brunch', 'brushed', 'bucket', 'buckeyes', 'buds', 'burns', 'burrito', 'burst', 'butterfinger', 'buttering', 'buzz', 'cabin', 'cacao', 'california', 'call', 'caloric', 'camembert', 'camping', 'canadian', 'candles', 'canister', 'cap', 'carbohydrate', 'carbonated', 'care', 'carman', 'carve', 'casseroles', 'catch', 'cauliflowerets', 'cause', 'cavities', 'cavity', 'ceramic', 'certo', 'chair', 'changing', 'character', 'charbroil', 'charcoal', 'charred', 'cheerfulness', 'cheerios', 'cheesecake', 'chickens', 'chickpea', 'chickpeas', 'chilli', 'chillies', 'china', 'chippewa', 'chive', 'choc', 'choco', 'chopping', 'christian', 'chunkiness', 'cleaning', 'clergy', 'clip', 'clock', 'clothes', 'clump', 'clumps', 'cm', 'coal', 'cognac', 'coins', 'collins', 'combining', 'coming', 'commercial', 'community', 'companion', 'complement', 'completion', 'compotes', 'concentrated', 'cone', 'cones', 'confectionary', 'confetti', 'consider', 'consistently', 'consuming', 'content', 'continues', 'continuous', 'conventional', 'conventionally', 'cookware', 'copped', 'copper', 'corer', 'corkscrew', 'corner', 'cornsyrup', 'course', 'cow', 'crater', 'creamette', 'creamier', 'creating', 'creole', 'crinkle', 'crispies', 'crispix', 'crispness', 'croquette', 'crowded', 'crowds', 'crunchies', 'crystal', 'crystallized', 'cukes', 'cupful', 'cupfuls', 'curing', 'curls', 'currant', 'currants', 'curves', 'cutlet', 'dairy', 'damaged', 'dandelion', 'dannon', 'darker', 'date', 'decide', 'decorations', 'decrease', 'deeply', 'defrost', 'defrosted', 'dehydrated', 'delicate', 'deliciously', 'depends', 'derib', 'destem', 'develop', 'diabetic', 'diamond', 'difficult', 'dills', 'dime', 'disaster', 'disc', 'discretion', 'discussing', 'dishpan', 'disk', 'disregard', 'donut', 'doo', 'doubling', 'doughlike', 'doughnut', 'doughnuts', 'doughy', 'dozens', 'draining', 'dredging', 'dressings', 'dribbles', 'drip', 'dropper', 'drugstore', 'drumsticks', 'duck', 'duckling', 'due', 'dull', 'dusted', 'dye', 'e', 'ear', 'earth', 'eggnog', 'eggroll', 'elegant', 'element', 'eleven', 'else', 'emulsion', 'enamel', 'endive', 'enhances', 'entertaining', 'enthusiasm', 'environment', 'equipment', 'equivalent', 'ernest', 'escaping', 'especially', 'essence', 'evening', 'eventually', 'everyday', 'exception', 'exclude', 'exodus', 'expand', 'expected', 'experiment', 'explode', 'exposed', 'exterior', 'eye', 'eyed', 'eyes', 'face', 'faces', 'facilitate', 'facing', 'failed', 'falling', 'fan', 'fancy', 'farms', 'fastening', 'fatty', 'feeding', 'feel', 'feels', 'feet', 'fence', 'fettucine', 'fewer', 'fig', 'fillet', 'final', 'fingernail', 'firmer', 'flecks', 'flesh', 'flexible', 'flip', 'flounder', 'flowerpot', 'fluff', 'fluffed', 'foaming', 'folks', 'forcing', 'forever', 'forking', 'forks', 'formerly', 'fortunate', 'fourths', 'frascati', 'friend', 'fritter', 'frizzle', 'frogs', 'froth', 'fruitcake', 'fuel', 'fur', 'garbanzos', 'garnishing', 'gelatins', 'german', 'gift', 'gifts', 'gill', 'gin', 'gizzards', 'glaceed', 'gloves', 'glutamate', 'gm', 'god', 'gold', 'goose', 'gouda', 'goulash', 'graininess', 'grandkid', 'granola', 'gravel', 'gray', 'grenadine', 'grinding', 'gristle', 'grit', 'grudges', 'guiness', 'gumbo', 'hair', 'handled', 'handles', 'happy', 'harder', 'hare', 'hashbrown', 'havarti', 'he', 'heads', 'heart', 'heats', 'heaven', 'height', 'heights', 'held', 'helper', 'herbal', 'herbed', 'hers', 'hickory', 'hidden', 'hide', 'highball', 'highest', 'hinged', 'hit', 'ho', 'hock', 'holders', 'hollows', 'hook', 'horns', 'hotdogs', 'hots', 'house', 'humid', 'hungry', 'hurry', 'husband', 'husks', 'iffin', 'immerse', 'impossible', 'imprint', 'improvise', 'included', 'incorporated', 'incorporating', 'increased', 'indent', 'indentation', 'indicates', 'inexpensive', 'inject', 'inserting', 'interesting', 'intervals', 'isaiah', 'islands', 'item', 'iz', 'jambalaya', 'jellos', 'jells', 'jiggle', 'jiggly', 'job', 'joe', 'joint', 'judges', 'judgment', 'julienne', 'k', 'kabob', 'kabobs', 'kalamata', 'kale', 'kay', 'kernel', 'kernels', 'kinda', 'kinds', 'king', 'kings', 'kitten', 'knitting', 'knock', 'know', 'knows', 'knuckles', 'kosher', 'krisp', 'krrrrisp', 'lack', 'lattice', 'laughter', 'lavender', 'layed', 'layered', 'laying', 'leek', 'leg', 'length', 'letters', 'lg', 'lifted', 'lifting', 'lighter', 'limeade', 'limes', 'linen', 'lines', 'lingonberries', 'lining', 'links', 'liquidiser', 'liquidy', 'liquified', 'liquify', 'liter', 'liters', 'living', 'loan', 'longhorn', 'loosens', 'lose', 'lost', 'loved', 'lowered', 'lump', 'lumping', 'luster', 'lux', 'luxury', 'macadamia', 'maker', 'malted', 'mangoes', 'manwich', 'marbleize', 'margaritas', 'margin', 'marinates', 'marinating', 'market', 'mascarpone', 'maseca', 'mashing', 'mason', 'mat', 'mazola', 'mcbutter', 'meals', 'meatball', 'meaty', 'melon', 'membrane', 'men', 'menthe', 'menu', 'mess', 'messy', 'methods', 'mexicorn', 'micromelt', 'microwaved', 'midway', 'might', 'mike', 'mil', 'milium', 'millet', 'mingle', 'minimum', 'mins', 'mintues', 'minuets', 'mistake', 'mmmm', 'mmmmmmm', 'moderation', 'molly', 'mom', 'moments', 'monde', 'monosodium', 'monster', 'moon', 'mornings', 'mostly', 'mother', 'motor', 'mounding', 'mousse', 'mullet', 'multipurpose', 'mussels', 'nachos', 'nan', 'napkin', 'napkins', 'natural', 'nature', 'needle', 'nest', 'nestle', 'nests', 'ninths', 'nog', 'nongreased', 'northern', 'nose', 'nowadays', 'nutritious', 'object', 'observing', 'occasions', 'okay', 'omelet', 'omitting', 'opposite', 'optionals', 'ordinary', 'others', 'otherwise', 'our', 'ourselves', 'outdoor', 'overcooking', 'overlap', 'owens', 'packaging', 'packed', 'panboil', 'papered', 'parfait', 'parkay', 'pass', 'passed', 'past', 'patch', 'patout', 'paul', 'pearl', 'pebbles', 'peeking', 'peeler', 'penny', 'peppermint', 'pepsi', 'percolator', 'perfect', 'period', 'periodically', 'permeates', 'persimmons', 'philadelphia', 'picket', 'picks', 'pilaf', 'piles', 'piling', 'pillsbury', 'pipe', 'pizzelles', 'plant', 'play', 'pleaser', 'pleasers', 'pleasingly', 'plenty', 'pockets', 'pods', 'polenta', 'popper', 'pore', 'possibly', 'potluck', 'potlucks', 'pottery', 'pounder', 'pounding', 'pralines', 'prayers', 'preboil', 'present', 'preservation', 'presifted', 'problem', 'processing', 'produce', 'product', 'progresso', 'proper', 'properly', 'provence', 'proverbs', 'provide', 'prune', 'pulse', 'pushed', 'quail', 'quartered', 'r', 'rabbits', 'radish', 'raised', 'ramekin', 'range', 'rapid', 'rave', 'ravioli', 'reach', 'reached', 'reaching', 'reads', 'rearranging', 'reassemble', 'receiver', 'rechill', 'recipes', 'recommends', 'reddish', 'redness', 'refreeze', 'refresh', 'refreshing', 'refrigeration', 'rehydrate', 'relates', 'rellenos', 'remake', 'removable', 'require', 'required', 'resealable', 'reshape', 'resift', 'restaurant', 'retaining', 'reviews', 'rib', 'ribbon', 'rices', 'rinds', 'rinses', 'risen', 'rising', 'rod', 'romano', 'root', 'ropes', 'rosettes', 'rough', 'roughy', 'rouille', 'rounder', 'rounding', 'rusty', 'sack', 'safflower', 'saifon', 'salts', 'sam', 'sandies', 'sangria', 'satin', 'saturated', 'saturday', 'saucepans', 'sauteing', 'saver', 'scalloped', 'scorch', 'scored', 'scramblers', 'scraps', 'scratch', 'screw', 'screwed', 'screwtop', 'scum', 'sec', 'seedless', 'sense', 'separating', 'settle', 'seven', 'severing', 'sewing', 'shakes', 'shalloww', 'shanks', 'shavings', 'sheen', 'shelled', 'sherbert', 'shift', 'shin', 'shingles', 'shoots', 'short', 'shortcake', 'shortenings', 'shortest', 'shortly', 'shot', 'shovel', 'showing', 'shown', 'shreds', 'sieved', 'sifter', 'sight', 'silly', 'simmers', 'simple', 'since', 'sinking', 'sir', 'sirloin', 'sits', 'skimmed', 'skimming', 'skinned', 'skor', 'sky', 'sleep', 'slicer', 'slimmer', 'slip', 'slivered', 'slivers', 'sloppy', 'slump', 'slurpy', 'slushy', 'smallest', 'smashing', 'smear', 'smoker', 'smokies', 'smoking', 'smoothly', 'smother', 'smothering', 'snacks', 'snakes', 'snap', 'snifter', 'soap', 'soba', 'sociables', 'softer', 'softly', 'soggy', 'sole', 'solomon', 'somewhat', 'sons', 'sooo', 'souls', 'sound', 'sounds', 'sourdough', 'spaces', 'spade', 'spam', 'spanakopeta', 'spareribs', 'sparingly', 'sparkle', 'specific', 'spicy', 'spill', 'spoiled', 'spot', 'spree', 'springerle', 'squeezed', 'squirrels', 'squirt', 'st', 'stacked', 'stainless', 'stakes', 'stalk', 'starved', 'state', 'steal', 'steaming', 'steps', 'stews', 'stickiness', 'stocks', 'stones', 'stopped', 'stopping', 'straight', 'strands', 'straw', 'stri', 'stroganoff', 'study', 'submerge', 'submerged', 'substitutes', 'sufficient', 'summer', 'sundae', 'sunday', 'surfaces', 'surprise', 'surprised', 'swanson', 'swedish', 'swirls', 'switch', 'swollen', 'syrups', 'tabletop', 'tamale', 'tamari', 'tang', 'tartness', 'tatar', 'technical', 'technique', 'telephone', 'tell', 'tells', 'temp', 'tempera', 'ten', 'tenderize', 'tenderizer', 'tenderness', 'tennis', 'tepid', 'term', 'terrific', 'terry', 'tested', 'thanksgiving', 'thaws', 'thickly', 'thins', 'though', 'threads', 'throughly', 'throwing', 'thumbprint', 'til', 'tilt', 'tilting', 'tinged', 'toaster', 'toasty', 'tolerance', 'tonic', 'topper', 'torn', 'tostados', 'totally', 'toughening', 'toweling', 'trace', 'triple', 'triples', 'triscuits', 'tropicana', 'trout', 'trowel', 'true', 'tub', 'tubular', 'tucking', 'tuning', 'tunnel', 'turner', 'turnips', 'twelve', 'twenty', 'tying', 'typical', 'unavailable', 'unbuttered', 'uncoated', 'uncovering', 'uncut', 'undercook', 'underneath', 'undissolved', 'unevenly', 'unheated', 'uniform', 'unmelted', 'unmolded', 'unpleasantness', 'unpricked', 'unsalted', 'unsifted', 'unused', 'upper', 'ups', 'useful', 'uses', 'utility', 'valley', 'vaporized', 'variations', 'vary', 'veg', 'veggie', 'vein', 'velvety', 'vented', 'vents', 'vienna', 'vinaigrette', 'visible', 'vitamin', 'vodka', 'w', 'waffle', 'wall', 'watching', 'watermelon', 'waters', 'watery', 'wattage', 'waverly', 'ways', 'weaving', 'weighted', 'were', 'wheatsworth', 'wheels', 'where', 'whether', 'whichever', 'whisking', 'whoop', 'wid', 'wiener', 'wiggly', 'wil', 'wilt', 'wing', 'wonder', 'wonderful', 'wonders', 'wonton', 'workable', 'worked', 'world', 'worth', 'wow', 'wrapping', 'wraps', 'wring', 'written', 'year', 'yet', 'zag', 'zeppole', 'zesty', 'zig', 'zipper', 'zippered', 'zita']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sequence(model, max_len=1000, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "    \n",
        "    inp = torch.Tensor([baseline_vocab_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None\n",
        "    step = 1\n",
        "\n",
        "    for c in range(max_len):\n",
        "          output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "          output_dist = output.data.view(-1).div(temperature).exp()\n",
        "          top = int(torch.multinomial(output_dist, 1)[0])\n",
        "\n",
        "          predicted_char = baseline_vocab_itos[top]\n",
        "\n",
        "          if predicted_char == \"<pad>\":\n",
        "              continue\n",
        "\n",
        "          if predicted_char == \"<BOS>\":\n",
        "              continue\n",
        "          \n",
        "          if predicted_char == \"<unk>\":\n",
        "              continue\n",
        "\n",
        "          if predicted_char == \";\":\n",
        "              step += 1\n",
        "              predicted_char = str(\"\\n \" + str(step) + \".\")\n",
        "\n",
        "          if predicted_char == \"<EOS>\":\n",
        "              break\n",
        "\n",
        "          generated_sequence += predicted_char + \" \"\n",
        "          inp = torch.Tensor([top]).long()\n",
        "\n",
        "    return generated_sequence"
      ],
      "metadata": {
        "id": "nv5xBtwsdSn-"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(descriminator, generator, data, lr, batch_size, num_epochs):\n",
        "  d_optimizer = optim.Adam(descriminator.parameters(), lr)\n",
        "  g_optimizer = optim.Adam(generator.parameters(), lr)\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "  \n",
        "  train_data = data\n",
        "  #train_loader = train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  data_iter = torchtext.legacy.data.BucketIterator(data, batch_size=batch_size, sort_key=lambda x: len(x.directions), sort_within_batch=True)\n",
        "\n",
        "  samples = []\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    descriminator.train()\n",
        "    generator.train()\n",
        "\n",
        "    for (directions, lengths), ingredients in data_iter:\n",
        "   # for batch_i,  real_recipes,ingredients) in enumerate(train_data['directions'],train_data['ingredients']):\n",
        "            real_directions = directions\n",
        "            #print(real_directions)\n",
        "\n",
        "            # batch_size = real_recipes.size(0)\n",
        "\n",
        "            # === Train the Discriminator ===\n",
        "            \n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # discriminator losses on real recipe \n",
        "          \n",
        "            D_real = descriminator(real_directions)\n",
        "            labels = torch.ones(batch_size)\n",
        "\n",
        "           \n",
        "            D_real= sum(D_real)/D_real.shape[0]\n",
        "            d_real_loss = criterion(D_real, labels)\n",
        "\n",
        "            \n",
        "            # discriminator losses on fake recipe\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "            \n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = generator(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "            D_fake = descriminator(fake_recipes)\n",
        "            \n",
        "            #labels = torch.zeros(batch_size) # fake labels = 0\n",
        "            labelsD = torch.zeros(1)\n",
        "            labelsD = torch.diag(labelsD,0)\n",
        "            d_fake_loss = criterion(D_fake, labelsD)\n",
        "            \n",
        "            # add up losses and update parameters\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "\n",
        "             # === Train the Generator ===\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # generator losses on fake images\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "\n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = generator(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "\n",
        "            D_fake = descriminator(fake_recipes)\n",
        "            #labels = torch.ones(batch_size) #flipped labels\n",
        "\n",
        "\n",
        "            labels = torch.ones(batch_size)\n",
        "            # compute loss and update parameters\n",
        "            g_loss = criterion(D_fake,labels)\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "                % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # append discriminator loss and generator loss\n",
        "            losses.append((d_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "NtBl3mWA1OKd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discriminator(baseline_vocab_size, 64, n_layers=1)\n",
        "gen = Generator(baseline_vocab_size, 64, n_layers=1)\n",
        "train_GAN(disc, gen, baseline_data, lr=lr, batch_size=batch_size, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "xzYaUvBf3F1L",
        "outputId": "587f60bb-7780-454d-f4bf-6b7a9e0710c0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
            "        [ 236,   49,   80,   13,  164,   13,   49,   13,   41,  594,  318,   28],\n",
            "        [ 127,  379,   57,   45,  116,   45,   45,   45,   45,  758,    5,    5],\n",
            "        [ 154,    5,    5,   23,    4,   23,  100,   23,   23,  782,   17,  234],\n",
            "        [   8,  126,  961,   20,   13,    4,   38,    4,   13,    5,  497,  164],\n",
            "        [ 276,   74,    4,   12,  378,    6,    9,   27,   32,  110,  267,   56],\n",
            "        [1013,    4,   33,    4,  346,   68,  126,   20,    4,   45,    4,   23],\n",
            "        [   5,    8,  228,   48,  143,  529,    4,   73,   59,   23,   50,   14],\n",
            "        [  21,  187,  266,    4,    9,  241,   74,   56,    5,   38,    4,   56],\n",
            "        [ 714,  473,    7,   27,  124,    5,    4,  173,  123,  367,  438,   10],\n",
            "        [ 628,  136,  256,  209,  297,   33,    8,  106,   88,   11,  677,    4],\n",
            "        [ 304,  134,  273,  225,    5,   22,   35,   17,   79,  274,    4,   28],\n",
            "        [ 175,    5,    4,  624,  219,  144,    5,   18, 1329,    5,   78,    6],\n",
            "        [   8,   42,   93,   59,   20,   14,  115,   31,  308,  319, 1349,   87],\n",
            "        [ 417,    7,  151,    9,  147,   62,    6,   14,    4,  767,    5,    5],\n",
            "        [ 496,  379,    4,   43,   19,   88,  100,   75,  490,    4, 2662,   98],\n",
            "        [   5,    4,   13,    5,   69,   79,    4,   10,   94,  179, 2322,    4],\n",
            "        [1230,    8,   38,  263,   54,    4,    8,    4,    7,   12,  597,   60],\n",
            "        [ 449,  214,  569,   20,   96,   70,  111,   92,   88,  237,  677,   20],\n",
            "        [  14,    5,    5,   12,   40,   25,    5,   99,  145,   29,  583,   29],\n",
            "        [1748,  103,  305,  301,   29,  142,  313,  145,    4,    4,    4,    4],\n",
            "        [   4,    4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3],\n",
            "        [   3,    3,    3,    3,    3,    3,    3,    3,    1,    1,    1,    1]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-7d626b6569c5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-0b1f786dc2eb>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[0;34m(descriminator, generator, data, lr, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# z = torch.from_numpy(z).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingredients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got NoneType)"
          ]
        }
      ]
    }
  ]
}