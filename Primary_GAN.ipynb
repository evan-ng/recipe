{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evan-ng/tastebud/blob/main/Primary_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Strcuture (GRU)\n"
      ],
      "metadata": {
        "id": "20rMhzbyCODl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install torchtext==0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0KWc4m-A-2z",
        "outputId": "ddb25692-eb61-49ae-e57c-5579fbf1f50c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.9/dist-packages (1.8.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0+cu111) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0+cu111) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.9 in /usr/local/lib/python3.9/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (1.8.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.9) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.8.0->torchtext==0.9) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.9) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "C8R9vyY0CNfu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchtext\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import json\n",
        "import ast\n",
        "import glob\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "from torchtext.legacy import data\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,hidden_size,n_layers=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.emb = torch.eye(vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.GRU(vocab_size,hidden_size,batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size,50)\n",
        "        self.fc2 = nn.Linear(50,1)\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        \n",
        "        x = self.emb[x]\n",
        "        out, hidden = self.rnn(x,hidden)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,hidden_size,n_layers=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.emb = torch.eye(vocab_size)\n",
        "        self.rnn = nn.GRU(vocab_size,hidden_size, n_layers,batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden = None):\n",
        "        x = self.emb[x]\n",
        "        out, hidden = self.rnn(x,hidden)\n",
        "        out = self.fc1(out)\n",
        "        return out, hidden\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous Training (Example From Tutorial)\n"
      ],
      "metadata": {
        "id": "ZazC4fmCCSsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train (trainDS,G,D,lr=0.002,batch_size=1,num_epochs=10):\n",
        "  d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "  g_optimizer = optim.Adam(G.parameters(), lr)\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  train_data = trainDS\n",
        "  #train_loader = train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  samples = []\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    D.train()\n",
        "    G.train()\n",
        "\n",
        "    for batch_i in range(len(train_data)):\n",
        "   # for batch_i,  real_recipes,ingredients) in enumerate(train_data['directions'],train_data['ingredients']):\n",
        "            real_recipes = train_data['directions'][batch_i]\n",
        "            ingredients = train_data['ingredients'][batch_i]\n",
        "\n",
        "            # batch_size = real_recipes.size(0)\n",
        "\n",
        "            # === Train the Discriminator ===\n",
        "            \n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # discriminator losses on real images \n",
        "          \n",
        "            D_real = D(real_recipes)\n",
        "            labels = torch.ones(batch_size)\n",
        "\n",
        "           \n",
        "            D_real= sum(D_real)/D_real.shape[0]\n",
        "            d_real_loss = criterion(D_real, labels)\n",
        "\n",
        "            \n",
        "            # discriminator losses on fake images\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "            \n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = G(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "            D_fake = D(fake_recipes)\n",
        "            \n",
        "            #labels = torch.zeros(batch_size) # fake labels = 0\n",
        "            labelsD = torch.zeros(1)\n",
        "            labelsD = torch.diag(labelsD,0)\n",
        "            d_fake_loss = criterion(D_fake, labelsD)\n",
        "            \n",
        "            # add up losses and update parameters\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "\n",
        "             # === Train the Generator ===\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # generator losses on fake images\n",
        "            # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # z = torch.from_numpy(z).float()\n",
        "\n",
        "            ingredients = torch.Tensor(ingredients) \n",
        "            ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            fake_recipes = G(ingredients)\n",
        "\n",
        "            fake_recipes = torch.Tensor(fake_recipes) \n",
        "            fake_recipes = fake_recipes.to(torch.long)\n",
        "\n",
        "            D_fake = D(fake_recipes)\n",
        "            #labels = torch.ones(batch_size) #flipped labels\n",
        "\n",
        "\n",
        "            labels = torch.ones(batch_size)\n",
        "            # compute loss and update parameters\n",
        "            g_loss = criterion(D_fake,labels)\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "                % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # append discriminator loss and generator loss\n",
        "            losses.append((d_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "vrz3-cmNCXMP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Training"
      ],
      "metadata": {
        "id": "eX9T17Z9vvbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r2Ujs4rAaMt",
        "outputId": "40e2c00a-bfae-4f75-9fff-5e6c19598803"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting HyperParameters"
      ],
      "metadata": {
        "id": "VvxXBAhGv6hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4\n",
        "batch_size = 12\n",
        "num_epochs = 5\n",
        "max_recipe_len = 1000\n"
      ],
      "metadata": {
        "id": "Qr43CaQpv15Y"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting Dataset (From Baseline)"
      ],
      "metadata": {
        "id": "4rATo3vpd0bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [r'\\'', r'\\\"', r'\\.', r'<br \\/>', r',', r'\\(', r'\\)', r'\\!', r'\\?', r'\\:', r'\\s+']\n",
        "replacements = [' \\'  ', '', ' . ', ' ', ' , ', ' ( ', ' ) ', ' ! ', ' ? ', ' ', ' ']\n",
        "patterns_dict = list((re.compile(p), r) for p, r in zip(patterns, replacements))\n",
        "\n",
        "def basic_english_normalize(line):\n",
        "    line = line.lower()\n",
        "    for pattern_re, replaced_str in patterns_dict:\n",
        "        line = pattern_re.sub(replaced_str, line)\n",
        "    return line.split()\n",
        "\n",
        "directions_field = torchtext.legacy.data.Field(sequential=True,\n",
        "                                  tokenize=basic_english_normalize,\n",
        "                                  include_lengths=True,\n",
        "                                  batch_first=False,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\")\n",
        "\n",
        "fields = [('directions', directions_field), ('ingredients', None)]\n",
        "baseline_data = torchtext.legacy.data.TabularDataset(\"drive/MyDrive/data/char_nlg_df.csv\", \"csv\", fields)\n",
        "\n",
        "directions_field.build_vocab(baseline_data)\n",
        "directions_field.vocab = torchtext.vocab.Vocab(directions_field.vocab.freqs, specials=['<unk>','<pad>', '<BOS>', '<EOS>', ';'])\n",
        "baseline_vocab_stoi = directions_field.vocab.stoi\n",
        "baseline_vocab_itos = directions_field.vocab.itos\n",
        "baseline_vocab_size = len(directions_field.vocab.itos)\n",
        "print(baseline_vocab_size)\n",
        "print(directions_field.vocab.itos)\n",
        "\n",
        "# make data fields for ingredients\n",
        "ingredients_field = torchtext.legacy.data.Field(sequential=True,\n",
        "                                  tokenize=basic_english_normalize,\n",
        "                                  include_lengths=True,\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,\n",
        "                                  init_token=\"<BOS>\",\n",
        "                                  eos_token=\"<EOS>\")\n",
        "ing_fields = [('directions', None), ('ingredients', ingredients_field)]\n",
        "ingredients_data = torchtext.legacy.data.TabularDataset(\"drive/MyDrive/data/char_nlg_df.csv\", \"csv\", ing_fields)\n",
        "ingredients_field.build_vocab(ingredients_data)\n",
        "ingredients_field.vocab = torchtext.vocab.Vocab(ingredients_field.vocab.freqs, specials=['<unk>','<pad>', '<BOS>', '<EOS>', ';'])\n",
        "ingredients_vocab_stoi = ingredients_field.vocab.stoi\n",
        "ingredients_vocab_itos = ingredients_field.vocab.itos\n",
        "ingredients_vocab_size = len(ingredients_field.vocab.itos)\n",
        "print(ingredients_vocab_size)\n",
        "print(ingredients_field.vocab.itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDU7SvPPAdYa",
        "outputId": "f461ac3c-0b9e-4f08-96ba-c67773915714"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3502\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ';', 'and', 'in', 'to', 'add', 'with', 'minutes', 'until', 'a', 'mix', 'for', '1', 'the', 'bake', 'at', 'of', 'into', '2', 'on', 'ingredients', 'sugar', 'over', 'or', 'pour', 'stir', 'pan', 'mixture', '350', 'well', 'cook', 'butter', 'cheese', 'heat', 'water', 'together', 'top', 'inch', 'combine', 'salt', 'flour', 'cream', 'all', 'place', 'oven', 'bowl', 'brown', 'cool', 'milk', 'beat', 'baking', 'x', '4', '3', 'chicken', 'eggs', 'cover', 'put', 'onion', 'about', 'dish', 'sprinkle', 'pepper', 'then', 'cup', 'large', '9', 'serve', '10', 'cut', 'greased', 'drain', '30', 'remaining', 'is', 'remove', 'hours', 'boil', 'from', 'sauce', 'let', 'spread', 'hour', 'egg', 'vanilla', '8', '5', 'cake', 'oil', 'makes', 'set', '6', 'hot', '13', '20', 'nuts', '15', 'meat', 'margarine', 'dry', 'simmer', 'roll', 'stirring', 'casserole', 'medium', 'chill', 'each', 'blend', 'soup', 'juice', 'tender', 'pie', 'melt', 'layer', 'potatoes', 'skillet', 'sheet', 'if', 'small', '45', 'refrigerate', 'melted', 'it', 'onions', 'before', 'cookie', 'out', 'saucepan', 'are', 'bread', 'as', 'garlic', 'be', 'powder', 'bring', 'half', 'smooth', 'cups', 'dough', 'rice', 'crumbs', 'low', 'servings', 'beef', 'bottom', '375', 'can', 'chocolate', 'aside', 'make', '12', 'serving', 'spoon', 'one', 'whip', 'slice', 'pineapple', 'boiling', 'lightly', 'soda', 'not', 'first', '25', 'pieces', 'done', 'serves', 'fold', 'when', '325', 'time', 'quart', 'crust', 'up', 'preheat', 'paper', 'grease', 'use', 'batter', 'lemon', 'you', 'chips', 'refrigerator', 'sour', 'other', 'tomatoes', 'vegetables', 'bacon', 'sift', 'saute', 'this', '40', 'by', 'cinnamon', 'high', 'thoroughly', 'corn', 'green', 'celery', 'desired', 'overnight', 'balls', '35', 'mixing', 'stand', 'warm', 'shortening', 'onto', '400', 'tomato', 'do', 'thick', 'beans', 'golden', 'will', 'an', 'chopped', 'press', 'slices', 'turn', 'crackers', 'pudding', 'vinegar', 'floured', 'more', 'drop', 'broccoli', 'off', 'pans', 'except', 'after', 'constantly', 'microwave', 'broth', 'just', 'loaf', 'foil', 'gradually', 'uncovered', 'pot', 'fat', 'pecans', 'cold', 'covered', 'noodles', 'blended', 'coconut', 'fruit', 'may', 'cooked', 'fry', 'center', 'very', 'mushrooms', 'package', 'liquid', 'reduce', 'tablespoons', 'apples', 'dissolve', 'browned', 'shape', 'size', 'whipped', 'according', 'shell', 'toss', 'while', 'garnish', 'parsley', 'teaspoon', 'directions', 'firm', 'ground', 'ice', 'minute', 'slightly', 'syrup', 'powdered', 'taste', 'carrots', 'cooking', 'filling', 'rest', 'sides', 'little', 'mixer', 'evenly', '50', 'dip', 'dressing', 'enough', 'least', 'whites', 'side', 'oleo', 'peel', 'buttered', 'peanut', 'ball', 'blender', 'form', '7', 'mayonnaise', 'layers', 'slowly', 'occasionally', 'fork', 'mustard', 'salad', 'speed', 'peppers', 'plate', 'ungreased', 'arrange', 'light', 'prepare', 'keep', 'longer', 'approximately', 'container', 'jello', 'mixed', 'sausage', 'coat', 'next', 'spray', 'through', 'wrap', 'cabbage', 'your', 'orange', 'soft', 'yields', 'cookies', 'cornstarch', 'marshmallows', 'topping', 'fluffy', 'raisins', 'ready', 'several', 'using', 'squares', 'cracker', 'beaten', 'beating', 'cocoa', 'tablespoon', 'crushed', 'muffin', 'store', 'take', 'clean', 'thin', 'but', 'completely', 'fill', 'good', 'rolls', 'spaghetti', 'them', 'thickened', 'two', 'chop', 'freeze', 'yolks', 'macaroni', 'shallow', 'that', 'wax', 'buttermilk', 'tube', 'yeast', 'down', 'gently', 'graham', 'hamburger', 'heavy', 'once', 'bananas', 'comes', 'dozen', 'few', 'gelatin', 'bubbly', 'double', 'grated', 'separate', 'white', 'last', 'stuffing', '425', 'another', 'inches', 'rack', 'seasoning', 'stiff', 'glass', 'directed', 'temperature', 'alternately', 'both', 'electric', 'immediately', 'strawberries', 'has', 'parmesan', 'repeat', 'return', 'bag', 'ham', 'lettuce', 'line', 'frozen', 'sliced', 'strips', 'cubes', 'fish', 'season', '300', 'continue', 'jell', 'rise', 'seasonings', 'spices', 'creamed', 'paprika', 'square', 'ginger', 'pork', 'brush', 'dissolved', 'drained', 'food', 'have', 'wash', 'waxed', 'heated', 'o', 'pat', 'soy', 'knife', '450', 'like', 'again', 'break', 'cherries', 'crisp', 'adding', 'chops', 'creamy', 'plastic', 's', 'been', 'box', 'bundt', 'cooled', 'room', 'seal', 'shake', '24', 'apple', 'around', 'walnuts', 'baked', 'nutmeg', 'shells', 'so', '60', 'addition', 'chili', 'deep', 'melts', 'mushroom', 'preheated', 'stick', 'also', 'biscuits', 'crumble', 'full', 'pasta', 'i', 'jars', 'pumpkin', 'seed', 'should', 'shrimp', 'yield', 'allow', 'apart', 'olives', 'peaches', 'any', 'oranges', 'pastry', 'vegetable', 'boiler', 'coated', 'knead', 'lined', 'rinse', 'steak', 'used', 'bouillon', 'crisco', 'jar', 'some', 'amount', 'cheddar', 'drizzle', 'freezer', 'fresh', 'unbaked', 'worcestershire', 'zucchini', 'confectioners', 'mash', 'oats', 'punch', 'round', 'sheets', 'wire', 'degrees', 'marinate', 'squash', 'bars', 'crock', 'hands', 'they', 'additional', 'flakes', 'hand', 'jelly', 'lay', 'mold', 'reserve', 'spinach', 'wine', '55', 'edges', 'nonstick', 'potato', 'prepared', 'sugars', 'toothpick', 'bell', 'cereal', 'day', 'divide', 'measure', 'needed', 'paste', 'peas', 'pound', 'skin', 'tortilla', '11', 'consistency', 'leaf', 'olive', 'roast', 'shredded', 'back', 'pizza', 'red', 'soften', 'soups', 'whisk', 'bite', 'cucumbers', 'moistened', 'process', 'recipe', 'times', 'too', 'between', 'board', 'breasts', 'lid', 'teaspoons', 'whole', 'bay', 'candy', 'inserted', 'meal', 'order', 'oregano', 'sure', 'turkey', 'almonds', 'cherry', 'crush', 'gravy', 'honey', 'leave', 'leaves', 'peanuts', 'soak', 'thicken', 'cheeses', 'chunks', 'clear', 'dice', 'extract', 'hard', 'no', 'pam', 'stove', 'frying', 'oatmeal', 'piece', 'salmon', 't', 'tightly', 'added', 'cottage', 'fine', 'frequently', 'long', 'reserved', 'save', 'surface', 'sweet', 'turning', 'calories', 'chilled', 'days', 'dutch', 'flavoring', 'halves', 'icing', 'juices', 'softened', 'ale', 'cans', 'cutting', 'frost', 'glaze', 'need', 'often', 'rind', 'salted', 'still', 'thickens', 'transfer', 'which', 'yogurt', '16', 'breast', 'catsup', 'crumbly', 'drippings', 'flatten', 'mozzarella', 'seconds', 'served', 'three', 'tortillas', '250', 'airtight', 'bone', 'ending', 'enjoy', 'forms', 'frosting', 'great', 'removing', 'sprayed', 'condensed', 'excess', 'per', 'taco', 'thickness', '18', 'berries', 'biscuit', 'broil', 'carefully', 'dill', 'dot', 'lengthwise', 'mashed', 'patties', 'processor', 'teaspoonfuls', 'evaporated', 'every', 'flat', 'foamy', 'marinade', 'moderate', 'quickly', 'salsa', 'slow', 'towel', 'wedges', 'applesauce', 'don', 'ends', 'made', 'necessary', 'steam', 'tartar', 'best', 'bits', 'cornmeal', 'cranberries', 'easy', 'grill', 'peaks', 'platter', 'quarts', 'remainder', 'tops', 'ahead', 'black', 'cloves', 'crescent', 'crumb', 'everything', 'loaves', 'muffins', 'oblong', 'only', 'open', 'pack', 'rings', 'uncover', 'under', 'almost', 'alternate', 'broiler', 'chilies', 'dates', 'kettle', 'sifted', 'spoonfuls', 'tuna', 'along', 'basil', 'cauliflower', 'combined', 'cornbread', 'edge', 'favorite', 'four', 'grate', 'ketchup', 'lukewarm', 'middle', 'pink', 'reserving', 'same', 'toast', 'towels', 'wafers', 'c', 'color', 'delicious', 'diced', 'end', 'freezes', 'french', 'italian', 'refrigerated', 'second', 'seeds', 'steaks', 'stems', 'almond', 'banana', 'barbecue', 'begins', 'better', 'chestnuts', 'crusts', 'cube', 'dust', 'even', 'finely', 'fruits', 'm', 'pyrex', 'rolling', 'scoop', 'sherbet', 'sit', 'thaw', 'weeks', 'above', 'cocktail', 'coffee', 'coloring', 'during', 'leaving', 'molasses', 'point', 'single', 'strawberry', 'want', 'wings', '14', 'bean', 'buns', 'cucumber', 'dash', 'eat', 'holes', 'lime', 'meanwhile', 'optional', 'ounce', 'pecan', 'preserves', 'rounded', 'strain', 'these', 'asparagus', 'big', 'blueberries', 'brand', 'curry', 'doritos', 'eagle', 'flavor', 'granulated', 'grind', 'iron', 'making', 'moist', 'puree', 'raw', 'ritz', 'stage', 'stuff', 'bubbles', 'etc', 'heaping', 'lower', 'morning', 'now', 'part', 'pears', 'pies', 'spatula', 'tins', 'trim', 'way', 'work', 'aluminum', 'beater', 'bisquick', 'boils', 'choice', 'cornflakes', 'five', 'get', 'greens', 'keeps', 'meatballs', 'oiled', 'partially', 'pepperoni', 'pimento', 'racks', 'seasoned', 'spring', 'tin', 'whipping', 'wooden', 'yellow', 'yolk', '0', '275', 'blending', 'cakes', 'come', 'entire', 'herbs', 'instant', 'measuring', 'moisten', 'prevent', 'real', 'sized', 'start', 'stock', 'tear', 'thyme', 'wet', 'avocado', 'baste', 'butterscotch', 'close', 'cranberry', 'crumbled', 'equal', 'extra', 'gallon', 'hole', 'less', 'marshmallow', 'much', 'pinch', 'poke', 'prick', 'rhubarb', 'roasting', 'sesame', 'sharp', 'sherry', 'thinly', 'tight', 'twice', 'without', 'across', 'away', 'bones', 'caramel', 'carrot', 'clams', 'core', 'cutter', 'dinner', 'discard', 'easily', 'finish', 'firmly', 'given', 'halfway', 'individual', 'inside', 'instead', 'invert', 'leftover', 'oysters', 'parts', 'pint', 'popcorn', 'possible', 'pull', 'pulp', 'quarters', 'ricotta', 'rub', 'shapes', 'shred', 'sterilized', 'strip', 'alternating', 'basting', 'canned', 'cayenne', 'cider', 'cooker', 'crab', 'croutons', 'cubed', 'debone', 'dissolves', 'doubled', 'fingers', 'flavors', 'following', 'gone', 'handle', 'its', 'logs', 'mandarin', 'meringue', 'paraffin', 'people', 'plain', 're', 'rectangle', 'skins', 'smaller', 'spice', 'stew', 'teaspoonful', 'than', 'toasted', 'veggies', 'velveeta', 'wheat', 'allspice', 'colored', 'custard', 'dream', 'dried', 'eggplant', 'empty', 'fillets', 'flavorings', 'generously', 'glasses', 'glazed', 'grits', 'jam', 'lard', 'listed', 'log', 'looks', 'minced', 'mixtures', 'morsels', 'night', 'plus', 'pounds', 'quick', 'ring', 'rye', 'sandwiches', 'spreading', 'squeeze', 'sticky', 'sweetened', 'swiss', 'tabasco', 'takes', 'transparent', 'uncooked', 'undrained', 'weed', 'wide', 'begin', 'circle', 'crabmeat', 'crack', 'cumin', 'dishes', 'dredge', 'drops', 'finger', 'fit', 'fried', 'g', 'gets', 'hold', 'leftovers', 'level', 'nut', 'overbake', 'poppy', 'regular', 'right', 'sauerkraut', 'scallops', 'scrape', 'setting', 'skim', 'unbeaten', '100', '36', 'absorbed', 'beer', 'beets', 'bit', 'brownies', 'coarse', 'couple', 'crosswise', 'dipped', 'filled', 'flake', 'hamburg', 'heating', 'karo', 'krispies', 'look', 'meats', 'miracle', 'note', 'overlapping', 'pancakes', 'pickles', 'quarter', 'ranch', 'rolled', 'rosemary', 'rounds', 'rum', 'run', 'simmering', 'slaw', 'split', 'sprouts', 'sticks', 'topped', 'walnut', 'wok', 'adjust', 'aid', 'american', 'being', 'blueberry', 'boiled', 'bourbon', 'browns', 'cheesecloth', 'cloth', 'coating', 'colander', 'cooling', 'covering', 'crispy', 'decorate', 'depending', 'dump', 'either', 'follow', 'frypan', 'fudge', 'grapes', 'hershey', 'kool', 'lemonade', 'lumpy', 'mini', 'oreos', 'ovenproof', 'overcook', 'packet', 'picante', 'pints', 'pop', 'portions', 'rectangular', 'sandwich', 'sauteed', 'scald', 'seam', 'separately', 'sprinkled', 'stem', 'stiffly', 'sweetener', 'tater', 'tea', 'tester', 'tests', 'thawed', 'there', 'tots', 'vigorously', '200', '22', '9x13', 'bed', 'beginning', 'bottoms', 'careful', 'chives', 'corned', 'cupcake', 'diameter', 'dogs', 'fashion', 'fire', 'fridge', 'fritos', 'grain', 'hens', 'later', 'liquids', 'loosen', 'lumps', 'mallet', 'mayo', 'miniature', 'must', 'near', 'outer', 'outside', 'own', 'power', 'ribs', 'ro', 'safe', 'sections', 'slicing', 'soon', 'spoonful', 'starting', 'starts', 'sticking', 'tbsp', 'tel', 'thermometer', 'tie', 'till', 'translucent', 'wait', 'watch', '48', '70', 'amounts', 'bags', 'bar', 'barely', 'bath', 'blade', 'bottle', 'bowls', 'broken', 'caramels', 'chipped', 'chuck', 'cilantro', 'clove', 'coals', 'coarsely', 'concentrate', 'containers', 'cutlets', 'diagonally', 'dropped', 'duty', 'enchilada', 'filo', 'griddle', 'harden', 'head', 'instructions', 'kidney', 'left', 'lift', 'maraschino', 'metal', 'molds', 'most', 'my', 'n', 'nearly', 'nice', 'oreo', 'packages', 'pancake', 'parboil', 'pickle', 'placing', 'portion', 'poultry', 'pressing', 'pretzels', 'reaches', 'refried', 'sage', 'scraping', 'sealed', 'skewers', 'slotted', 'substitute', 'such', 'tablespoonfuls', 'tastes', 'third', 'toothpicks', 'touch', 'unroll', 'wish', 'would', '17', '65', 'angel', 'apricot', 'artichoke', 'assorted', 'avocados', 'becomes', 'bran', 'breaking', 'briefly', 'browning', 'bubbling', 'bulk', 'check', 'chiles', 'chip', 'congeal', 'creme', 'crusty', 'dark', 'dessert', 'does', 'doneness', 'drink', 'excellent', 'fitting', 'frothy', 'gelatine', 'go', 'hearts', 'heath', 'herb', 'increase', 'limp', 'loses', 'main', 'min', 'mint', 'moderately', 'nicely', 'okra', 'old', 'pimentos', 'pin', 'pitcher', 'poured', 'prefer', 'puffed', 'push', 'relish', 'removed', 'rich', 'running', 'semi', 'six', 'slit', 'strainer', 'style', 'tarragon', 'test', 'their', 'thread', 'tortellini', 'tossing', 'tough', 'triangles', 'undiluted', 'upside', 'usually', 'v', 'wafer', 'week', 'whatever', 'whiz', '120', '32', 'air', 'always', 'among', 'baby', 'brandy', 'breadcrumbs', 'brushing', 'caps', 'centers', 'chafing', 'cheez', 'cholesterol', 'cob', 'contains', 'continually', 'cools', 'counter', 'cupcakes', 'degree', 'depression', 'dipping', 'disappears', 'distribute', 'doesn', 'door', 'finished', 'fully', 'gel', 'give', 'glossy', 'halved', 'hash', 'hominy', 'horseradish', 'including', 'keeping', 'kind', 'lamb', 'layering', 'lemons', 'liberally', 'lids', 'lot', 'many', 'marble', 'months', 'narrow', 'needs', 'omit', 'overmix', 'party', 'patty', 'pepperidge', 'pick', 'placed', 'plates', 'preparation', 'pretty', 'proof', 'qt', 'radishes', 'ragu', 'raisin', 'raspberries', 'roaster', 'rotary', 'saut', 'secure', 'slits', 'soaked', 'solid', 'stream', 'table', 'tapioca', 'texture', 'touched', 'try', 'turmeric', 'twinkies', 'veal', '130', '475', '75', '80', 'advance', 'alum', 'appetizer', 'atop', 'available', 'bagels', 'baker', 'bark', 'basket', 'beaters', 'become', 'below', 'burn', 'burner', 'caraway', 'cereals', 'children', 'chilling', 'combination', 'complete', 'crawfish', 'crepes', 'cuts', 'cutters', 'damp', 'diluted', 'directs', 'easier', 'elastic', 'extracts', 'farm', 'finally', 'find', 'flowers', 'foam', 'followed', 'forming', 'franks', 'free', 'frosted', 'fryer', 'funnel', 'generous', 'granules', 'jalapeno', 'kept', 'kiss', 'kraut', 'label', 'ladle', 'lasagna', 'liver', 'love', 'mango', 'maple', 'melting', 'mg', 'move', 'mushy', 'nectar', 'non', 'noodle', 'oat', 'ones', 'ounces', 'pare', 'parties', 'peach', 'peeled', 'pimientos', 'preferably', 'preference', 'preparing', 'prior', 'prunes', 'rectangles', 'reduced', 'resembles', 'roni', 'rotate', 'roux', 'rubber', 'scallions', 'sealing', 'seems', 'shallots', 'shelf', 'snack', 'souffle', 'spears', 'sprigs', 'springs', 'stack', 'stuffed', 'sunflower', 'swirl', 'tablespoonful', 'tamales', 'thicker', 'throughout', 'tinfoil', 'tiny', 'tray', 'tupperware', 'unmold', 'washed', 'wipe', 'working', 'wrapper', 'yams', '10x', '23', '370', 'absorb', 'altogether', 'anise', 'apply', 'avoid', 'base', 'bird', 'blot', 'blue', 'boned', 'brisket', 'brownie', 'bubble', 'burning', 'called', 'candies', 'canning', 'cast', 'chopper', 'christmas', 'circles', 'club', 'coats', 'coca', 'cola', 'contents', 'cooks', 'corners', 'cornflake', 'cracked', 'cross', 'crowd', 'crumbles', 'diagonal', 'different', 'dumplings', 'eaten', 'english', 'envelope', 'equals', 'evaporates', 'f', 'feeds', 'fettuccine', 'fingertips', 'fitted', 'fix', 'flavored', 'float', 'flute', 'folding', 'follows', 'formed', 'gallons', 'germ', 'grams', 'guacamole', 'halve', 'headspace', 'holds', 'hollow', 'icebox', 'important', 'itself', 'jack', 'kisses', 'knives', 'ladyfingers', 'larger', 'lawry', 'lentils', 'life', 'liners', 'marmalade', 'measured', 'microwavable', 'milnot', 'mixes', 'moisture', 'monterey', 'mugs', 'never', 'omitted', 'paint', 'peppercorns', 'pimiento', 'piping', 'points', 'position', 'pouring', 'pressure', 'pretzel', 'pulled', 'pulls', 'pureed', 'purpose', 'quantity', 'quite', 'raise', 'ramen', 'really', 'reheated', 'remains', 'rinsed', 'rises', 'rows', 'sauces', 'sausages', 'saving', 'scant', 'sear', 'section', 'sets', 'shoulder', 'sieve', 'simply', 'skewer', 'slide', 'snow', 'sort', 'soupy', 'special', 'spoons', 'sprinkling', 'squirrel', 'stalks', 'starter', 'stays', 'step', 'stewed', 'stone', 'stored', 'tart', 'thirds', 'throw', 'thumb', 'tint', 'total', 'turned', 'turns', 'twist', 'type', 'usual', 'wesson', 'what', 'works', 'wrapped', 'ziploc', 'ziti', '150', '240', '33', '46', '500', '64', '90', 'absorbent', 'against', 'al', 'alone', 'angle', 'anything', 'arugula', 'balance', 'batches', 'because', 'blackberries', 'blanch', 'blanched', 'blossoms', 'braise', 'breading', 'breads', 'breakfast', 'brine', 'bringing', 'brussels', 'bun', 'burgers', 'burritos', 'butterfly', 'capers', 'cashews', 'change', 'chex', 'chinese', 'chow', 'chunk', 'chutney', 'clam', 'closed', 'closely', 'clusters', 'congealed', 'could', 'countertop', 'couscous', 'cracklins', 'creaming', 'create', 'crepe', 'crimp', 'crockpot', 'crunch', 'crunchy', 'curdle', 'daily', 'de', 'deboned', 'decoration', 'decorative', 'deer', 'dente', 'depth', 'devein', 'diet', 'dippers', 'directly', 'dollop', 'early', 'eating', 'eight', 'elephant', 'enclose', 'everyone', 'exchange', 'family', 'fasten', 'feed', 'flaked', 'flaky', 'florets', 'flower', 'frankfurters', 'fun', 'garnished', 'gather', 'gentle', 'goes', 'grapefruit', 'grilled', 'grinder', 'halibut', 'hamburgers', 'handful', 'heatproof', 'holding', 'home', 'homemade', 'horizontally', 'how', 'hr', 'indefinitely', 'indention', 'ingredient', 'insert', 'insides', 'inverted', 'items', 'jackets', 'jellyroll', 'kahlua', 'kids', 'leeks', 'lengths', 'liquor', 'lite', 'loose', 'lots', 'machine', 'masher', 'mein', 'mill', 'milligrams', 'mine', 'mostaccioli', 'mound', 'mug', 'nacho', 'nine', 'number', 'nutmeats', 'overbeat', 'oyster', 'oz', 'packaged', 'palms', 'papers', 'particles', 'partly', 'pats', 'peek', 'peeling', 'penne', 'person', 'pet', 'pile', 'pinto', 'pit', 'pith', 'please', 'popped', 'pops', 'prebaked', 'preferred', 'pressed', 'procedure', 'puffs', 'puffy', 'pumpernickel', 'purchased', 'putting', 'rabbit', 'rapidly', 'release', 'remain', 'replace', 'requires', 'reynolds', 'rim', 'rinsing', 'ripe', 'roasted', 'romaine', 'rotel', 'rotini', 'row', 'runs', 'russian', 'saltwater', 'saucepot', 'saucer', 'scatter', 'scorching', 'seafood', 'seams', 'see', 'shaped', 'shows', 'sifting', 'slash', 'smoke', 'smoked', 'soaking', 'sodium', 'something', 'spiral', 'spooned', 'sprinkles', 'squeezing', 'standing', 'star', 'stay', 'steep', 'sterile', 'stirred', 'stockpot', 'stop', 'storage', 'strings', 'strokes', 'substituted', 'sun', 'surround', 'tails', 'taken', 'tall', 'tears', 'teriyaki', 'themselves', 'thickening', 'thinner', 'tip', 'tongs', 'toward', 'trays', 'triangle', 'trimmings', 'tsp', 'tuck', 'underside', 'upright', 'variety', 'various', 'vegies', 'vent', 'vermicelli', 'warmed', 'was', 'we', 'weather', 'whiskey', 'wieners', 'wild', 'wilted', 'within', 'yum', '112', '13x9', '1st', '234', '2nd', '365', '85', '86', '8ths', '9x9', 'able', 'accent', 'actually', 'age', 'alcohol', 'allowed', 'allowing', 'already', 'amaretto', 'amber', 'anchor', 'appear', 'appearance', 'approx', 'apricots', 'artichokes', 'artificial', 'average', 'awful', 'awhile', 'bakes', 'bakon', 'bamboo', 'basic', 'batch', 'beet', 'beverages', 'bigger', 'birds', 'birthday', 'blintzes', 'block', 'bombs', 'boneless', 'bottles', 'building', 'bundles', 'butterfingers', 'butterflied', 'button', 'buy', 'cajun', 'campbell', 'candied', 'candle', 'cantaloupe', 'carton', 'catalina', 'champagne', 'changes', 'checking', 'chestnut', 'chewy', 'child', 'chitterlings', 'chocolates', 'choose', 'chowder', 'chunked', 'church', 'circular', 'cleaned', 'clings', 'clumping', 'cluster', 'coke', 'cokes', 'cole', 'collards', 'colors', 'confectioner', 'consomme', 'containing', 'continuing', 'continuously', 'control', 'cores', 'corns', 'correct', 'country', 'covers', 'cracks', 'crank', 'crescents', 'criss', 'crumbling', 'crystals', 'd', 'dab', 'dampen', 'dashes', 'decorating', 'delicately', 'delightful', 'desire', 'deviled', 'dew', 'dilute', 'dirt', 'disappear', 'discarding', 'dividing', 'dog', 'doing', 'dome', 'dots', 'draft', 'draw', 'drinks', 'dropping', 'drying', 'dunk', 'eggbeater', 'elbow', 'emulsify', 'enchiladas', 'ensure', 'envelopes', 'equally', 'escape', 'espresso', 'evaporate', 'exactly', 'exchanges', 'extremely', 'fairly', 'faith', 'fall', 'fashioned', 'fast', 'fats', 'feta', 'field', 'figs', 'firms', 'fist', 'fits', 'flame', 'flank', 'flattened', 'flatter', 'floating', 'flours', 'flowerets', 'fluted', 'fondant', 'freezing', 'freshly', 'fries', 'future', 'game', 'garbanzo', 'garnishes', 'gas', 'genesis', 'giblets', 'gives', 'gloss', 'going', 'golf', 'gr', 'grains', 'grape', 'grocery', 'groundhog', 'group', 'gruyere', 'guests', 'gummy', 'had', 'haddock', 'handling', 'hang', 'hardened', 'having', 'help', 'helpings', 'helps', 'here', 'hi', 'holidays', 'hollowed', 'hors', 'hotter', 'human', 'hunger', 'incorporate', 'indirect', 'individually', 'intact', 'island', 'jelled', 'jicama', 'joy', 'juicy', 'kettles', 'kielbasa', 'kindly', 'kindness', 'kitchen', 'kiwi', 'kneading', 'knox', 'kraft', 'largest', 'lasagne', 'lastly', 'lasts', 'leak', 'lean', 'legs', 'letting', 'lighten', 'liking', 'lima', 'limburger', 'linguine', 'linguini', 'liqueur', 'll', 'loosely', 'lowest', 'luke', 'lunch', 'luncheon', 'mace', 'manufacturer', 'marjoram', 'marsala', 'maximum', 'maybe', 'melba', 'membranes', 'method', 'mexican', 'microcook', 'mince', 'mints', 'mocha', 'month', 'motion', 'mounds', 'mountain', 'mouth', 'mush', 'navy', 'neck', 'new', 'newspaper', 'nonfat', 'normally', 'oeuvre', 'oleomargarine', 'opaque', 'opening', 'option', 'original', 'orzo', 'oval', 'p', 'pale', 'palm', 'parchment', 'pattern', 'pea', 'peak', 'peaked', 'pear', 'pectin', 'perforations', 'perhaps', 'perk', 'persimmon', 'personal', 'pheasant', 'phyllo', 'pickling', 'pig', 'pineapples', 'pistachio', 'pizzas', 'pizzelle', 'pkg', 'pliable', 'plump', 'pocket', 'pointed', 'prawns', 'prayer', 'precooked', 'preheating', 'prep', 'probably', 'proceed', 'pronged', 'protein', 'provolone', 'puddings', 'puff', 'pulling', 'purple', 'quiche', 'raspberry', 'rather', 'realemon', 'regularly', 'reheat', 'remember', 'resemble', 'resistant', 'results', 'retain', 'reused', 'richer', 'rims', 'rivels', 'rock', 'roses', 'rotating', 'rotelle', 'runny', 'salads', 'saltine', 'salty', 'samuel', 'saran', 'satisfy', 'saved', 'savory', 'says', 'scalded', 'scissors', 'scoops', 'score', 'scramble', 'scrambled', 'scrub', 'seasons', 'securely', 'seem', 'select', 'self', 'separated', 'separates', 'service', 'settling', 'shaker', 'shaking', 'shears', 'shellfish', 'shiny', 'showers', 'shoyu', 'shut', 'similar', 'sink', 'sizzling', 'skillets', 'skinless', 'slab', 'slight', 'slush', 'smash', 'smearing', 'smile', 'smoothness', 'solids', 'solution', 'sooner', 'sorrow', 'spanish', 'spiced', 'spins', 'splashing', 'splenda', 'spoke', 'spots', 'spreadable', 'sprite', 'stands', 'starch', 'stars', 'steadily', 'steady', 'steamed', 'steel', 'sterilize', 'stiffen', 'stops', 'stretch', 'string', 'stuck', 'sugared', 'suit', 'supper', 'supreme', 'sweat', 'sweeten', 'sweeter', 'sympathy', 'syrupy', 'taffy', 'tail', 'taking', 'tan', 'tap', 'tapped', 'tasty', 'teflon', 'thickest', 'thighs', 'those', 'thousand', 'tied', 'tines', 'tips', 'tofu', 'toppings', 'tossed', 'tostitos', 'touching', 'toy', 'treat', 'trifle', 'truss', 'tvp', 'undercooked', 'unpeeled', 'unsweetened', 'unwrap', 'upon', 'uv', 'venison', 'vermouth', 'vessel', 'volume', 'waffles', 'wanted', 'warmer', 'watercress', 'wedge', 'whirl', 'width', 'winter', 'won', 'worms', 'wrappers', 'xxxx', 'yummy', 'zest', '01', '111', '118', '125', '12x3', '160', '175', '199', '21', '212', '225', '245', '249', '255', '26', '260', '265', '27', '270', '28', '2x15', '310', '315', '340', '344', '360', '37', '377', '405', '41', '43', '435', '44', '450f', '465', '4th', '4x', '520', '5th', '61', '633', '66', '6655', '68', '73', '800', '8x9', '93', '96', '9x1', '9x13x2', 'accompanied', 'accomplished', 'accordance', 'accordion', 'accumulate', 'accumulates', 'achieve', 'achiote', 'activate', 'active', 'acts', 'adds', 'advice', 'afford', 'afternoon', 'agent', 'alex', 'alfalfa', 'alongside', 'alot', 'alternated', 'altitude', 'ample', 'anchovies', 'anchovy', 'angling', 'angostura', 'animal', 'appears', 'appetizers', 'appropriate', 'aroma', 'arranging', 'arrowroot', 'assemble', 'assembled', 'attached', 'attachment', 'attractive', 'awareness', 'backbone', 'bagel', 'baguette', 'baller', 'bands', 'barbecued', 'barbecuing', 'baskets', 'bathroom', 'bathtub', 'battered', 'batters', 'beads', 'bearing', 'beau', 'beautiful', 'bends', 'bermuda', 'berry', 'beware', 'bias', 'bible', 'birthdays', 'bitter', 'bitterness', 'blackberry', 'blackened', 'blades', 'bland', 'blenders', 'bleu', 'bologna', 'bonnet', 'book', 'bother', 'boulder', 'bow', 'boxes', 'boys', 'braising', 'breaded', 'breadstick', 'brickle', 'bricks', 'brittle', 'broiled', 'broiling', 'brook', 'broths', 'brought', 'brownish', 'brunch', 'brushed', 'bucket', 'buckeyes', 'buds', 'burns', 'burrito', 'burst', 'butterfinger', 'buttering', 'buzz', 'cabin', 'cacao', 'california', 'call', 'caloric', 'camembert', 'camping', 'canadian', 'candles', 'canister', 'cap', 'carbohydrate', 'carbonated', 'care', 'carman', 'carve', 'casseroles', 'catch', 'cauliflowerets', 'cause', 'cavities', 'cavity', 'ceramic', 'certo', 'chair', 'changing', 'character', 'charbroil', 'charcoal', 'charred', 'cheerfulness', 'cheerios', 'cheesecake', 'chickens', 'chickpea', 'chickpeas', 'chilli', 'chillies', 'china', 'chippewa', 'chive', 'choc', 'choco', 'chopping', 'christian', 'chunkiness', 'cleaning', 'clergy', 'clip', 'clock', 'clothes', 'clump', 'clumps', 'cm', 'coal', 'cognac', 'coins', 'collins', 'combining', 'coming', 'commercial', 'community', 'companion', 'complement', 'completion', 'compotes', 'concentrated', 'cone', 'cones', 'confectionary', 'confetti', 'consider', 'consistently', 'consuming', 'content', 'continues', 'continuous', 'conventional', 'conventionally', 'cookware', 'copped', 'copper', 'corer', 'corkscrew', 'corner', 'cornsyrup', 'course', 'cow', 'crater', 'creamette', 'creamier', 'creating', 'creole', 'crinkle', 'crispies', 'crispix', 'crispness', 'croquette', 'crowded', 'crowds', 'crunchies', 'crystal', 'crystallized', 'cukes', 'cupful', 'cupfuls', 'curing', 'curls', 'currant', 'currants', 'curves', 'cutlet', 'dairy', 'damaged', 'dandelion', 'dannon', 'darker', 'date', 'decide', 'decorations', 'decrease', 'deeply', 'defrost', 'defrosted', 'dehydrated', 'delicate', 'deliciously', 'depends', 'derib', 'destem', 'develop', 'diabetic', 'diamond', 'difficult', 'dills', 'dime', 'disaster', 'disc', 'discretion', 'discussing', 'dishpan', 'disk', 'disregard', 'donut', 'doo', 'doubling', 'doughlike', 'doughnut', 'doughnuts', 'doughy', 'dozens', 'draining', 'dredging', 'dressings', 'dribbles', 'drip', 'dropper', 'drugstore', 'drumsticks', 'duck', 'duckling', 'due', 'dull', 'dusted', 'dye', 'e', 'ear', 'earth', 'eggnog', 'eggroll', 'elegant', 'element', 'eleven', 'else', 'emulsion', 'enamel', 'endive', 'enhances', 'entertaining', 'enthusiasm', 'environment', 'equipment', 'equivalent', 'ernest', 'escaping', 'especially', 'essence', 'evening', 'eventually', 'everyday', 'exception', 'exclude', 'exodus', 'expand', 'expected', 'experiment', 'explode', 'exposed', 'exterior', 'eye', 'eyed', 'eyes', 'face', 'faces', 'facilitate', 'facing', 'failed', 'falling', 'fan', 'fancy', 'farms', 'fastening', 'fatty', 'feeding', 'feel', 'feels', 'feet', 'fence', 'fettucine', 'fewer', 'fig', 'fillet', 'final', 'fingernail', 'firmer', 'flecks', 'flesh', 'flexible', 'flip', 'flounder', 'flowerpot', 'fluff', 'fluffed', 'foaming', 'folks', 'forcing', 'forever', 'forking', 'forks', 'formerly', 'fortunate', 'fourths', 'frascati', 'friend', 'fritter', 'frizzle', 'frogs', 'froth', 'fruitcake', 'fuel', 'fur', 'garbanzos', 'garnishing', 'gelatins', 'german', 'gift', 'gifts', 'gill', 'gin', 'gizzards', 'glaceed', 'gloves', 'glutamate', 'gm', 'god', 'gold', 'goose', 'gouda', 'goulash', 'graininess', 'grandkid', 'granola', 'gravel', 'gray', 'grenadine', 'grinding', 'gristle', 'grit', 'grudges', 'guiness', 'gumbo', 'hair', 'handled', 'handles', 'happy', 'harder', 'hare', 'hashbrown', 'havarti', 'he', 'heads', 'heart', 'heats', 'heaven', 'height', 'heights', 'held', 'helper', 'herbal', 'herbed', 'hers', 'hickory', 'hidden', 'hide', 'highball', 'highest', 'hinged', 'hit', 'ho', 'hock', 'holders', 'hollows', 'hook', 'horns', 'hotdogs', 'hots', 'house', 'humid', 'hungry', 'hurry', 'husband', 'husks', 'iffin', 'immerse', 'impossible', 'imprint', 'improvise', 'included', 'incorporated', 'incorporating', 'increased', 'indent', 'indentation', 'indicates', 'inexpensive', 'inject', 'inserting', 'interesting', 'intervals', 'isaiah', 'islands', 'item', 'iz', 'jambalaya', 'jellos', 'jells', 'jiggle', 'jiggly', 'job', 'joe', 'joint', 'judges', 'judgment', 'julienne', 'k', 'kabob', 'kabobs', 'kalamata', 'kale', 'kay', 'kernel', 'kernels', 'kinda', 'kinds', 'king', 'kings', 'kitten', 'knitting', 'knock', 'know', 'knows', 'knuckles', 'kosher', 'krisp', 'krrrrisp', 'lack', 'lattice', 'laughter', 'lavender', 'layed', 'layered', 'laying', 'leek', 'leg', 'length', 'letters', 'lg', 'lifted', 'lifting', 'lighter', 'limeade', 'limes', 'linen', 'lines', 'lingonberries', 'lining', 'links', 'liquidiser', 'liquidy', 'liquified', 'liquify', 'liter', 'liters', 'living', 'loan', 'longhorn', 'loosens', 'lose', 'lost', 'loved', 'lowered', 'lump', 'lumping', 'luster', 'lux', 'luxury', 'macadamia', 'maker', 'malted', 'mangoes', 'manwich', 'marbleize', 'margaritas', 'margin', 'marinates', 'marinating', 'market', 'mascarpone', 'maseca', 'mashing', 'mason', 'mat', 'mazola', 'mcbutter', 'meals', 'meatball', 'meaty', 'melon', 'membrane', 'men', 'menthe', 'menu', 'mess', 'messy', 'methods', 'mexicorn', 'micromelt', 'microwaved', 'midway', 'might', 'mike', 'mil', 'milium', 'millet', 'mingle', 'minimum', 'mins', 'mintues', 'minuets', 'mistake', 'mmmm', 'mmmmmmm', 'moderation', 'molly', 'mom', 'moments', 'monde', 'monosodium', 'monster', 'moon', 'mornings', 'mostly', 'mother', 'motor', 'mounding', 'mousse', 'mullet', 'multipurpose', 'mussels', 'nachos', 'nan', 'napkin', 'napkins', 'natural', 'nature', 'needle', 'nest', 'nestle', 'nests', 'ninths', 'nog', 'nongreased', 'northern', 'nose', 'nowadays', 'nutritious', 'object', 'observing', 'occasions', 'okay', 'omelet', 'omitting', 'opposite', 'optionals', 'ordinary', 'others', 'otherwise', 'our', 'ourselves', 'outdoor', 'overcooking', 'overlap', 'owens', 'packaging', 'packed', 'panboil', 'papered', 'parfait', 'parkay', 'pass', 'passed', 'past', 'patch', 'patout', 'paul', 'pearl', 'pebbles', 'peeking', 'peeler', 'penny', 'peppermint', 'pepsi', 'percolator', 'perfect', 'period', 'periodically', 'permeates', 'persimmons', 'philadelphia', 'picket', 'picks', 'pilaf', 'piles', 'piling', 'pillsbury', 'pipe', 'pizzelles', 'plant', 'play', 'pleaser', 'pleasers', 'pleasingly', 'plenty', 'pockets', 'pods', 'polenta', 'popper', 'pore', 'possibly', 'potluck', 'potlucks', 'pottery', 'pounder', 'pounding', 'pralines', 'prayers', 'preboil', 'present', 'preservation', 'presifted', 'problem', 'processing', 'produce', 'product', 'progresso', 'proper', 'properly', 'provence', 'proverbs', 'provide', 'prune', 'pulse', 'pushed', 'quail', 'quartered', 'r', 'rabbits', 'radish', 'raised', 'ramekin', 'range', 'rapid', 'rave', 'ravioli', 'reach', 'reached', 'reaching', 'reads', 'rearranging', 'reassemble', 'receiver', 'rechill', 'recipes', 'recommends', 'reddish', 'redness', 'refreeze', 'refresh', 'refreshing', 'refrigeration', 'rehydrate', 'relates', 'rellenos', 'remake', 'removable', 'require', 'required', 'resealable', 'reshape', 'resift', 'restaurant', 'retaining', 'reviews', 'rib', 'ribbon', 'rices', 'rinds', 'rinses', 'risen', 'rising', 'rod', 'romano', 'root', 'ropes', 'rosettes', 'rough', 'roughy', 'rouille', 'rounder', 'rounding', 'rusty', 'sack', 'safflower', 'saifon', 'salts', 'sam', 'sandies', 'sangria', 'satin', 'saturated', 'saturday', 'saucepans', 'sauteing', 'saver', 'scalloped', 'scorch', 'scored', 'scramblers', 'scraps', 'scratch', 'screw', 'screwed', 'screwtop', 'scum', 'sec', 'seedless', 'sense', 'separating', 'settle', 'seven', 'severing', 'sewing', 'shakes', 'shalloww', 'shanks', 'shavings', 'sheen', 'shelled', 'sherbert', 'shift', 'shin', 'shingles', 'shoots', 'short', 'shortcake', 'shortenings', 'shortest', 'shortly', 'shot', 'shovel', 'showing', 'shown', 'shreds', 'sieved', 'sifter', 'sight', 'silly', 'simmers', 'simple', 'since', 'sinking', 'sir', 'sirloin', 'sits', 'skimmed', 'skimming', 'skinned', 'skor', 'sky', 'sleep', 'slicer', 'slimmer', 'slip', 'slivered', 'slivers', 'sloppy', 'slump', 'slurpy', 'slushy', 'smallest', 'smashing', 'smear', 'smoker', 'smokies', 'smoking', 'smoothly', 'smother', 'smothering', 'snacks', 'snakes', 'snap', 'snifter', 'soap', 'soba', 'sociables', 'softer', 'softly', 'soggy', 'sole', 'solomon', 'somewhat', 'sons', 'sooo', 'souls', 'sound', 'sounds', 'sourdough', 'spaces', 'spade', 'spam', 'spanakopeta', 'spareribs', 'sparingly', 'sparkle', 'specific', 'spicy', 'spill', 'spoiled', 'spot', 'spree', 'springerle', 'squeezed', 'squirrels', 'squirt', 'st', 'stacked', 'stainless', 'stakes', 'stalk', 'starved', 'state', 'steal', 'steaming', 'steps', 'stews', 'stickiness', 'stocks', 'stones', 'stopped', 'stopping', 'straight', 'strands', 'straw', 'stri', 'stroganoff', 'study', 'submerge', 'submerged', 'substitutes', 'sufficient', 'summer', 'sundae', 'sunday', 'surfaces', 'surprise', 'surprised', 'swanson', 'swedish', 'swirls', 'switch', 'swollen', 'syrups', 'tabletop', 'tamale', 'tamari', 'tang', 'tartness', 'tatar', 'technical', 'technique', 'telephone', 'tell', 'tells', 'temp', 'tempera', 'ten', 'tenderize', 'tenderizer', 'tenderness', 'tennis', 'tepid', 'term', 'terrific', 'terry', 'tested', 'thanksgiving', 'thaws', 'thickly', 'thins', 'though', 'threads', 'throughly', 'throwing', 'thumbprint', 'til', 'tilt', 'tilting', 'tinged', 'toaster', 'toasty', 'tolerance', 'tonic', 'topper', 'torn', 'tostados', 'totally', 'toughening', 'toweling', 'trace', 'triple', 'triples', 'triscuits', 'tropicana', 'trout', 'trowel', 'true', 'tub', 'tubular', 'tucking', 'tuning', 'tunnel', 'turner', 'turnips', 'twelve', 'twenty', 'tying', 'typical', 'unavailable', 'unbuttered', 'uncoated', 'uncovering', 'uncut', 'undercook', 'underneath', 'undissolved', 'unevenly', 'unheated', 'uniform', 'unmelted', 'unmolded', 'unpleasantness', 'unpricked', 'unsalted', 'unsifted', 'unused', 'upper', 'ups', 'useful', 'uses', 'utility', 'valley', 'vaporized', 'variations', 'vary', 'veg', 'veggie', 'vein', 'velvety', 'vented', 'vents', 'vienna', 'vinaigrette', 'visible', 'vitamin', 'vodka', 'w', 'waffle', 'wall', 'watching', 'watermelon', 'waters', 'watery', 'wattage', 'waverly', 'ways', 'weaving', 'weighted', 'were', 'wheatsworth', 'wheels', 'where', 'whether', 'whichever', 'whisking', 'whoop', 'wid', 'wiener', 'wiggly', 'wil', 'wilt', 'wing', 'wonder', 'wonderful', 'wonders', 'wonton', 'workable', 'worked', 'world', 'worth', 'wow', 'wrapping', 'wraps', 'wring', 'written', 'year', 'yet', 'zag', 'zeppole', 'zesty', 'zig', 'zipper', 'zippered', 'zita']\n",
            "1325\n",
            "['<unk>', '<pad>', '<BOS>', '<EOS>', ';', 'sugar', 'salt', 'cream', 'cheese', 'flour', 'butter', 'eggs', 'milk', 'onion', 'pepper', 'water', 'vanilla', 'baking', 'chicken', 'powder', 'sauce', 'oil', 'green', 'soup', 'egg', 'soda', 'garlic', 'margarine', 'ground', 'juice', 'brown', 'celery', 'of', 'cinnamon', 'lemon', 'beef', 'onions', 'pineapple', 'nuts', 'chocolate', 'sour', 'vinegar', 'tomato', 'tomatoes', 'white', 'corn', 'pecans', 'cheddar', 'potatoes', 'bread', 'mix', 'mayonnaise', 'beans', 'rice', 'crumbs', 'red', 'mustard', 'mushroom', 'shortening', 'chips', 'parsley', 'cake', 'frozen', 'worcestershire', 'mushrooms', 'orange', 'sweet', 'coconut', 'powdered', 'nutmeg', 'fresh', 'parmesan', 'carrots', 'bacon', 'black', 'cocoa', 'broccoli', 'buttermilk', 'peanut', 'vegetable', 'raisins', 'pie', 'pudding', 'oleo', 'chili', 'boiling', 'broth', 'cornstarch', 'walnuts', 'pork', 'cracker', 'extract', 'ginger', 'syrup', 'bananas', 'oregano', 'apples', 'breasts', 'yellow', 'graham', 'cloves', 'condensed', 'paprika', 'peppers', 'salad', 'soy', 'marshmallows', 'instant', 'bell', 'jello', 'mozzarella', 'clove', 'noodles', 'strawberries', 'crackers', 'sausage', 'cold', 'olive', 'whole', 'light', 'olives', 'cooking', 'cherries', 'macaroni', 'cabbage', 'dressing', 'dry', 'hamburger', 'hot', 'grated', 'italian', 'catsup', 'wine', 'cherry', 'shell', 'confectioners', 'pumpkin', 'yeast', 'apple', 'filling', 'oranges', 'flakes', 'crisco', 'whites', 'basil', 'oats', 'peas', 'almonds', 'ham', 'all', 'seasoning', 'crust', 'cottage', 'ketchup', 'zucchini', 'paste', 'allspice', 'semi', 'shredded', 'dill', 'fruit', 'honey', 'strawberry', 'lime', 'meat', 'bay', 'shrimp', 'velveeta', 'almond', 'swiss', 'flavoring', 'yolks', 'bouillon', 'purpose', 'gelatin', 'lean', 'spaghetti', 'cider', 'kernel', 'kidney', 'lettuce', 'whipping', 'peanuts', 'taco', 'tartar', 'oatmeal', 'style', 'bisquick', 'thyme', 'meal', 'o', 'peaches', 'pimento', 'whipped', 'mixed', 'stuffing', 'chestnuts', 'chilies', 'mandarin', 'rolls', 'cayenne', 'chuck', 'dates', 'seed', 'topping', 'carrot', 'cocktail', 'molasses', 'cucumbers', 'leaf', 'maraschino', 'tortillas', 'vegetables', 'candied', 'cumin', 'heavy', 'ale', 'american', 'halves', 'spinach', 'tabasco', 'cauliflower', 'ice', 'liquid', 'sharp', 'wafers', 'yogurt', 'salsa', 'cubes', 'pecan', 'regular', 's', 'chops', 'cornmeal', 'cranberry', 'crescent', 'head', 'sherry', 'stalks', 'cereal', 'coffee', 'curry', 'food', 'leaves', 'flaked', 'pasta', 'potato', 'applesauce', 'cranberries', 'fat', 'pastry', 'rind', 'salmon', 'slices', 'weed', 'apricot', 'blueberries', 'butterscotch', 'coloring', 'horseradish', 'nonfat', 'shells', 'tuna', 'whip', 'barbecue', 'preserves', 'spice', 'tortilla', 'unsweetened', 'wesson', 'pickle', 'wheat', 'cookies', 'flavored', 'miracle', 'pinto', 'ro', 'stew', 'tel', 'turkey', 'warm', 'karo', 'pizza', 'ricotta', 'sherbet', 'steak', 'bits', 'elbow', 'low', 'pimentos', 'raspberry', 'rosemary', 'sesame', 'banana', 'campbell', 'crabmeat', 'golden', 'krispies', 'morsels', 'sauerkraut', 'thin', 'crunchy', 'dream', 'freshly', 'jell', 'lemonade', 'marshmallow', 'pepperoni', 'rum', 'stock', 'cornflakes', 'crab', 'cucumber', 'hash', 'herb', 'ripe', 'sage', 'squash', 'tarragon', 'turmeric', 'unflavored', 'biscuit', 'chives', 'grapes', 'jack', 'mashed', 'paraffin', 'pet', 'poultry', 'angel', 'bean', 'boneless', 'cornbread', 'corned', 'fryer', 'lard', 'pickles', 'poppy', 'relish', 'unsalted', 'accent', 'baby', 'chunky', 'concentrate', 'croutons', 'jelly', 'lima', 'marjoram', 'monterey', 'peel', 'pretzels', 'rye', 'smoke', 'spray', 'avocado', 'buns', 'buttered', 'cilantro', 'grape', 'lemons', 'long', 'pistachio', 'provolone', 'seasoned', 'seeds', 'avocados', 'flavor', 'jalapeno', 'maple', 'mayo', 'philadelphia', 'picante', 'pickling', 'roast', 'roll', 'slivered', 'substitute', 'breast', 'browns', 'carnation', 'cooked', 'crusts', 'fish', 'grain', 'sprouts', 'yolk', 'your', 'beer', 'biscuits', 'chopped', 'clams', 'creme', 'enchilada', 'extra', 'muffin', 'n', 'oreo', 'pimientos', 'prunes', 'purple', 'rhubarb', 'skinless', 'tea', 'bourbon', 'caramel', 'caraway', 'cheerios', 'chiles', 'containers', 'english', 'flowerets', 'gravy', 'hearts', 'lasagna', 'oysters', 'pieces', 'pimiento', 'plain', 'rolled', 'root', 'scallions', 'stems', 'stove', 'top', 'unbaked', 'wide', 'bone', 'bran', 'candy', 'caramels', 'choice', 'cool', 'doritos', 'eggplant', 'favorite', 'granules', 'grits', 'hominy', 'kisses', 'mace', 'okra', 'peach', 'peppercorns', 'radishes', 'raspberries', 'romaine', 'round', 'sirloin', 'spanish', 'supreme', 'tater', 'tenderloin', 'vodka', 'wax', 'anise', 'balsamic', 'blueberry', 'bowl', 'breadcrumbs', 'bulk', 'creamer', 'crisp', 'dip', 'drippings', 'gelatine', 'greens', 'hamburg', 'meats', 'nondairy', 'nut', 'pear', 'ramen', 'season', 'strips', 'summer', 'sunflower', 'tidbits', 'veg', 'active', 'alum', 'beets', 'ben', 'brandy', 'brownie', 'club', 'coarse', 'colored', 'cookie', 'country', 'crushed', 'dough', 'fillets', 'from', 'guacamole', 'herbs', 'imitation', 'layer', 'lite', 'mint', 'monde', 'northern', 'oyster', 'packets', 'party', 'pearl', 'peeled', 'pulp', 'shoulder', 'spread', 'tart', 'uncle', 'vermicelli', 'wedges', 'wild', 'amount', 'artichokes', 'asparagus', 'bar', 'bark', 'berries', 'bottles', 'brisket', 'butterfinger', 'butternut', 'cans', 'capers', 'cashews', 'clam', 'consomme', 'cutlets', 'dark', 'deli', 'dinner', 'dried', 'equal', 'florets', 'french', 'granulated', 'grease', 'hard', 'jalapenos', 'lamb', 'longhorn', 'nectar', 'new', 'nonstick', 'nutmeats', 'parkay', 'pears', 'popcorn', 'refrigerator', 'romano', 'rotini', 'salami', 'saltine', 'sandwich', 'shallots', 'slaw', 'slice', 'snow', 'sweetener', 'tapioca', 'unpeeled', 'valley', 'very', 'virgin', 'wafer', 'acting', 'anchovy', 'apricots', 'bermuda', 'california', 'candies', 'chinese', 'cole', 'crystals', 'cube', 'currant', 'cuts', 'double', 'duncan', 'eyed', 'fillet', 'firm', 'flank', 'flounder', 'frankfurters', 'franks', 'fritos', 'frosting', 'fudge', 'germ', 'haddock', 'irish', 'jam', 'jars', 'kind', 'leeks', 'lentils', 'loin', 'lump', 'marmalade', 'medium', 'navy', 'no', 'or', 'oreos', 'pack', 'peppermint', 'pink', 'pretzel', 'raw', 'rings', 'scallops', 'seasonings', 'sprigs', 'stack', 'veal', 'walnut', 'whiskey', 'wieners', 'with', 'and', 'any', 'bagels', 'barley', 'bars', 'bite', 'blackberries', 'cakes', 'canola', 'chickens', 'chip', 'chunk', 'chunks', 'clear', 'cola', 'coriander', 'crawfish', 'currants', 'deer', 'delicious', 'drink', 'flake', 'flowers', 'garbanzo', 'glass', 'glaze', 'grahams', 'great', 'gummy', 'hawaiian', 'hens', 'hidden', 'icing', 'junior', 'kahlua', 'kitchen', 'linguine', 'liqueur', 'lobster', 'mango', 'marsala', 'mostaccioli', 'muffins', 'nacho', 'niblets', 'original', 'orzo', 'pasteurized', 'pea', 'peg', 'persimmon', 'pitted', 'pods', 'portion', 'prepared', 'quick', 'ready', 'rigatoni', 'shoyu', 'size', 'sliced', 'smooth', 'solid', 'sorghum', 'spareribs', 'splenda', 'split', 'sprinkles', 'stalk', 'sticks', 't', 'tails', 'teriyaki', 'tortellini', 'veggies', 'worms', 'xxxx', 'yams', 'alfalfa', 'anchovies', 'artichoke', 'arugula', 'au', 'bake', 'bamboo', 'base', 'batter', 'bitters', 'bittersweet', 'blackberry', 'blanched', 'blend', 'bones', 'boston', 'bottle', 'brien', 'c', 'cajun', 'canned', 'capful', 'cardamom', 'chachere', 'chickpeas', 'children', 'chile', 'chillies', 'chipped', 'chitterlings', 'chutney', 'citron', 'coca', 'cognac', 'colby', 'comino', 'consideration', 'cook', 'cornflake', 'cornish', 'cracked', 'cracklins', 'creole', 'crumb', 'curls', 'cut', 'deeds', 'dew', 'dijon', 'dog', 'dogs', 'drops', 'feta', 'fettucine', 'fettucini', 'fine', 'fluid', 'fried', 'fruits', 'fully', 'garbanzos', 'garden', 'generous', 'gold', 'grand', 'grapefruit', 'grenadine', 'griffin', 'gruyere', 'gumbo', 'gumdrops', 'half', 'heads', 'healthy', 'hen', 'hocks', 'hormel', 'hots', 'jicama', 'jus', 'knorr', 'kosher', 'ladyfingers', 'limas', 'linguini', 'liter', 'marnier', 'medal', 'mein', 'melted', 'mexicorn', 'mini', 'minute', 'natural', 'packed', 'patties', 'peck', 'penne', 'peppercorn', 'pepperoncini', 'per', 'person', 'phyllo', 'pound', 'process', 'puffed', 'puree', 'quartered', 'ranch', 'ritz', 'rotelle', 'roughy', 'sack', 'saffron', 'salted', 'sausages', 'sec', 'segments', 'shin', 'shoots', 'smoked', 'sparkling', 'spears', 'spices', 'spiral', 'spring', 'steaks', 'steamed', 'stewing', 'stick', 'swans', 'sweetened', 'tamales', 'tender', 'thawed', 'tiny', 'tip', 'tofu', 'torn', 'triple', 'tubs', 'turnips', 'twinkies', 'unbeaten', 'understanding', 'watercress', 'watermelon', 'well', 'whitefish', 'wrappers', 'zest', 'zinfandel', 'a', 'achiote', 'acorn', 'additional', 'ajinomoto', 'alaga', 'alfredo', 'also', 'anisette', 'another', 'arborio', 'armour', 'aromatic', 'arrowroot', 'bac', 'backfin', 'baguette', 'baked', 'baker', 'bakon', 'basic', 'basmati', 'batch', 'bayleaf', 'bbq', 'beaters', 'beefy', 'beet', 'benne', 'bing', 'bird', 'birds', 'bitter', 'blintzes', 'blossoms', 'blueing', 'bologna', 'bon', 'boned', 'borden', 'bottled', 'bouquet', 'box', 'boxes', 'boyardee', 'braunschweiger', 'breadsticks', 'breakfast', 'breyers', 'broiler', 'brook', 'brussels', 'bugles', 'bunches', 'burgundy', 'button', 'cabbages', 'cacao', 'caloric', 'calorie', 'calves', 'camembert', 'canadian', 'candid', 'canning', 'cantaloupe', 'cantaloupes', 'cap', 'cardamon', 'carman', 'cartons', 'castor', 'catfish', 'cauliflowerets', 'certo', 'chablis', 'champagne', 'cheez', 'cherrystones', 'chick', 'chilis', 'chilled', 'chilli', 'choc', 'choco', 'chop', 'chorizo', 'chowder', 'chronicles', 'citrus', 'cleaned', 'clump', 'coal', 'coated', 'coating', 'colada', 'collard', 'color', 'colorful', 'column', 'combination', 'commerical', 'confectioner', 'consomm', 'container', 'cooled', 'couple', 'couscous', 'crackling', 'creamed', 'creamette', 'creamy', 'cremora', 'crepes', 'crispies', 'crispins', 'crisply', 'croquette', 'crouton', 'crsico', 'crunchies', 'crush', 'crystal', 'crystallized', 'cubed', 'cukes', 'curly', 'custard', 'dandelion', 'dashes', 'de', 'del', 'deluxe', 'dessert', 'dipping', 'drumsticks', 'duck', 'dumpling', 'dusting', 'eastern', 'eggnog', 'eggplants', 'envelope', 'espresso', 'faults', 'faux', 'feet', 'fennel', 'fettuccini', 'fiber', 'field', 'fifth', 'figard', 'figs', 'filet', 'filets', 'fixins', 'flaky', 'flax', 'floured', 'fluff', 'foods', 'for', 'forest', 'four', 'frango', 'freeze', 'friendly', 'fry', 'full', 'galliano', 'game', 'gherkin', 'giardiniera', 'glutamate', 'glycerine', 'goat', 'good', 'gouda', 'granola', 'granuales', 'grassy', 'groundhog', 'hair', 'halibut', 'halved', 'harina', 'hashbrowns', 'havarti', 'hearty', 'heath', 'herbes', 'hi', 'hickory', 'hines', 'hock', 'home', 'homemade', 'husks', 'i', 'iceberg', 'ida', 'included', 'ingredients', 'jel', 'jeremiah', 'jerk', 'jigger', 'joe', 'jucie', 'junket', 'just', 'k', 'kaiser', 'kale', 'kamaboko', 'kellogg', 'kennel', 'kernels', 'kielbasa', 'king', 'kiwis', 'kolbassi', 'langostinos', 'large', 'lasagne', 'lavender', 'layers', 'leftover', 'level', 'lily', 'limeade', 'limes', 'line', 'lipton', 'liquor', 'liters', 'liver', 'loaves', 'lopez', 'lots', 'macadamia', 'madeira', 'magnolia', 'manwich', 'maria', 'marinade', 'masa', 'mascarpone', 'match', 'mcbutter', 'meatless', 'meaty', 'medley', 'melting', 'meringue', 'mexican', 'meyers', 'milky', 'millet', 'mincemeat', 'mineral', 'miniature', 'minor', 'mixture', 'mock', 'moist', 'monosodium', 'monte', 'morton', 'mountain', 'muenster', 'mueselix', 'mullet', 'multi', 'nachos', 'nahum', 'nestle', 'nestles', 'non', 'nonhomogenized', 'noodle', 'nuggets', 'numbers', 'oat', 'oleomargarine', 'os', 'other', 'others', 'oven', 'overripe', 'owens', 'own', 'packet', 'pads', 'pan', 'pancake', 'pared', 'parsnip', 'parsnips', 'parts', 'pastries', 'pate', 'pats', 'patty', 'pebbles', 'pectin', 'pemento', 'peppermints', 'pepsi', 'perch', 'persimmons', 'picks', 'pig', 'pina', 'planters', 'plastic', 'playing', 'plus', 'points', 'pop', 'porterhouse', 'prayers', 'prune', 'puffs', 'punch', 'pwd', 'q', 'rabbits', 'ravioli', 'real', 'realemon', 'recipe', 'reduced', 'reese', 'rhine', 'rhodes', 'ribs', 'rich', 'ring', 'rinsed', 'roasted', 'rods', 'roni', 'rose', 'rutabaga', 'sacrifice', 'safflower', 'saifon', 'sandies', 'savers', 'scallion', 'schilling', 'scotch', 'scramblers', 'seafood', 'seashell', 'secret', 'sections', 'seeded', 'seedless', 'serrano', 'serving', 'shallot', 'shank', 'shaped', 'shavings', 'shellfish', 'shoe', 'shoepeg', 'shortbread', 'shot', 'skins', 'snickers', 'snipped', 'soap', 'soba', 'soft', 'softened', 'sole', 'some', 'sourdough', 'southern', 'spatini', 'special', 'speck', 'spicy', 'spirals', 'spoonful', 'spoons', 'sprinkling', 'sprite', 'squares', 'squeezed', 'squirrels', 'squirt', 'squish', 'stale', 'starch', 'starter', 'state', 'stewed', 'stir', 'stocks', 'stout', 'strained', 'string', 'stroganoff', 'stuffed', 'sucaryl', 'suet', 'suey', 'sumer', 'sundae', 'tablets', 'tamari', 'tandoor', 'tap', 'taragon', 'tenders', 'texas', 'thank', 'thick', 'thighs', 'three', 'tia', 'tightly', 'time', 'toast', 'toasted', 'tobacco', 'toes', 'together', 'tomatoe', 'tonight', 'tooth', 'tops', 'tostitos', 'tricolor', 'trout', 'tsp', 'turtle', 'twirls', 'unbleached', 'unbolted', 'uncooked', 'undiluted', 'unprocessed', 'unshelled', 'varieties', 'vegetarian', 'venison', 'vidalia', 'watchers', 'waverly', 'weight', 'wheel', 'whiz', 'will', 'winter', 'wip', 'wish', 'wishbone', 'without', 'wonton', 'worcester', 'words', 'wraps', 'york', 'you', 'young', 'zesta', 'zita']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sequence(model, max_len=1000, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "    \n",
        "    inp = torch.Tensor([baseline_vocab_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None\n",
        "    step = 1\n",
        "\n",
        "    for c in range(max_len):\n",
        "          output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "          output_dist = output.data.view(-1).div(temperature).exp()\n",
        "          top = int(torch.multinomial(output_dist, 1)[0])\n",
        "\n",
        "          predicted_char = baseline_vocab_itos[top]\n",
        "\n",
        "          if predicted_char == \"<pad>\":\n",
        "              continue\n",
        "\n",
        "          if predicted_char == \"<BOS>\":\n",
        "              continue\n",
        "          \n",
        "          if predicted_char == \"<unk>\":\n",
        "              continue\n",
        "\n",
        "          if predicted_char == \";\":\n",
        "              step += 1\n",
        "              predicted_char = str(\"\\n \" + str(step) + \".\")\n",
        "\n",
        "          if predicted_char == \"<EOS>\":\n",
        "              break\n",
        "\n",
        "          generated_sequence += predicted_char + \" \"\n",
        "          inp = torch.Tensor([top]).long()\n",
        "\n",
        "    return generated_sequence"
      ],
      "metadata": {
        "id": "nv5xBtwsdSn-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TESTING"
      ],
      "metadata": {
        "id": "1OHiuD1YM8YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(descriminator, generator, data, lr, batch_size, num_epochs):\n",
        "  d_optimizer = optim.Adam(descriminator.parameters(), lr)\n",
        "  g_optimizer = optim.Adam(generator.parameters(), lr)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  train_data = data\n",
        "  #train_loader = train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  data_iter = torchtext.legacy.data.BucketIterator(data, batch_size=batch_size, sort_key=lambda x: len(x.directions), sort_within_batch=True)\n",
        "\n",
        "  samples = []\n",
        "  losses = []\n",
        "\n",
        "  iter = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    descriminator.train()\n",
        "    generator.train()\n",
        "\n",
        "    avg_loss = 0\n",
        "    min_loss = float('inf')\n",
        "    print\n",
        "\n",
        "    for (directions, lengths), ingredients in data_iter:\n",
        "   # for batch_i,  real_recipes,ingredients) in enumerate(train_data['directions'],train_data['ingredients']):\n",
        "            real_directions = directions[:, :-1]\n",
        "            # print(real_directions)\n",
        "\n",
        "            # # batch_size = real_recipes.size(0)\n",
        "\n",
        "            # # === Train the Discriminator ===\n",
        "            \n",
        "            # d_optimizer.zero_grad()\n",
        "\n",
        "            # # discriminator losses on real recipe \n",
        "          \n",
        "            # D_real = descriminator(real_directions)\n",
        "            # labels = torch.ones(batch_size)\n",
        "\n",
        "           \n",
        "            # D_real= sum(D_real)/D_real.shape[0]\n",
        "            # d_real_loss = criterion(D_real, labels)\n",
        "\n",
        "            \n",
        "            # # discriminator losses on fake recipe\n",
        "            # # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # # z = torch.from_numpy(z).float()\n",
        "            \n",
        "            # ingredients = torch.Tensor(ingredients) \n",
        "            # ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            # fake_recipes = generator(ingredients)\n",
        "\n",
        "            # fake_recipes = torch.Tensor(fake_recipes) \n",
        "            # fake_recipes = fake_recipes.to(torch.long)\n",
        "            # D_fake = descriminator(fake_recipes)\n",
        "            \n",
        "            # #labels = torch.zeros(batch_size) # fake labels = 0\n",
        "            # labelsD = torch.zeros(1)\n",
        "            # labelsD = torch.diag(labelsD,0)\n",
        "            # d_fake_loss = criterion(D_fake, labelsD)\n",
        "            \n",
        "            # # add up losses and update parameters\n",
        "            # d_loss = d_real_loss + d_fake_loss\n",
        "            # d_loss.backward()\n",
        "            # d_optimizer.step()\n",
        "\n",
        "\n",
        "            #  # === Train the Generator ===\n",
        "            # g_optimizer.zero_grad()\n",
        "            \n",
        "            # # generator losses on fake images\n",
        "            # # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # # z = torch.from_numpy(z).float()\n",
        "\n",
        "            # ingredients = torch.Tensor(ingredients) \n",
        "            # ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            # fake_recipes = generator(ingredients)\n",
        "\n",
        "            # fake_recipes = torch.Tensor(fake_recipes) \n",
        "            # fake_recipes = fake_recipes.to(torch.long)\n",
        "\n",
        "            # D_fake = descriminator(fake_recipes)\n",
        "            # #labels = torch.ones(batch_size) #flipped labels\n",
        "\n",
        "\n",
        "            # labels = torch.ones(batch_size)\n",
        "            # # compute loss and update parameters\n",
        "            # g_loss = criterion(D_fake,labels)\n",
        "            # g_loss.backward()\n",
        "            # g_optimizer.step()\n",
        "\n",
        "\n",
        "            # print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "            #     % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # # append discriminator loss and generator loss\n",
        "            # losses.append((d_loss.item(), g_loss.item()))\n",
        "            target = directions[:, 1:]\n",
        "            inp = directions[:, :-1]\n",
        "            \n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            output, _ = generator(inp)\n",
        "            loss = criterion(output.reshape(-1, baseline_vocab_size), target.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            iter += 1\n",
        "            losses.append(float(loss))\n",
        "            samples.append(iter)\n",
        "\n",
        "            if iter % 200 == 0:\n",
        "                  print(\"Iteration # %d: Loss %f\" % (iter+1, float(avg_loss/200)))\n",
        "                  print(\"Generated Recipe: \\n 1. \" + sample_sequence(generator, 1000, 1.5))\n",
        "                  avg_loss = 0\n",
        "\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "NtBl3mWA1OKd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CURRENT VERSION"
      ],
      "metadata": {
        "id": "UigeC_m7NCa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(discriminator, generator, data, lr, batch_size, num_epochs, max_recipe_len):\n",
        "  d_optimizer = optim.Adam(discriminator.parameters(), lr)\n",
        "  g_optimizer = optim.Adam(generator.parameters(), lr)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "  \n",
        "  train_data = data\n",
        "  #train_loader = train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  data_iter = torchtext.legacy.data.BucketIterator(data, batch_size=batch_size, sort_key=lambda x: len(x.directions), sort_within_batch=True)\n",
        "\n",
        "  samples = []\n",
        "  losses = []\n",
        "\n",
        "  iter = 0\n",
        "  print(len(data_iter))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    avg_loss = 0\n",
        "    min_loss = float('inf')\n",
        "\n",
        "    for (directions, lengths), ingredients in data_iter:\n",
        "        target = directions[:, 1:]\n",
        "        inp = directions[:, :-1]\n",
        "\n",
        "        # Zero the gradients\n",
        "        g_optimizer.zero_grad()\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        # -------- Training the Generator ---------#\n",
        "        # Generate a fake recipe\n",
        "        # The real directions are being used as the input for the generator\n",
        "        # This will need to be changed to the list of ingredients but for\n",
        "        # now it acts as \"random noise\"\n",
        "        first_word = torch.Tensor([baseline_vocab_stoi[\"<BOS>\"]]).long()\n",
        "        hidden = None\n",
        "        fake_recipe = torch.empty(max_recipe_len)\n",
        "\n",
        "        for c in range(max_recipe_len):\n",
        "            output, hidden = generator(first_word.unsqueeze(0), hidden)\n",
        "            output_dist = output.data.view(-1).div(1.5).exp()\n",
        "            top = int(torch.multinomial(output_dist, 1)[0])\n",
        "            fake_recipe[c] = top\n",
        "        fake_recipe = fake_recipe.long()\n",
        "        print(\"-------------\")\n",
        "        print(fake_recipe)\n",
        "\n",
        "        # Get the discriminator to make a prediction on whether the generated\n",
        "        # recipe is real or fake\n",
        "        D_fake = discriminator(inp, None)\n",
        "\n",
        "        # Calculate the loss for the generator\n",
        "        labels = torch.ones(batch_size)\n",
        "        g_loss = criterion(D_fake,labels)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # -------- Training the Discriminator  ---------#\n",
        "        # Discriminator losses on real recipes\n",
        "        D_real = discriminator(inp, None)\n",
        "        labels = torch.ones(batch_size-1)\n",
        "\n",
        "        D_real.type(torch.LongTensor)\n",
        "        labels.type(torch.LongTensor)\n",
        "           \n",
        "        D_real= sum(D_real)/D_real.shape[0]\n",
        "        d_real_loss = criterion(D_real, labels.long())\n",
        "\n",
        "        # Discriminator losses on fake recipes\n",
        "        fake_recipe, _ = generator(inp)\n",
        "        D_fake = discriminator(inp)\n",
        "        labels = torch.zeros(batch_size)\n",
        "        d_fake_loss = criterion(D_fake, labels)\n",
        "\n",
        "        # Add up the losses and update parameters\n",
        "        # (Some sources say to average the losses but\n",
        "        # in the tutorial they just add them together)\n",
        "        d_loss = (d_fake_loss + d_real_loss) / 2\n",
        "        d_loss.backward()\n",
        "        d_loss.step()\n",
        "\n",
        "\n",
        "\n",
        "        #loss = criterion(fake_recipes.reshape(-1, baseline_vocab_size), target.reshape(-1))\n",
        "        \n",
        "        losses.append(float(d_loss))\n",
        "        samples.append(iter)\n",
        "\n",
        "        iter += 1\n",
        "        if iter % 10 == 0:\n",
        "            print(\"Iteration # %d:\" % (iter+1))\n",
        "            print(\"Generated Recipe: \\n 1. \" + sample_sequence(generator, 1000, 1.5))\n",
        "\n",
        "        # === Train the Discriminator ===\n",
        "\n",
        "        \n",
        "\n",
        "            \n",
        "            # # discriminator losses on fake recipe\n",
        "            # # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # # z = torch.from_numpy(z).float()\n",
        "            \n",
        "            # ingredients = torch.Tensor(ingredients) \n",
        "            # ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            # fake_recipes = generator(ingredients)\n",
        "\n",
        "            # fake_recipes = torch.Tensor(fake_recipes) \n",
        "            # fake_recipes = fake_recipes.to(torch.long)\n",
        "            # D_fake = descriminator(fake_recipes)\n",
        "            \n",
        "            # #labels = torch.zeros(batch_size) # fake labels = 0\n",
        "            # labelsD = torch.zeros(1)\n",
        "            # labelsD = torch.diag(labelsD,0)\n",
        "            # d_fake_loss = criterion(D_fake, labelsD)\n",
        "            \n",
        "            # # add up losses and update parameters\n",
        "            # d_loss = d_real_loss + d_fake_loss\n",
        "            # d_loss.backward()\n",
        "            # d_optimizer.step()\n",
        "\n",
        "\n",
        "            #  # === Train the Generator ===\n",
        "            # g_optimizer.zero_grad()\n",
        "            \n",
        "            # # generator losses on fake images\n",
        "            # # z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
        "            # # z = torch.from_numpy(z).float()\n",
        "\n",
        "            # ingredients = torch.Tensor(ingredients) \n",
        "            # ingredients = ingredients.to(torch.long)\n",
        "\n",
        "            # fake_recipes = generator(ingredients)\n",
        "\n",
        "            # fake_recipes = torch.Tensor(fake_recipes) \n",
        "            # fake_recipes = fake_recipes.to(torch.long)\n",
        "\n",
        "            # D_fake = descriminator(fake_recipes)\n",
        "            # #labels = torch.ones(batch_size) #flipped labels\n",
        "\n",
        "\n",
        "            # labels = torch.ones(batch_size)\n",
        "            # # compute loss and update parameters\n",
        "            # g_loss = criterion(D_fake,labels)\n",
        "            # g_loss.backward()\n",
        "            # g_optimizer.step()\n",
        "\n",
        "\n",
        "            # print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
        "            #     % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # # append discriminator loss and generator loss\n",
        "            # losses.append((d_loss.item(), g_loss.item()))\n",
        "            \n",
        "            \n",
        "        \n",
        "\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "t_QsF4JcNfPl"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discriminator(baseline_vocab_size, 64, n_layers=1)\n",
        "gen = Generator(baseline_vocab_size, 64, n_layers=1)\n",
        "losses = train_GAN(disc, gen, baseline_data, lr=lr, batch_size=3, num_epochs=num_epochs, max_recipe_len=max_recipe_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xzYaUvBf3F1L",
        "outputId": "d726e311-a845-4e6b-c613-5a00c9f29b35"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1667\n",
            "-------------\n",
            "tensor([1479, 1574, 2763,  952, 3448, 1373, 1612, 2094,  343,  481, 1279, 3161,\n",
            "         798, 3039, 2783, 2362,  915, 2151, 1334, 1376, 2480, 3120,  361, 3306,\n",
            "        3431,  694, 1428,  405,  407,  122, 3108, 1722, 2105, 3141,  966, 1440,\n",
            "          54, 1485,   38, 2728, 1757, 2590, 3466, 2377, 1015,  607, 1181, 3281,\n",
            "        2427, 1484,  148, 2762,  532, 1674, 2477, 1397, 3388, 1064, 1036, 1765,\n",
            "          45, 1810, 3242, 3015,   20, 1717, 3202, 3150,  725, 1536, 2567, 3326,\n",
            "        2819, 1606, 2055, 2688, 2839,  972, 2449, 2475, 2920, 1249,  811, 2962,\n",
            "        3004, 3301, 2120,   24, 3454, 3032, 2019, 2225,  168,  525, 1128,  167,\n",
            "         632, 2491,  665, 1620, 2012, 1105, 2067,  320, 1446, 1871,  624, 1296,\n",
            "        2521, 1566,  187, 2878,  417, 2650,  482, 3311, 2334, 2935,  931, 2952,\n",
            "        1506, 1761,  827, 1810, 1793, 3113, 1203,  572, 2643, 2921, 1240,   39,\n",
            "        1810, 1431, 2314, 2576, 2711, 2910, 3500, 1254, 1702, 2006,  962, 2901,\n",
            "        1092,  947, 2635, 1069, 1032, 3491, 2323, 2702, 2693, 3149,  494,  544,\n",
            "        2069, 1447, 2271,  353, 2728, 1928, 2601, 3486, 1234,  430, 2785, 2499,\n",
            "         794, 2808,  447,  976, 2611, 1793, 1742,  618,  421,  401,   29, 1253,\n",
            "        2411,  456, 1796, 1820, 1408, 1530, 2014, 3255, 1586, 2195, 2491, 1414,\n",
            "        1140, 1998, 3220, 1133,  647,   61, 1984,  579,  389,  134, 1652,  967,\n",
            "        1715,  383, 2592, 2468,  542, 1126, 2582,  136, 1397, 3485, 2963, 3468,\n",
            "        1946, 1127, 2571, 3144, 2084, 2609,  884, 1738,  459, 1548, 1781,  607,\n",
            "        1806, 1058,  899, 3208, 2789, 2881, 1153, 2781,    1, 2235, 1834, 2087,\n",
            "        1982, 2385,  304, 1812, 3415, 1299, 2463, 1955, 3464, 2372, 3387, 2996,\n",
            "         487, 2812, 1337, 1748,  539, 2049, 2474,  913, 2903,  128,  946, 2841,\n",
            "        2600, 1123,  327, 3142,  621, 2154,  960, 2065, 1317,  248,  490, 2794,\n",
            "        2547, 2962,  516,    4, 3317,  535, 2653, 1557,  888, 2653, 1232,   13,\n",
            "        3013, 2100, 1126, 1111,  646,  165,   55, 2706, 2147, 2348, 2699,  542,\n",
            "        1798,  774, 2395, 1350, 2517,  308,  198, 2810, 3246, 3397, 2682, 2123,\n",
            "        2292,  696, 1884, 2467, 1392, 3242, 2318,  296, 2743, 2295, 1949, 2527,\n",
            "         571,  881, 2201, 1054, 2457, 1319, 1377,  514,  296,  206,   57, 1135,\n",
            "         576, 1679, 3142, 1251, 1630, 1624,   63,  765,  500, 2703, 3065,  312,\n",
            "        1185, 2762, 2989, 2396, 3044, 3460, 1358, 2127,  293, 1407, 2892, 2779,\n",
            "         648, 2805,   44, 2295, 1184, 2216, 2609, 1738, 2560, 2929, 2093,  409,\n",
            "          75, 3487,  661, 2572, 3409, 1398, 2954,  120,  863, 2658, 1319, 1711,\n",
            "        1692, 2350, 1031, 2994, 2747, 3182, 1628, 2366, 2822, 3358, 1096,  727,\n",
            "        3383, 2838,   49, 3142,  555, 3423, 1969,  267, 2036, 1776, 1566, 1846,\n",
            "        1452,  726,  553, 3028, 3431, 2577, 2081,  329, 3437, 1322, 2986,   53,\n",
            "         386,  652, 2765, 3018, 1189,  768, 2384,  164, 2507,  350, 2588, 2195,\n",
            "        2111, 1746, 2618, 3090, 1262, 2663, 1598, 2042, 3049,  988,  581, 3246,\n",
            "        3169, 2245, 1314,   24, 2445,  999,   37, 2076, 1249,  987, 1859,  799,\n",
            "         813, 1220, 1805, 2820,  759, 1281, 3133, 2498, 2231,  773, 1934, 1895,\n",
            "        2496,   72, 1139,  235, 2419, 2021, 1455, 2851, 2918, 2654, 1656, 3499,\n",
            "        1982, 1421, 2816, 2853, 2192,  283, 3026,  971, 1653, 2613, 1894, 2408,\n",
            "        2553,  116, 1121, 1407, 2310, 3230, 2603, 1087, 2669, 3095, 3054,  558,\n",
            "        3405, 3028, 2567, 1818, 2567,  670,  391, 2822, 3028, 3133,  239, 2873,\n",
            "         498,  672,  407, 1657, 2170, 2181,  238, 2940, 3206, 3482,  122, 1313,\n",
            "        2078, 2711, 2473,  813, 1701, 1398, 1806,  781, 3085, 2627,  677, 3101,\n",
            "         765, 2287, 3084, 2813, 1834, 3444, 2563,  654,  377, 1869, 2350, 2502,\n",
            "        1317, 3198,  695, 1494, 3054, 2829, 2491, 2805,  760,  861, 1664,  725,\n",
            "         465, 1206, 1446, 3255,  666, 1414, 1598, 2390, 3318, 1033, 1612, 1360,\n",
            "        3442,  331, 2356,  459,  896,  155,   27, 1764, 2493, 3025, 2292,  582,\n",
            "        1314, 1965, 3366, 2521, 3248,  155, 2426, 3307, 2547,  183, 1778,  417,\n",
            "        3110, 1748,  262, 3366,  542, 2161,  559, 1806,  218, 2049,  901, 2381,\n",
            "         933, 2835, 2816,  374, 3035, 1534, 1117, 1368,  872,  381, 3055,   67,\n",
            "        2202, 1328,  424, 1278,  204, 3338, 3469, 2959,  734,   45, 1093, 2429,\n",
            "        1287,    4,  741, 2335, 2509, 1258, 1327, 2474, 2139, 2951,  989, 2432,\n",
            "        2987, 2040, 2898, 1526,  794, 1014,   57, 2254,  870,  742, 1028,  666,\n",
            "        2824,  902, 2713, 2083, 3470, 3011, 2592, 2583, 2228, 1831, 1445, 2185,\n",
            "         393, 1564, 3488, 3138, 2839, 2709,  993,  983,  535, 2155, 3248, 3188,\n",
            "        1302,  502, 1867, 3363,   62, 2118, 2620, 1373, 1677, 1355, 2473,  383,\n",
            "        2179,  125, 3442, 3137, 1316,   16, 2705, 2171, 3368, 1911, 1203,  212,\n",
            "        1091, 3286, 2619, 3318, 3371, 3403, 1911, 1966, 1846, 2196, 2459, 2060,\n",
            "        2467, 3062,  519, 3443, 2185, 3406, 1084,  306, 2470, 2268,   62,  953,\n",
            "        1730,   86, 2674, 1605, 2408,  146, 1958,   73, 1815, 3040, 1877, 1626,\n",
            "         716,   56,  542, 3240,  403,  659, 2655,  333, 3172, 1649, 1536, 1575,\n",
            "         399,  785, 3207, 2036, 2827, 1858, 2938, 2235, 3196,  507,  857, 1849,\n",
            "        3416, 1373, 2532, 2968, 1534, 2936, 2345,  234, 1668, 2297,  613, 2920,\n",
            "          57, 1828,  596, 1436, 1346, 2976,  552,  812, 1413, 1872, 3046, 3451,\n",
            "         174, 1399, 1391,  260, 1529, 1149, 3011, 2436, 2218, 1854, 2445, 2399,\n",
            "         942, 1380, 2211,  638, 1657, 2516, 1767, 2271, 1621,  623,  222, 2815,\n",
            "        1635,  348, 2294, 1230, 3444,  333,  848,  850, 1269,  346, 3334, 1438,\n",
            "        1663, 1339,  443, 1879, 1679,  372, 3408, 3339,  849, 1412, 1035, 1108,\n",
            "        3205, 1905, 3471,  101, 2714,  896, 1004, 3429, 3457, 3028,  695, 1072,\n",
            "        2370,  397, 3191,  369, 1943, 2366, 2044, 1509, 3212, 1703, 1193, 1711,\n",
            "         527, 2984,  718, 3381, 1194,  773, 2425,  200, 1384, 2872, 3364, 2327,\n",
            "        3068, 1236, 1493, 2936, 2909, 2696, 1979,  387, 2296, 3242,  288, 2475,\n",
            "        2102,  682, 2850, 2345, 2642, 3040, 2769,  328, 1802,  150, 1768, 1215,\n",
            "        3424, 1679, 1265, 2112, 3044, 2467, 1947, 3310, 1665, 3196,  983, 1290,\n",
            "         330, 2599, 1293, 3360, 1439, 1481, 2235, 2113, 2525, 3110, 2794, 2033,\n",
            "         493, 1827, 2877, 1871,  824, 2072, 1987, 2592, 1948,  563, 2394, 2316,\n",
            "        1321, 1410, 3427, 3124,  427, 1781, 2679, 3301, 1686, 1658,  423,  268,\n",
            "        1826,  146, 1739, 2313, 2103, 2174, 3119, 1379,  895, 2578, 2760, 1325,\n",
            "        3447,  257,  691,   48,  826,  686,  106, 1934, 2501, 3196, 1683, 1295,\n",
            "        1294, 1106, 3280, 2160, 2669,  679, 2351,  831,  385,  297, 1711,  300,\n",
            "         905,  463,  625,   83, 2602, 3314,  426, 1519,  273, 1217, 2770, 1436,\n",
            "        1169, 3386, 2496,  664])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-187-6354f32afa35>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_recipe_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_recipe_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-186-da6e8cc9a9f9>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[0;34m(discriminator, generator, data, lr, batch_size, num_epochs, max_recipe_len)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_recipe_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0moutput_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2748\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2750\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2751\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([67, 2, 1])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    }
  ]
}