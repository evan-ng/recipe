{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TasteBud: GAN Based Recipe Generation with Graph\n",
    "\n",
    "Introductory words about this project..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Data Processing\n",
    "\n",
    "Data processing here has two main goals each with smaller milestones: tokenizing recipe data and creating the ingredients graph.\n",
    "Tokenizing data requires parsing the RecipeNGL dataset, which will be subsetted due to its large size.\n",
    "Creating the ingredients graph first requires a list of ingredients. A raw list will be obtained from the What's Cooking and RecipeNGL datasets. Then, the list will be filtered into a smaller list. The filtered list will be used for indexing ingredients, finding related/close ingredients, and creating the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import json\n",
    "import ast\n",
    "import glob\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing = True\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Cooking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "# loading the What's Cooking dataset from the .json file\n",
    "wc_train_path = './data/whats_cooking/train.json'\n",
    "wc_train_data = json.load(open(wc_train_path, 'r'))\n",
    "print(wc_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mustard sauce', 'yams', 'blanco tequila', 'back bacon rashers', 'gluten-free flour', 'whole wheat rotini pasta', 'barbecue rub', 'chili con carne', 'hazelnut flour', 'celery', 'anchovy fillets', 'dry milk powder', 'hard salami', 'cut up chicken', 'wish bone guacamol ranch dress', 'hot pork sausage', 'San Marzano tomatoes', 'low-fat balsamic vinaigrette', \"Quorn Chik''n Tenders\", 'curry leaves', 'sloe gin', 'saffron powder', 'passata', 'red curry paste', 'low sodium store bought chicken stock']\n"
     ]
    }
   ],
   "source": [
    "if pre_processing == True:\n",
    "    # creating a list of unique ingredients from the datasets\n",
    "    ingredients_set = set()\n",
    "\n",
    "    for i in range(len(wc_train_data)):\n",
    "        ingredients_set = ingredients_set | set(wc_train_data[i]['ingredients'])\n",
    "    print(list(ingredients_set)[0:25])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subsetting\n",
    "Since RecipeNGL contains 2.23 million recipes, select a subset to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/recipe_ngl\\dataset_0_100.csv\n",
      "./data/recipe_ngl\\dataset_0_5000.csv\n"
     ]
    }
   ],
   "source": [
    "full_ngl_path = './data/recipe_ngl/full_dataset.csv'\n",
    "# if the full recipeNGL dataset csv file exists, read a subset of it\n",
    "if os.path.exists(full_ngl_path):\n",
    "    ngl_subset = [0, 5000]\n",
    "    ngl_df = pd.read_csv(full_ngl_path, skiprows=ngl_subset[0], nrows=ngl_subset[1], index_col=0)\n",
    "    ngl_df.to_csv(f'./data/recipe_ngl/dataset_{ngl_subset[0]}_{ngl_subset[1]}.csv')\n",
    "\n",
    "for file in glob.glob('./data/recipe_ngl/dataset*.csv'):\n",
    "    print(file)\n",
    "    ngl_df = pd.read_csv(file, index_col=0, \n",
    "                         converters={'ingredients':pd.eval, 'directions':pd.eval, 'NER':pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[1 c. firmly packed brown sugar, 1/2 c. evapor...</td>\n",
       "      <td>[In a heavy 2-quart saucepan, mix brown sugar,...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[brown sugar, milk, vanilla, nuts, butter, bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[1 small jar chipped beef, cut up, 4 boned chi...</td>\n",
       "      <td>[Place chipped beef on bottom of baking dish.,...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[beef, chicken breasts, cream of mushroom soup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...</td>\n",
       "      <td>[In a slow cooker, combine all ingredients. Co...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[frozen corn, cream cheese, butter, garlic pow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [1 c. firmly packed brown sugar, 1/2 c. evapor...   \n",
       "1  Jewell Ball'S Chicken  [1 small jar chipped beef, cut up, 4 boned chi...   \n",
       "2            Creamy Corn  [2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [In a heavy 2-quart saucepan, mix brown sugar,...   \n",
       "1  [Place chipped beef on bottom of baking dish.,...   \n",
       "2  [In a slow cooker, combine all ingredients. Co...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [brown sugar, milk, vanilla, nuts, butter, bit...  \n",
       "1  [beef, chicken breasts, cream of mushroom soup...  \n",
       "2  [frozen corn, cream cheese, butter, garlic pow...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngl_df[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create List of Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mustard sauce', 'back bacon rashers', 'gluten-free flour', 'whole wheat rotini pasta', 'red raspberry jello', 'hazelnut flour', 'celery', 'anchovy fillets', 'San Marzano tomatoes', 'wish bone guacamol ranch dress', 'low-fat balsamic vinaigrette', \"Quorn Chik''n Tenders\", 'sloe gin', 'saffron powder', 'passata', 'red curry paste', 'low sodium store bought chicken stock', 'fresno pepper', 'leaf parsley', \"Campbell's cream\", 'brown basmati rice', 'gumbo file powder', 'amaretti', 'reduced fat coconut milk', 'condensed reduced fat reduced sodium cream of chicken soup', 'frozen mustard greens', 'pineapple pie filling', 'king oyster mushroom', 'linguine pasta', 'Tabasco sauce', 'flour tortillas (not low fat)', 'asian noodles', 'low-fat parmesan cheese', 'sausage casings', 'new york strip steaks', 'beef rib roast', 'apple juice', 'cherry jello', 'White Pepper', 'caramel icing', 'Tia Maria', 'meat sauce', 'baby okra', 'green garlic', 'onion buns', 'white tuna', 'dry vermouth', 'small potatoes', 'cream cheese', 'frozen lemonade concentrate, thawed and undiluted']\n"
     ]
    }
   ],
   "source": [
    "if pre_processing == True:\n",
    "    for i in range(len(ngl_df[\"NER\"])):\n",
    "        ingredients_set = ingredients_set | set(ngl_df[\"NER\"][i])\n",
    "    print(list(ingredients_set)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    cw = csv.writer(open(\"data/raw_ingredients_list.csv\",'w'))\n",
    "    cw.writerow(list(ingredients_set))\n",
    "    pd.read_csv('data/raw_ingredients_list.csv', header=None).T.to_csv('data/raw_ingredients_list_transpose.csv', header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a list of 8500 ingredients, we manually removed duplicated items that had spelling errors, semantic similarities, and extra numeric or qualitative descriptors (e.g. chopped, 2% fat, shredded, unsweetened), giving 955 ingredients. Then, each is paired its gloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: token_list - a list of tokens, no spaces or symbols\n",
    "# return: a tensor of the averaged GloVe embeddings of each token\n",
    "def glove_average(token_list):\n",
    "    embeds_list = []\n",
    "    for token in token_list:\n",
    "        embeds_list.append(glove[token])\n",
    "    embeds_average = torch.mean(torch.stack(embeds_list), dim=0)\n",
    "    return embeds_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    filtered_ingredients_df = pd.read_csv('data/filtered_ingredients_list_transpose.csv', header=None, names=[\"ingredient\"])\n",
    "    ingredient_embeddings = []\n",
    "\n",
    "    for i, row in filtered_ingredients_df.iterrows():\n",
    "        token_list = re.sub(r\"[^a-zA-Z ]+\", '', filtered_ingredients_df['ingredient'][i].lower()).split(' ')\n",
    "        embed_list = []\n",
    "        ingredient_embeddings.append(glove_average(token_list).tolist())\n",
    "        \n",
    "    filtered_ingredients_df['embedding'] = ingredient_embeddings\n",
    "    filtered_ingredients_df.to_csv('data/glove_ingredients_list.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_df = pd.read_csv('data/glove_ingredients_list.csv', header=None, names=[\"ingredient\", \"embedding\"],\n",
    "                         converters={'embedding':pd.eval})\n",
    "ingredients_df['ingredient'] = ingredients_df['ingredient'].apply(lambda x: x.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredient</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aioli</td>\n",
       "      <td>[0.053968001157045364, 0.027247000485658646, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ale</td>\n",
       "      <td>[-0.4636099934577942, 0.6578099727630615, -1.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>almond</td>\n",
       "      <td>[-0.023429999127984047, 0.47051000595092773, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ingredient                                          embedding\n",
       "0      aioli  [0.053968001157045364, 0.027247000485658646, -...\n",
       "1        ale  [-0.4636099934577942, 0.6578099727630615, -1.3...\n",
       "2     almond  [-0.023429999127984047, 0.47051000595092773, -..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_df[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GloVe give embeddings for out-of-vocabulary words, the words with null tensors `[0,0,0,...,0]` were removed from the ingredients list, giving a total of 840 ingredients. Using these embeddings will allow for ingredients not in the list to be mapped to the closest ingredient, then put in the ingredient graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary pairing ingredient with index value\n",
    "ingredient_index_dict = {}\n",
    "for i in range(ingredients_df.shape[0]):\n",
    "    if ingredients_df['ingredient'][i] not in ingredient_index_dict:\n",
    "        ingredient_index_dict[ ingredients_df['ingredient'][i] ] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only \"Gathered\" sources, since those have more consistent format\n",
    "gathered_ngl_df = ngl_df[ngl_df.source == 'Gathered']\n",
    "# remove unnecessary columns\n",
    "filtered_ngl_df = gathered_ngl_df[['title', 'ingredients', 'directions', 'NER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[1 c. firmly packed brown sugar, 1/2 c. evapor...</td>\n",
       "      <td>[In a heavy 2-quart saucepan, mix brown sugar,...</td>\n",
       "      <td>[brown sugar, milk, vanilla, nuts, butter, bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[1 small jar chipped beef, cut up, 4 boned chi...</td>\n",
       "      <td>[Place chipped beef on bottom of baking dish.,...</td>\n",
       "      <td>[beef, chicken breasts, cream of mushroom soup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...</td>\n",
       "      <td>[In a slow cooker, combine all ingredients. Co...</td>\n",
       "      <td>[frozen corn, cream cheese, butter, garlic pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[1 large whole chicken, 2 (10 1/2 oz.) cans ch...</td>\n",
       "      <td>[Boil and debone chicken., Put bite size piece...</td>\n",
       "      <td>[chicken, chicken gravy, cream of mushroom sou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [1 c. firmly packed brown sugar, 1/2 c. evapor...   \n",
       "1  Jewell Ball'S Chicken  [1 small jar chipped beef, cut up, 4 boned chi...   \n",
       "2            Creamy Corn  [2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...   \n",
       "3          Chicken Funny  [1 large whole chicken, 2 (10 1/2 oz.) cans ch...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [In a heavy 2-quart saucepan, mix brown sugar,...   \n",
       "1  [Place chipped beef on bottom of baking dish.,...   \n",
       "2  [In a slow cooker, combine all ingredients. Co...   \n",
       "3  [Boil and debone chicken., Put bite size piece...   \n",
       "\n",
       "                                                 NER  \n",
       "0  [brown sugar, milk, vanilla, nuts, butter, bit...  \n",
       "1  [beef, chicken breasts, cream of mushroom soup...  \n",
       "2  [frozen corn, cream cheese, butter, garlic pow...  \n",
       "3  [chicken, chicken gravy, cream of mushroom sou...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ngl_df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing uppercase and symbols from directions and fuse using '\\n'\n",
    "for i, row in filtered_ngl_df.iterrows():\n",
    "    cleaned_directions = []\n",
    "    for step in row.directions:\n",
    "        step = re.sub(r\"[^a-zA-Z0-9]+\", ' ', step.lower()) # remove uppercase and symbols\n",
    "        cleaned_directions.append(step)\n",
    "    row.directions = cleaned_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[1 c. firmly packed brown sugar, 1/2 c. evapor...</td>\n",
       "      <td>[in a heavy 2 quart saucepan mix brown sugar n...</td>\n",
       "      <td>[brown sugar, milk, vanilla, nuts, butter, bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[1 small jar chipped beef, cut up, 4 boned chi...</td>\n",
       "      <td>[place chipped beef on bottom of baking dish ,...</td>\n",
       "      <td>[beef, chicken breasts, cream of mushroom soup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...</td>\n",
       "      <td>[in a slow cooker combine all ingredients cove...</td>\n",
       "      <td>[frozen corn, cream cheese, butter, garlic pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[1 large whole chicken, 2 (10 1/2 oz.) cans ch...</td>\n",
       "      <td>[boil and debone chicken , put bite size piece...</td>\n",
       "      <td>[chicken, chicken gravy, cream of mushroom sou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [1 c. firmly packed brown sugar, 1/2 c. evapor...   \n",
       "1  Jewell Ball'S Chicken  [1 small jar chipped beef, cut up, 4 boned chi...   \n",
       "2            Creamy Corn  [2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...   \n",
       "3          Chicken Funny  [1 large whole chicken, 2 (10 1/2 oz.) cans ch...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [in a heavy 2 quart saucepan mix brown sugar n...   \n",
       "1  [place chipped beef on bottom of baking dish ,...   \n",
       "2  [in a slow cooker combine all ingredients cove...   \n",
       "3  [boil and debone chicken , put bite size piece...   \n",
       "\n",
       "                                                 NER  \n",
       "0  [brown sugar, milk, vanilla, nuts, butter, bit...  \n",
       "1  [beef, chicken breasts, cream of mushroom soup...  \n",
       "2  [frozen corn, cream cheese, butter, garlic pow...  \n",
       "3  [chicken, chicken gravy, cream of mushroom sou...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ngl_df[:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Data Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No-Bake', 'Nut', 'Cookies']\n",
      "[['1', 'c.', 'firmly', 'packed', 'brown', 'sugar'], ['1/2', 'c.', 'evaporated', 'milk'], ['1/2', 'tsp.', 'vanilla'], ['1/2', 'c.', 'broken', 'nuts', '(pecans)'], ['2', 'Tbsp.', 'butter', 'or', 'margarine'], ['3', '1/2', 'c.', 'bite', 'size', 'shredded', 'rice', 'biscuits']]\n",
      "[['in', 'a', 'heavy', '2', 'quart', 'saucepan', 'mix', 'brown', 'sugar', 'nuts', 'evaporated', 'milk', 'and', 'butter', 'or', 'margarine', ''], ['stir', 'over', 'medium', 'heat', 'until', 'mixture', 'bubbles', 'all', 'over', 'top', ''], ['boil', 'and', 'stir', '5', 'minutes', 'more', 'take', 'off', 'heat', ''], ['stir', 'in', 'vanilla', 'and', 'cereal', 'mix', 'well', ''], ['using', '2', 'teaspoons', 'drop', 'and', 'shape', 'into', '30', 'clusters', 'on', 'wax', 'paper', ''], ['let', 'stand', 'until', 'firm', 'about', '30', 'minutes', '']]\n",
      "[['brown', 'sugar'], ['milk'], ['vanilla'], ['nuts'], ['butter'], ['bite', 'size', 'shredded', 'rice', 'biscuits']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_titles = []\n",
    "tokenized_ingredients = []\n",
    "tokenized_directions = []\n",
    "tokenized_NER = []\n",
    "\n",
    "for i, row in filtered_ngl_df.iterrows():\n",
    "    # tokenize titles\n",
    "    tokens_list = filtered_ngl_df.title.values[i].split(' ')\n",
    "    tokenized_titles.append(tokens_list)\n",
    "    \n",
    "    # tokenize ingredients\n",
    "    tokens_list = []\n",
    "    for ingredient_item in row.ingredients:\n",
    "        tokens_list.append(ingredient_item.split(' '))\n",
    "    tokenized_ingredients.append(tokens_list)\n",
    "    \n",
    "    # tokenize directions\n",
    "    tokens_list = []\n",
    "    for direction_item in row.directions:\n",
    "        tokens_list.append(direction_item.split(' '))\n",
    "    tokenized_directions.append(tokens_list)\n",
    "    \n",
    "    # tokenize ingredients\n",
    "    tokens_list = []\n",
    "    for NER_item in row.NER:\n",
    "        tokens_list.append( re.sub(r\"[^a-zA-Z ]+\", '', NER_item.lower()).split(' ') )\n",
    "    tokenized_NER.append(tokens_list)\n",
    "\n",
    "print(tokenized_titles[0])\n",
    "print(tokenized_ingredients[0])\n",
    "print(tokenized_directions[0])\n",
    "print(tokenized_NER[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ngl_df = filtered_ngl_df.copy()\n",
    "tokenized_ngl_df['token_title'] = tokenized_titles\n",
    "tokenized_ngl_df['token_ingredients'] = tokenized_ingredients\n",
    "tokenized_ngl_df['token_directions'] = tokenized_directions\n",
    "tokenized_ngl_df['token_NER'] = tokenized_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>NER</th>\n",
       "      <th>token_title</th>\n",
       "      <th>token_ingredients</th>\n",
       "      <th>token_directions</th>\n",
       "      <th>token_NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[1 c. firmly packed brown sugar, 1/2 c. evapor...</td>\n",
       "      <td>[in a heavy 2 quart saucepan mix brown sugar n...</td>\n",
       "      <td>[brown sugar, milk, vanilla, nuts, butter, bit...</td>\n",
       "      <td>[No-Bake, Nut, Cookies]</td>\n",
       "      <td>[[1, c., firmly, packed, brown, sugar], [1/2, ...</td>\n",
       "      <td>[[in, a, heavy, 2, quart, saucepan, mix, brown...</td>\n",
       "      <td>[[brown, sugar], [milk], [vanilla], [nuts], [b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[1 small jar chipped beef, cut up, 4 boned chi...</td>\n",
       "      <td>[place chipped beef on bottom of baking dish ,...</td>\n",
       "      <td>[beef, chicken breasts, cream of mushroom soup...</td>\n",
       "      <td>[Jewell, Ball'S, Chicken]</td>\n",
       "      <td>[[1, small, jar, chipped, beef,, cut, up], [4,...</td>\n",
       "      <td>[[place, chipped, beef, on, bottom, of, baking...</td>\n",
       "      <td>[[beef], [chicken, breasts], [cream, of, mushr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...</td>\n",
       "      <td>[in a slow cooker combine all ingredients cove...</td>\n",
       "      <td>[frozen corn, cream cheese, butter, garlic pow...</td>\n",
       "      <td>[Creamy, Corn]</td>\n",
       "      <td>[[2, (16, oz.), pkg., frozen, corn], [1, (8, o...</td>\n",
       "      <td>[[in, a, slow, cooker, combine, all, ingredien...</td>\n",
       "      <td>[[frozen, corn], [cream, cheese], [butter], [g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [1 c. firmly packed brown sugar, 1/2 c. evapor...   \n",
       "1  Jewell Ball'S Chicken  [1 small jar chipped beef, cut up, 4 boned chi...   \n",
       "2            Creamy Corn  [2 (16 oz.) pkg. frozen corn, 1 (8 oz.) pkg. c...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [in a heavy 2 quart saucepan mix brown sugar n...   \n",
       "1  [place chipped beef on bottom of baking dish ,...   \n",
       "2  [in a slow cooker combine all ingredients cove...   \n",
       "\n",
       "                                                 NER  \\\n",
       "0  [brown sugar, milk, vanilla, nuts, butter, bit...   \n",
       "1  [beef, chicken breasts, cream of mushroom soup...   \n",
       "2  [frozen corn, cream cheese, butter, garlic pow...   \n",
       "\n",
       "                 token_title  \\\n",
       "0    [No-Bake, Nut, Cookies]   \n",
       "1  [Jewell, Ball'S, Chicken]   \n",
       "2             [Creamy, Corn]   \n",
       "\n",
       "                                   token_ingredients  \\\n",
       "0  [[1, c., firmly, packed, brown, sugar], [1/2, ...   \n",
       "1  [[1, small, jar, chipped, beef,, cut, up], [4,...   \n",
       "2  [[2, (16, oz.), pkg., frozen, corn], [1, (8, o...   \n",
       "\n",
       "                                    token_directions  \\\n",
       "0  [[in, a, heavy, 2, quart, saucepan, mix, brown...   \n",
       "1  [[place, chipped, beef, on, bottom, of, baking...   \n",
       "2  [[in, a, slow, cooker, combine, all, ingredien...   \n",
       "\n",
       "                                           token_NER  \n",
       "0  [[brown, sugar], [milk], [vanilla], [nuts], [b...  \n",
       "1  [[beef], [chicken, breasts], [cream, of, mushr...  \n",
       "2  [[frozen, corn], [cream, cheese], [butter], [g...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ngl_df[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction Step Combining\n",
    "To ease the complexity of the GAN and RNN, the list of directions for each recipe will be fused into a single list of words, with each step separated by `'\\n'` (note all prior symbols were removed). The end of the directions will be marked by `'<EOS>'` for \"End of String.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roll', 'steak', 'strips', 'in', 'flour', '\\n', 'brown', 'in', 'skillet', '\\n', 'salt', 'and', 'pepper', '\\n', 'combine', 'tomato', 'liquid', 'water', 'onions', 'and', 'browned', 'steak', 'cover', 'and', 'simmer', 'for', 'one', 'and', 'a', 'quarter', 'hours', '\\n', 'uncover', 'and', 'stir', 'in', 'worcestershire', 'sauce', '\\n', 'add', 'tomatoes', 'green', 'peppers', 'and', 'simmer', 'for', '5', 'minutes', '\\n', 'serve', 'over', 'hot', 'cooked', 'rice', '\\n', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "combined_directions = []\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    direction = []\n",
    "    for step in row.token_directions:\n",
    "        for word in step:\n",
    "            if word == '': # skip empty words\n",
    "                continue\n",
    "            direction.append(word)\n",
    "        direction.append('\\n')\n",
    "    direction.append('<EOS>')\n",
    "    combined_directions.append(direction)\n",
    "     \n",
    "tokenized_ngl_df['combined_directions'] = combined_directions\n",
    "print(combined_directions[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction Word Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 3499\n",
      "Unique words after ignoring: 2393\n",
      "['raisin', 'fitted', 'yams', 'snack', 'wish', 'chutney', 'service', 'hunger', 'pizzelle', 'celery', 'prayer', 'dot', 'headspace', 'chill', 'sprayed', 'faith', 'uv', 'stockpot', 'puddings', '16', 'overnight', 'rim', 'vegies', 'coals', 'atop', 'pepperidge', 'sifting', 'prawns', 'results', 'rind', 'teflon', 'nicely', 'nice', 'caramel', 'sear', 'gets', 'bake', 'water', 'hours', 'knead', 'sprouts', 'dip', 'pumpernickel', 'salads', 'bits', 'reynolds', 'soups', 'tart', 'slaw', 'rolled', 'filled', 'splenda', 'putting', 'cilantro', 'free', 'showers', 'treat', 'veggies', 'cokes', 'clear', 'rosemary', 'halibut', 'chafing', 'sterile', 'directions', 'sealed', 'saving', 'minutes', 'wilted', 'consistency', 'speed', 'coarse', 'combined', '0', 'liquids', 'fill', 'slice', 'flute', 'tin', 'sure', 'tartar', 'seconds', 'crackers', 'burritos', 'spreading', 'vermouth', 'fist', 'starter', 'half', 'pull', 'sticks', 'pitcher', 'kiwi', 'couple', 'sugars', 'tel', 'life', 'confectioner', 'orange', 'hard', 'evaporated', 'stock', 'chilling', 'rinse', 'cleaned', 'butter', 'ro', 'jicama', 'fire', 'crimp', 'adding', 'raise', 'mushy', 'platter', 'powdered', 'oats', 'reserve', 'linguini', 'cocoa', 'tester', 'container', 'bark', 'insides', 'cumin', 'love', 'beat', 'big', 'won', 'seal', 'then', 'crushed', 'addition', 'sheets', 'method', '\\n', 'to', 'cinnamon', 'dish', 'starch', 'grated', 'place', 'cob', 'fall', 'mine', 'original', 'spoon', 'constantly', 'papers', 'rims', 'made', 'sweeter', 'smooth', 'fillets', 'omit', 'fresh', 'teriyaki', 'dirt', 'kindly', 'ale', 'sausage', 'roux', 'takes', 'sauteed', 'hominy', 'enough', 'dash', 'option', 'inserted', 'pick', 'eggs', 'cool', 'end', 'pile', 'rinsed', 'gallons', 'yolk', 'depending', 'hors', 'fits', 'dice', 'spice', 'sugared', 'marinate', 'remove', 'oiled', 'chestnut', 'sized', 'in', 'actually', 'lawry', 'microwave', 'shoyu', 'ketchup', 'work', 'cheeses', 'dark', 'minute', 'crystals', 'corned', 'serves', 'frequently', 'raisins', 'serving', 'preferably', 'cook', 'ovenproof', 'fine', 'works', 'long', 'shallow', 'set', 'zucchini', 'thickest', 'control', 'wax', 'pink', 'metal', 'night', 'what', 'some', 'kraft', 'ricotta', 'rubber', 'dill', 'fairly', 'dogs', 'apple', 'here', 'sprinkles', 'eggbeater', 'instant', 'seed', 'form', 'rise', 'tines', 'fashion', 'tip', 'sprite', 'thoroughly', 'd', 'tots', 'runs', 'beans', 'choice', 'for', '36', 'creamy', 'liking', 'emulsify', 'times', 'dust', 'leftover', 'debone', 'replace', 'delightful', 'rotary', '86', 'beef', 'great', 'low', 'cutter', 'cloth', 'applesauce', 'cooled', 'continue', 'drop', 'sodium', 'loses', 'two', 'been', 'o', 'overbeat', 'whiskey', 'sympathy', 'slab', 'doing', 'scald', 'longer', '55', 'requires', 'up', 'covers', 'thinner', 'rotini', 'tsp', 'browns', 'distribute', 'partly', '120', 'turmeric', 'min', 'thinly', 'yellow', 'blended', 'strainer', 'kindness', 'bundt', 'occasionally', 'mound', 'chocolate', 'light', 'funnel', 'flour', 'extra', 'dried', 'meringue', 'mashed', 'sorrow', 'glaze', 'stars', 'shaking', 'roll', 'normally', 'forming', 'microcook', 'overcook', 'canned', 'tapped', 'lentils', 'loaves', 'buttered', 'leftovers', 'halfway', 'using', 'rabbit', 'feta', 'saran', 'leave', 'oatmeal', '9x9', 'parchment', 'cheez', 'calories', 'ginger', 'section', 'chewy', 'colors', 'points', 'salsa', 'jell', 'enclose', 'step', 'weed', 'runny', 'lid', 'feed', 'dome', 'saved', 'perhaps', '70', 'from', 'purple', 'chili', 'little', 'shell', 'bed', 'core', 'morsels', 'pith', 'secure', 'taste', 'prevent', 'inverted', 'drying', 'squeeze', 'boil', 'frosted', 'slightly', 'melting', 'stew', 'slit', 'shortening', 'souffle', 'pickle', 'molds', 'spoonful', 'among', 'sweeten', 'position', 'variety', 'haddock', 'unpeeled', 'tint', 'pocket', 'machine', 'nonstick', 'chunks', 'preheated', 'hearts', 'shredded', 'catalina', 'legs', 'steadily', 'pulling', 'strawberries', 'according', 'pastry', 'provolone', 'patties', 'saltwater', 'scalded', 'mash', 'lima', 'crumbles', 'steam', 'except', 'save', 'vanilla', 'different', 'pet', 'last', 'seeds', '8ths', 'procedure', 'damp', 'formed', 'noodle', 'bottles', 'dough', 'buy', 'pouring', 'egg', 'lumps', 'stop', 'rounded', 'peaked', 'shaker', 'pattern', 'help', 'spoonfuls', 'tamales', 'plus', 'unsweetened', 'ice', 'servings', 'available', 'gel', 'oven', 'oleo', 'stretch', 'take', 'pepper', 'circular', 'franks', 'pat', '100', 'having', 'complete', 'dampen', 'continuing', 'going', 'changes', 'trays', 'trim', 'about', 'whipping', 'cast', 'quite', 'campbell', '1', 'between', 'first', 'sauces', 'overbake', 'ending', 'tough', 'follows', 'smaller', 'salty', 'golf', 'rows', 'toothpick', 'pecan', 'bag', 'unbeaten', 'moisture', 'touched', 'coat', 'simply', 'tablespoonful', 'sort', 'french', 'pressed', 'american', 'heatproof', 'tightly', 'level', 'cold', 'refrigerate', 'juice', 'tvp', '200', 'good', 'crispy', 'cooking', 'split', 'pointed', 'average', 'flat', 'possible', 'dinner', 'small', 'spices', 'feeds', 'but', 'dissolves', 'centers', 'next', 'ones', 'gives', '13', 'chow', 'spaghetti', 'children', 'lime', 'texture', 'done', 'fried', 'three', 'tops', 'well', 'blot', 'rounds', 'knife', 'bamboo', 'broth', 'touch', 'leaf', 'candied', 'them', 'bags', 'chiles', 'crust', 'vinegar', 'wafers', 'penne', 'approx', 'bottoms', 'six', 'crusts', 'tan', 'lite', 'handling', 'against', '75', 'heated', 'boils', 'separates', 'largest', 'russian', 'apricots', 'string', 'round', 'grind', 'criss', 'okra', 'leak', 'weather', 'standing', 'tossing', 'recipe', 'eating', 'ring', 'age', 'beet', 'with', 'cutters', 'breast', 'semi', 'skim', 'directs', 'cherries', 'removed', 'crescents', 'satisfy', 'chip', 'shape', 'glasses', 'couscous', 'used', 'prefer', 'yummy', 'please', 'season', 'size', 'pressure', 'my', 'begin', '46', 'peppers', 'parties', 'prepared', 'kitchen', 'batch', 'dilute', 'stuffing', 'skillets', 'above', 'separately', 'wheat', 'underside', 'decoration', 'mixer', 'toast', 'lowest', 'peaches', 'pretzel', 'melted', 'radishes', '25', 'slash', 'pretty', 'rotel', 'disappear', 'beets', 'cabbage', 'herb', 'doesn', 'muffin', '500', 'slicing', 'stops', 'tube', 'out', 'teaspoonful', 'quiche', 'twinkies', 'separated', 'stiff', 'creamed', 'qt', 'grinder', 'meats', 'rack', 'leaves', 'c', 'plain', 'same', 'grate', 'coca', 'peeling', 'delicately', 'beer', 'dab', 'return', 'twice', 'thickened', 'almond', 'moderately', 'other', 'toward', 'cores', 'plump', 'divide', 'peaks', 'uncovered', 'garlic', 'sections', 'appetizer', 'flaked', 'floating', 'evaporate', 'envelope', 'becomes', 'pyrex', 'begins', 'yeast', 'firms', 'skin', 'milligrams', '11', 'scrape', 'along', 'mixture', 'moist', 'fluted', 'farm', 'roast', 'jackets', 'poke', 'snow', 'beaters', 'jalapeno', 'only', 'stays', 'package', 'devein', 'close', 'cutlets', 'gather', 'coarsely', 'arugula', 'bath', 'soup', 'grapes', 'usual', 'burner', 'left', 'insert', 'turning', 'extracts', 'star', 'flatter', 'adjust', 'thighs', 'creme', 'protein', 'basil', 'pounds', 'real', 'wrappers', 'whisk', 'heath', 'measured', 'baked', 'stream', 'peak', 'moisten', 'overlapping', 'throughout', 'squeezing', 'this', 'undercooked', 'carton', 'glass', 'jello', 'artichokes', 'depression', 'cheese', 'limburger', 'amber', 'meat', 'sweetened', 'several', 'berries', 'chex', 'casserole', 'pop', 'your', 'salt', '17', 'daily', 'retain', 'refrigerator', 'venison', 'cheddar', 'wine', '275', 'cranberries', 'cookies', 'coloring', 'circles', 'karo', 'gone', 'bringing', 'important', 'settling', 'soaked', 'roses', 'ends', 'pineapple', 'crepes', '64', 'horseradish', 'tomato', 'enchilada', 'towel', 'crockpot', 'advance', 'marshmallow', 'english', 'pudding', 'mg', 'seasonings', 'wire', 'pineapples', 'roasted', 'added', '85', 'main', 'substituted', 'finally', 'rectangular', 'shred', 'allspice', 'grains', 'coke', 'top', 'salmon', 'crumbled', 'pulp', 'cranberry', 'mix', 'pressing', 'raspberry', 'amount', 'follow', 'finely', 'marjoram', 'jelled', 'raspberries', 'salted', 'its', 'again', 'partially', 'right', 'any', 'meanwhile', 'gas', 'cereals', 'blintzes', 'old', 'ball', 'lightly', 'stand', 'kielbasa', 'exchange', 'letting', 'gr', 'even', 'diced', 'dipped', 'really', 'stiffen', 'club', 'rectangles', 'guests', 'pears', 'use', 'ribs', 'whirl', 'stuck', 'tasty', 'another', 'chilled', 'spring', 'disappears', 'we', 'covering', 'portion', 'sprinkled', 'process', 'dew', 'supper', 'v', 'baking', 'steamed', 'pin', 'velveeta', 'navy', '65', 'shaped', 'spray', 'non', 'spread', 'warmer', 'scissors', 'halve', 'pot', 'needed', 'soupy', 'covered', 'savory', 'blade', 'picante', '48', 'desired', 'espresso', 'coating', 'batches', 'stands', 'birthday', 'instructions', 'into', 'glossy', 'angle', 'italian', 'dipping', 'bakon', 'warmed', 'fat', 'coconut', 'braise', 'fingers', 'shells', 'marmalade', 'splashing', 'skillet', 'granulated', 'mini', 'bottom', 'miracle', 'shelf', 'toasted', 'crowd', 'teaspoons', 'grams', '2', 'flame', 'whites', 'saucepot', 'soaking', 'number', '9', 'kraut', 'green', 'deboned', 'side', 'transparent', 'mush', 'poppy', 'popcorn', 'lids', 'i', 'bouillon', 'remains', '13x9', 'home', 'wash', 'palms', 'nutmeg', 'various', 'of', 'ground', 'balls', 'safe', 'completely', 'dots', 'foam', 'bit', 'genesis', 'swiss', 'tail', 'paraffin', 'thawed', '8', 'chips', 'stove', 'oreo', 'store', 'serve', 'punch', 'wrap', 'drops', 'brandy', 'bisquick', 'finish', 'champagne', 'every', 'scrub', 'pints', 'on', 'scraping', 'that', 'hands', 'leaving', 'slices', 'beaten', 'flatten', 'rinsing', 'rises', 'vegetables', 'grease', 'firm', 'blanch', 'bell', 'cakes', 'during', 'knives', 'brussels', 'aside', 'very', 'poultry', 'watch', 'combination', 'after', 'scoops', 'cluster', 'beverages', 'chopper', 'near', 'self', 'oyster', 'flavoring', 'steak', 'moistened', 'crush', 'indirect', 'chopped', 'fitting', 'select', 'cauliflower', 'supreme', 'layers', 'diagonal', 'chocolates', 'crisco', 'go', 'griddle', 'colored', 'fasten', 'almonds', 'table', 'raw', 'chitterlings', 'alternating', 'drained', 'spreadable', 'saut', 'people', 'ungreased', 'allow', 'boiled', 'sweetener', 'degrees', 'oleomargarine', 'grapefruit', 'dropping', 'special', 'cholesterol', 'remaining', 'hot', 'future', 'packaged', 'wedge', 'piping', 'enjoy', 'soda', 'sandwich', 'chicken', 'oblong', 'thirds', 'tuck', 'edges', 'tiny', 'vermicelli', 'bun', 'patty', 'around', 'brine', 'game', 'cross', 'plate', 'curry', 'beginning', 'n', 'individually', 'orzo', 'boneless', 'are', 'alone', 'bread', 'you', 'tabasco', 'mints', '10x', 'crusty', 'meatballs', 'cornmeal', 'working', 'taking', 'watercress', 'way', 'pimentos', 'within', 'strip', 'easy', 'freezes', 'discarding', 'dates', 'indefinitely', 'mold', 'alcohol', 'crawfish', 'flours', 'unwrap', 'spins', 'dropped', 'deep', 'escape', 'medium', 'third', 'smearing', 'flake', 'mint', 'duty', 'temperature', 'ranch', 'cantaloupe', 'glazed', '300', 'parmesan', 'exchanges', 'oregano', 'absorb', 'phyllo', 'crepe', 'pasta', 'part', 'soft', 'sausages', 'being', 'cools', 'strain', 'own', 'realemon', 'gently', 'packages', 'saltine', 'pint', 'lower', 'contents', 'test', 'juices', 'herbs', 'pepperoni', 'flower', 'cup', 'roaster', 'amounts', 'graham', 'flavors', 'seasoned', 'homemade', 'log', 'reduce', 'across', 'slits', 'eight', 'stage', 'draw', 'pour', 'soon', 'juicy', 'full', 'strips', 'equally', 'lay', 'pig', 'generous', 'slush', 'granules', 'hand', 'finger', 'brush', 'proceed', 'suit', 'cutting', 'floured', 'twist', 'now', 'crumbling', 'fish', 'button', 'as', 'tastes', 'wrapper', 'filling', 'heavy', 'gravy', 'bombs', 'check', 'perk', 'foamy', 'bars', 'try', 'should', 'cover', 'smoothness', 'toppings', 'salad', 'scorching', 'marinade', 'mace', 'tray', 'bacon', 'much', 'onions', 'fork', 'pimiento', 'still', 'hardened', 'ladle', 'rolling', 'mixtures', 'continually', 'stuff', 'avocado', 'substitute', 'kettle', 'truss', 'pies', 'rolls', 'wedges', 'blueberry', 'warm', 'spanish', 'whipped', 'fruit', 'tight', 'coated', 'puree', 'roasting', 'under', 'smile', 'both', 'fingertips', 'butterfingers', 'sliced', 'cube', 'lastly', 'cupcake', 'blend', 'shallots', 'toy', 'knox', 'reserved', 'maraschino', 'mounds', 'stem', 'sets', 'brownies', 'tips', 'hamburgers', 'bowls', 'peanut', 'avoid', 'thyme', 'put', 'muffins', 'cherry', 'deviled', 'fruits', 'dog', 'foil', 'fry', 'richer', 'better', 'fully', 'best', 'congealed', 'early', 'kahlua', 'looks', 'dividing', 'smoke', 'liqueur', 'croutons', 'grits', 'inch', 'onion', 'months', 'noodles', 'horizontally', 'want', 'pkg', 'sauce', 'molasses', 'sifted', 'opaque', 'll', 'zest', 'quick', 'hershey', 'flavored', 'reduced', 'thick', 'baby', 'sealing', 'line', 'skewer', 'tater', 'filo', 'mustard', 'was', 'cake', 'till', 'ziploc', 'rock', 'loosely', 'hollow', 'caraway', 'cider', 'condensed', 'inside', 'sweat', 'anise', 'browned', 'stay', 'chowder', 'doubled', '80', 'all', 'freeze', 'steep', 'more', 'would', 'plates', 'airtight', 'lamb', 'lumpy', 'it', 'collards', 'cocktail', 'breading', 'most', 'strings', 'tie', 'elastic', 'freezing', 'slide', '240', 'precooked', 'less', 'omitted', 'ladyfingers', 'halves', 'prepare', 'bird', 'strawberry', 'absorbed', 'dump', 'sterilize', '90', 'so', 'sour', 'handle', 'awhile', 'quarter', 'banana', 'mince', 'wafer', 'bubble', 'liver', 'carefully', 'diagonally', 'style', 's', '3', 'keeping', 'hang', 'family', 'solids', 'quickly', 'butterfly', 'maximum', 'needs', 'or', 'giblets', '6', 'per', 'sugar', 'balance', 'melts', 'directed', 'day', 'kettles', 'liquid', 'circle', 'choose', 'opening', 'evenly', 'hens', 'pack', 'sunflower', 'sesame', 'quarts', 'candies', 'wait', 'carrot', 'reaches', 'open', 'p', 'buttermilk', 'tall', 'holes', 'decorating', 'removing', 'garnish', 'crank', 'cucumbers', 'cola', 'yum', 'not', 'peas', 'figs', 'racks', 'closely', 'one', 'undrained', 'powder', 'loaf', 'dente', 'macaroni', 'crumb', 'motion', 'pimento', 'deer', 'cracked', 'tails', 'eagle', 'pickles', 'wanted', 'combine', 'their', 'mouth', 'onto', 'trifle', 'rest', 'decorative', 'extremely', 'once', 'sift', 'simmer', '10', 'easier', 'larger', 'mug', 'outer', 'soak', 'melba', 'seasoning', 'relish', 'cornflakes', 'bubbles', 'sides', 'jar', 'waxed', 'hold', 'dissolved', 'always', 'pats', 'maybe', 'parts', 'run', 'flank', 'sticking', 'seams', 'fries', 'group', 'sprinkle', 'chop', 'upon', 'corns', 'tortilla', 'heat', 'lot', 'lots', 'fettuccine', 'winter', 'puff', 'reheated', 'either', 'ripe', 'corn', 'mixed', 'the', 'cracker', 'tuna', 'stems', 'purchased', 'chunk', 'rich', 'additional', 'firmly', 'tins', 'crock', 'clams', 'almost', 'continuously', 'mayonnaise', 'ahead', 'pureed', 'breakfast', '1st', 'drink', 'human', 'served', 'making', 'preference', 'canning', 'slowly', 'many', 'lined', '24', 'peanuts', 'lasts', 'never', 'basting', 'hamburg', 'kool', 'oeuvre', 'careful', 'tarragon', 'they', 'room', 'undiluted', 'bubbling', 'cole', 'necessary', 'pans', 'volume', 'anchor', 'miniature', 'sherry', 'membranes', 'thousand', 'rye', 'boned', 'mugs', 'transfer', 'caramels', 'parsley', '4', 'frankfurters', 'scrambled', 'chunked', 'dozen', 'squares', 'forms', 'tapioca', 'dollop', 'evaporates', 'caps', 'checking', 'lean', 'tears', 'repeat', 'lasagna', 'a', 'persimmon', 'cashews', 'square', 'seems', 'throw', '35', 'crumbly', 'gloss', 'allowed', 'dry', 'release', 'back', 'ramen', 'seam', 'florets', 'whatever', 'upside', 'hash', 'mixes', 'unmold', 'sterilized', 'halved', 'rectangle', 'food', 'spoke', 'spoons', 'paint', 'worcestershire', 'tea', 'drain', 'luke', 'tap', 'tomatoes', 'batter', 'measure', 'poured', 'packet', 'bay', 'mocha', 'total', '20', 'sun', 'smoked', 'need', 'pickling', 'cheesecloth', 'usually', 'thermometer', 'ham', 'shows', 'pea', 'edge', 'spiral', 'soy', 'thickening', 'springs', 'pheasant', 'beater', 'few', 'comes', 'slow', 'oreos', 'discard', 'sink', 'waffles', 'marshmallows', '375', 'skins', 'clam', 'remainder', 'resistant', 'equal', 'flavorings', 'optional', 'ounce', 'barbecue', 'thumb', 'like', 'beating', 'jars', '14', 'toothpicks', 'church', 'building', 'outside', 'pliable', 'custard', 'chestnuts', 'pork', 'blender', 'lengthwise', 'had', 'overmix', 'bean', 'breadcrumbs', 'running', 'lift', 'tossed', 'blossoms', 'saute', 'single', 'blackberries', 'red', 'creaming', 'drizzle', 'coats', 'cupcakes', 'jelly', 'nutmeats', 'holds', 'scallions', 'bring', '60', 'dutch', 'empty', 'seafood', 'cottage', 'turns', 'manufacturer', 'gentle', 'crunch', 'capers', 'increase', 'accent', 'oval', 'aluminum', 'garnishes', 'joy', 'upright', '50', 'has', '350', 'kneading', 'setting', 'before', 'pan', 'flavor', 'rotating', 'drippings', 'block', 'corners', 'followed', 'desire', 'carrots', 'air', 'extract', 'piece', '130', 'mountain', 'fluffy', 'field', '30', 'portions', '234', 'preheating', 'yogurt', 'eat', 'refrigerated', 'make', 'ready', 'score', 'pale', 'blending', 'child', 'dumplings', 'yields', 'peeled', 'stuffed', 'jellyroll', 'holding', 'meal', 'washed', 'gelatine', 'candy', 'mozzarella', 'could', 'taffy', 'an', 'alum', 'dressing', 'basket', 'change', 'including', 'chinese', 'point', '250', 'alternate', 'fryer', 'brown', 'frothy', 'get', 'listed', 'eggplant', 'and', 'wide', 'baker', 'row', 'buns', 'tofu', 'm', '2nd', 'shellfish', 'sherbet', 'starting', 'sharp', 'curdle', 'pectin', 'containers', 'bubbly', 'marble', 'note', 'krispies', 'cream', 'heating', 'frying', 'burn', 'bakes', 'those', 'margarine', 'wieners', 'popped', 'let', 'together', 'folding', 'consomme', 'bigger', 'narrow', 'milk', 'alternately', 'biscuit', 'worms', 'apart', 'breasts', 'breads', 'pare', 'lengths', 'teaspoonfuls', 'at', 'thin', 'refried', 'prick', 'oysters', 'kidney', 'wild', 'blueberries', 'entire', 'preheat', 'nacho', 'shoulder', 'separate', 'honey', 'spooned', 'is', 'cans', 'apples', 'also', 'saucepan', 'mushroom', 'lemon', 'instead', 'tablespoon', 'yolks', 'turned', 'tests', 'kids', 'do', 'wrapped', 'double', 'sizzling', 'peek', 'move', 'pops', 'days', 'party', 'makes', 'sweet', 'chuck', 'person', 'coffee', 'crack', 'stalks', '450', 'roni', 'solution', 'liberally', 'remember', 'equals', 'proof', 'flowers', 'eaten', 'remain', 'electric', 'ounces', 'bar', 'logs', 'correct', 'tortellini', 'without', 'placed', 'regularly', 'greased', 'fix', 'shut', 'vessel', 'dashes', 'hotter', 'trimmings', 'there', 'peel', 'minced', 'hole', 'pumpkin', 'push', 'give', 'purpose', 'rotate', 'syrup', 'invert', 'regular', 'milnot', 'de', 'be', 'shears', 'cloves', 'cayenne', 'diluted', 'gruyere', 'ragu', 'hr', 'shiny', 'golden', 'everything', 'wok', 'bran', 'kiss', 'directly', 'brand', 'indention', 'drinks', 'cooks', 'angel', 'loosen', 'moderate', 'kept', 'brushing', 'containing', 'dishes', 'how', 'storage', 'unroll', 'center', 'assorted', 'steady', 'rather', 'peppercorns', '425', 'ingredient', 'flakes', 'rotelle', 'samuel', '18', 'monterey', 'base', 'cubed', 'particles', 'white', 'bananas', 'draft', 'resembles', 'float', 'clumping', 'fun', 'burgers', 'bourbon', 'lunch', '22', 'rings', 'x', 'breaking', 'nuts', 'jack', 'crumble', 'absorbent', 'walnut', 'browning', 'chives', '7', 'keep', 'guacamole', 'apply', 'tbsp', 'spots', 'parboil', 'label', 'fats', 'maple', 'smash', 'black', 'have', 'just', 'head', 'broiler', 'sit', 'turn', 'pieces', 'sage', 'allowing', 'pronged', 'bite', 'yield', 'soften', 'harden', 'wipe', 'slotted', 'freshly', 'nonfat', 'cubes', 'immediately', 'asparagus', '33', 'scatter', 'lettuce', 'clusters', 'stick', 'biscuits', 'cornbread', 'see', 'stir', 'elephant', 'taco', 'tablespoonfuls', 'leeks', 'often', 'gummy', 'vigorously', 'swirl', 'pizza', 'broil', 'dissolve', 'pinto', 'such', 'prior', 'tear', 'cornflake', 'securely', 'etc', 'anything', 'topped', 'microwavable', 'pretzels', 'candle', 'bagels', 'rivels', '365', 'delicious', 'mill', 'sheet', 'colander', 'frosting', 'f', 'crescent', 'greens', 'spiced', 'pizzas', 'blue', 'topping', 'fast', '40', 'concentrate', 'crunchy', 'grain', 'grocery', 'heaping', 'tupperware', 'lighten', 'mallet', 'generously', 'processor', 'cooked', 'olives', 'marsala', 'time', 'bone', 'bundles', 'surround', 'become', 'everyone', 'preserves', 'pound', 'reused', 'type', 'cuts', 'look', 'resemble', 'xxxx', 'probably', 'confectioners', 'pit', 'broken', 'whole', 'tablespoons', 'shrimp', 'perforations', 'hour', 'veal', 'newspaper', 'skewers', 'gallon', 'potatoes', 'too', 'excess', 'does', 'brisket', 'dredge', 'personal', 'kisses', 'below', 'mushrooms', 'altogether', 'mango', 'enchiladas', 'cracks', 'surface', '45', 'limp', 'find', 'favorite', 'than', 'grill', 'rhubarb', '9x13', 'if', 'layer', '32', 'similar', 'softened', 'individual', 'new', 'island', 'thickens', 'preparing', 'mexican', 'when', 'rapidly', 'sieve', '112', 're', 'uncooked', 'pancake', 'puffs', 'linguine', 'shake', 'lasagne', 'luncheon', 'gelatin', 'aid', 'tostitos', 'steaks', 'garnished', 'lemons', 'fritos', 'closed', 'skinless', 'exactly', 'arrange', 'fold', 'crosswise', 'until', 'crumbs', 'wet', 'oil', 'measuring', 'cereal', 'break', 'something', 'sooner', 'crisp', 'stewed', 'steel', 'goes', '5', 'incorporate', 'handful', 'order', 'second', 'paprika', 'mayo', 'walnuts', 'clean', 'through', 'solid', 'chops', 'uncover', 'bulk', 'called', 'least', 'sauerkraut', 'helps', 'must', 'towels', 'tied', 'germ', 'mostaccioli', 'pistachio', '150', 'dunk', 'will', 'given', 'sticky', 'wings', 'dessert', 'create', 'thicken', 'briefly', 'spatula', 't', 'apricot', 'cucumber', '325', 'al', 'themselves', 'come', 'diet', 'large', 'pimientos', '23', 'taken', 'down', 'appear', 'artificial', 'puffy', 'bones', 'wesson', 'g', 'saucer', 'groundhog', 'week', 'teaspoon', 'don', 'five', 'seasons', 'placing', 'iron', 'rice', 'nearly', 'starts', 'cracklins', 'touching', 'ziti', 'stirred', 'fashioned', 'triangle', 'vent', 'avocados', 'freezer', 'counter', 'oz', 'romaine', 'chipped', 'frost', 'stiffly', 'clove', 'door', 'scallops', 'flowerets', 'country', 'frypan', 'reserving', 'spears', 'icebox', 'cooling', 'degree', 'middle', 'gradually', 'lard', 'burning', 'broccoli', 'reheat', 'pear', 'baste', 'off', 'liners', 'mixing', 'scramble', 'syrupy', 'whip', 'pie', 'dippers', 'fridge', 'simmering', 'icing', 'bowl', 'diameter', 'sprinkling', 'slight', 'ensure', 'decorate', 'pam', 'able', 'fondant', 'wooden', '370', 'while', 'itself', 'can', 'countertop', 'morning', 'excellent', 'layering', 'amaretto', 'high', 'these', 'whiz', 'flaky', 'paste', 'quart', 'nectar', 'away', 'thread', 'oat', 'prebaked', 'stack', 'over', 'peach', 'puffed', 'liquor', 'seem', 'finished', 'doneness', 'plastic', 'squirrel', 'artichoke', 'squash', 'garbanzo', '475', 'appearance', 'potato', 'weeks', 'prep', 'blanched', 'masher', 'toss', 'paper', 'cooker', 'add', 'thaw', 'tinfoil', 'flattened', 'following', 'thickness', 'doritos', 'cookie', 'cut', 'congeal', 'dream', 'intact', 'press', 'start', 'rub', 'nine', 'crabmeat', 'because', 'color', 'butterscotch', 'thicker', 'stirring', 'quarters', 'power', 'may', 'grape', 'scoop', 'cornstarch', 'shapes', 'catsup', 'barely', 'hollowed', 'brownie', 'cups', 'palm', 'strokes', 'frozen', 'jam', 'pecans', 'mein', 'four', 'preparation', 'christmas', 'neck', 'sandwiches', 'cajun', 'contains', 'no', 'loose', 'translucent', 'nut', 'mandarin', 'stone', 'triangles', 'rum', 'hamburger', 'fit', 'later', 'scant', 'lemonade', 'boiling', '15', 'melt', 'boiler', '12', 'quantity', 'inches', 'ritz', 'awful', 'grilled', '<EOS>', 'tortillas', 'sprigs', 'depth', 'chilies', 'helpings', 'items', 'hi', 'by', 'lukewarm', 'width', 'birds', 'pancakes', 'preferred', 'tender', 'butterflied', 'vegetable', 'unbaked', 'envelopes', 'pinch', 'ingredients', 'prunes', 'board', 'tongs', 'spinach', 'kind', 'fudge', 'already', 'which', 'olive', 'box', 'basic', 'each', 'says', 'month', 'bottle', 'pulled', 'oranges', 'approximately', 'holidays', 'turkey', 'pulls', 'stored', 'clings', 'keeps', 'easily', '400', 'crab', 'elbow']\n"
     ]
    }
   ],
   "source": [
    "# calculate word frequency from directions and ingredients list (NER)\n",
    "directions_word_freq = {}\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    for word in row.combined_directions:\n",
    "        directions_word_freq[word] = directions_word_freq.get(word, 0) + 1\n",
    "\n",
    "    for ingredient_list in row.token_NER:\n",
    "        for ingredient in ingredient_list:\n",
    "            directions_word_freq[word] = directions_word_freq.get(word, 0) + 1\n",
    "        \n",
    "minimum_word_freq = 2\n",
    "vocab = set()\n",
    "for word, freq in directions_word_freq.items():\n",
    "    if directions_word_freq[word] >= minimum_word_freq:\n",
    "        vocab.add(word)\n",
    "\n",
    "print('Unique words before ignoring:', len(directions_word_freq))\n",
    "print('Unique words after ignoring:', len(vocab))\n",
    "\n",
    "vocab = list(vocab)\n",
    "vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
    "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction Character Vocabulary\n",
    "For the baseline model, a character based vocabulary will be used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n', '<EOS>']\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '0': 26, '1': 27, '2': 28, '3': 29, '4': 30, '5': 31, '6': 32, '7': 33, '8': 34, '9': 35, ' ': 36, '\\n': 37, '<EOS>': 38}\n"
     ]
    }
   ],
   "source": [
    "char_vocab = list(string.ascii_lowercase) + list(string.digits) + [' ', '\\n', '<EOS>']\n",
    "char_vocab_stoi = {s: i for i, s in enumerate(char_vocab)}\n",
    "char_vocab_itos = {i: s for i, s in enumerate(char_vocab)}\n",
    "char_vocab_size = len(vocab)\n",
    "print(char_vocab)\n",
    "print(char_vocab_stoi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[311, 984, 1135, 187, 302, 134, 1718, 187, 887, 134, 739, 1723, 441, 134, 1403, 764, 1357, 37, 1177, 1723, 1287, 984, 1167, 1723, 1413, 244, 1388, 1723, 1529, 1328, 38, 134, 2122, 1723, 1957, 187, 1557, 1256, 134, 2274, 1562, 905, 589, 1723, 1413, 244, 2110, 67, 134, 940, 2252, 1021, 2008, 2173, 134, 2344]\n"
     ]
    }
   ],
   "source": [
    "indexed_directions = []\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    direction = []\n",
    "    for word in row.combined_directions:\n",
    "        if word in vocab:\n",
    "            direction.append(vocab_stoi[word])\n",
    "    indexed_directions.append(direction)\n",
    "     \n",
    "print(indexed_directions[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1562, 37, 1177, 1557, 1256, 905, 589, 2100]\n"
     ]
    }
   ],
   "source": [
    "indexed_ingredients = []\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    ingredients = []\n",
    "    for ingredient_list in row.token_NER:\n",
    "        for word in ingredient_list:\n",
    "            if word in vocab:\n",
    "                ingredients.append(vocab_stoi[word])\n",
    "    indexed_ingredients.append(ingredients)\n",
    "     \n",
    "print(indexed_ingredients[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_ngl_df = pd.DataFrame({'directions': indexed_directions, 'ingredients': indexed_ingredients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directions</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[187, 1529, 1158, 897, 2245, 1786, 792, 1718, ...</td>\n",
       "      <td>[1718, 1351, 1757, 407, 1909, 105, 1938, 584, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[140, 2188, 257, 951, 890, 922, 845, 137, 134,...</td>\n",
       "      <td>[257, 1029, 1762, 1742, 922, 1787, 693, 1325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[187, 1529, 1590, 2273, 1403, 1292, 2364, 1167...</td>\n",
       "      <td>[2310, 1454, 1742, 728, 105, 656, 1390, 739, 441]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          directions  \\\n",
       "0  [187, 1529, 1158, 897, 2245, 1786, 792, 1718, ...   \n",
       "1  [140, 2188, 257, 951, 890, 922, 845, 137, 134,...   \n",
       "2  [187, 1529, 1590, 2273, 1403, 1292, 2364, 1167...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [1718, 1351, 1757, 407, 1909, 105, 1938, 584, ...  \n",
       "1  [257, 1029, 1762, 1742, 922, 1787, 693, 1325, ...  \n",
       "2  [2310, 1454, 1742, 728, 105, 656, 1390, 739, 441]  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_ngl_df[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to Character Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 14, 11, 11, 36, 18, 19, 4, 0, 10, 36, 18, 19, 17, 8, 15, 18, 36, 8, 13, 36, 5, 11, 14, 20, 17, 36, 37, 1, 17, 14, 22, 13, 36, 8, 13, 36, 18, 10, 8, 11, 11, 4, 19, 36, 37, 18, 0, 11, 19, 36, 0, 13, 3, 36, 15, 4, 15, 15, 4, 17, 36, 37, 2, 14, 12, 1, 8, 13, 4, 36, 19, 14, 12, 0, 19, 14, 36, 11, 8, 16, 20, 8, 3, 36, 22, 0, 19, 4, 17, 36, 14, 13, 8, 14, 13, 18, 36, 0, 13, 3, 36, 1, 17, 14, 22, 13, 4, 3, 36, 18, 19, 4, 0, 10, 36, 2, 14, 21, 4, 17, 36, 0, 13, 3, 36, 18, 8, 12, 12, 4, 17, 36, 5, 14, 17, 36, 14, 13, 4, 36, 0, 13, 3, 36, 0, 36, 16, 20, 0, 17, 19, 4, 17, 36, 7, 14, 20, 17, 18, 36, 37, 20, 13, 2, 14, 21, 4, 17, 36, 0, 13, 3, 36, 18, 19, 8, 17, 36, 8, 13, 36, 22, 14, 17, 2, 4, 18, 19, 4, 17, 18, 7, 8, 17, 4, 36, 18, 0, 20, 2, 4, 36, 37, 0, 3, 3, 36, 19, 14, 12, 0, 19, 14, 4, 18, 36, 6, 17, 4, 4, 13, 36, 15, 4, 15, 15, 4, 17, 18, 36, 0, 13, 3, 36, 18, 8, 12, 12, 4, 17, 36, 5, 14, 17, 36, 31, 36, 12, 8, 13, 20, 19, 4, 18, 36, 37, 18, 4, 17, 21, 4, 36, 14, 21, 4, 17, 36, 7, 14, 19, 36, 2, 14, 14, 10, 4, 3, 36, 17, 8, 2, 4, 36, 37, 38]\n",
      "roll steak strips in flour \n",
      "brown in skillet \n",
      "salt and pepper \n",
      "combine tomato liquid water onions and browned steak cover and simmer for one and a quarter hours \n",
      "uncover and stir in worcestershire sauce \n",
      "add tomatoes green peppers and simmer for 5 minutes \n",
      "serve over hot cooked rice \n",
      "<EOS>"
     ]
    }
   ],
   "source": [
    "char_indexed_directions = []\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    direction = []\n",
    "    for word in row.combined_directions:\n",
    "        if word in ['\\n', '<EOS>']:\n",
    "            direction.append(char_vocab_stoi[word])\n",
    "        else:\n",
    "            for char in word:\n",
    "                if char in char_vocab:\n",
    "                    direction.append(char_vocab_stoi[char])\n",
    "            direction.append(char_vocab_stoi[' '])\n",
    "        \n",
    "    char_indexed_directions.append(direction)\n",
    "     \n",
    "print(char_indexed_directions[8])\n",
    "for char in char_indexed_directions[8]:\n",
    "    print(char_vocab_itos[char], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 14, 12, 0, 19, 14, 4, 18, 37, 22, 0, 19, 4, 17, 37, 14, 13, 8, 14, 13, 18, 37, 22, 14, 17, 2, 4, 18, 19, 4, 17, 18, 7, 8, 17, 4, 37, 18, 0, 20, 2, 4, 37, 6, 17, 4, 4, 13, 37, 15, 4, 15, 15, 4, 17, 18, 37, 14, 8, 11, 37]\n",
      "tomatoes\n",
      "water\n",
      "onions\n",
      "worcestershire\n",
      "sauce\n",
      "green\n",
      "peppers\n",
      "oil\n"
     ]
    }
   ],
   "source": [
    "char_indexed_ingredients = []\n",
    "for i, row in tokenized_ngl_df.iterrows():\n",
    "    ingredients = []\n",
    "    for ingredient_list in row.token_NER:\n",
    "        for word in ingredient_list:\n",
    "            if word in ['\\n', '<EOS>']:\n",
    "                ingredients.append(char_vocab_stoi[word])\n",
    "            else:\n",
    "                for char in word:\n",
    "                    if char in char_vocab:\n",
    "                        ingredients.append(char_vocab_stoi[char])\n",
    "                ingredients.append(char_vocab_stoi['\\n'])\n",
    "    char_indexed_ingredients.append(ingredients)\n",
    "     \n",
    "print(char_indexed_ingredients[8])\n",
    "for char in char_indexed_ingredients[8]:\n",
    "    print(char_vocab_itos[char], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indexed_ngl_df = pd.DataFrame({'directions': char_indexed_directions, 'ingredients': char_indexed_ingredients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directions</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 13, 36, 0, 36, 7, 4, 0, 21, 24, 36, 28, 36...</td>\n",
       "      <td>[1, 17, 14, 22, 13, 37, 18, 20, 6, 0, 17, 37, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[15, 11, 0, 2, 4, 36, 2, 7, 8, 15, 15, 4, 3, 3...</td>\n",
       "      <td>[1, 4, 4, 5, 37, 2, 7, 8, 2, 10, 4, 13, 37, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8, 13, 36, 0, 36, 18, 11, 14, 22, 36, 2, 14, ...</td>\n",
       "      <td>[5, 17, 14, 25, 4, 13, 37, 2, 14, 17, 13, 37, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          directions  \\\n",
       "0  [8, 13, 36, 0, 36, 7, 4, 0, 21, 24, 36, 28, 36...   \n",
       "1  [15, 11, 0, 2, 4, 36, 2, 7, 8, 15, 15, 4, 3, 3...   \n",
       "2  [8, 13, 36, 0, 36, 18, 11, 14, 22, 36, 2, 14, ...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [1, 17, 14, 22, 13, 37, 18, 20, 6, 0, 17, 37, ...  \n",
       "1  [1, 4, 4, 5, 37, 2, 7, 8, 2, 10, 4, 13, 37, 1,...  \n",
       "2  [5, 17, 14, 25, 4, 13, 37, 2, 14, 17, 13, 37, ...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indexed_ngl_df[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closest Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: ingredient - list of strings of an ingredient (tokenized)\n",
    "#        ingredient_df - dataframe containing ingredient vocabulary and\n",
    "#                        their corresponding GloVe embedding\n",
    "# return: string of closest ingredient in vocabulary\n",
    "#         if none (e.g. ingredient has is OOV in GloVe), returns empty string\n",
    "def get_closest_ingredient (ingredient, ingredients_df):\n",
    "    # check if ingredient is in ingredients_df\n",
    "    if len(ingredient) == 1 and ingredient[0] in ingredients_df['ingredient'].values:\n",
    "        return ingredient[0]\n",
    "    \n",
    "    closest_ingredient = ''\n",
    "    smallest_distance = float('inf')\n",
    "    \n",
    "    # compute the GloVe embedding of the ingredient\n",
    "    ingredient_embedding = glove_average(ingredient)\n",
    "    \n",
    "    if torch.count_nonzero(ingredient_embedding) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # compute distances between embeddings, choose the smallest distance\n",
    "    for _, row in ingredients_df.iterrows():\n",
    "        difference = ingredient_embedding - torch.FloatTensor(row['embedding'])\n",
    "        distance = torch.sum(torch.square(difference))\n",
    "        if distance < smallest_distance:\n",
    "            smallest_distance = distance\n",
    "            closest_ingredient = row['ingredient']\n",
    "    \n",
    "    return closest_ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown sugar', 'milk', 'vanilla', 'soy nut', 'butter', 'rice paper']\n"
     ]
    }
   ],
   "source": [
    "if pre_processing == True:\n",
    "    # changes the ingredients list into their corresponding closest ingredients\n",
    "    # in the vocabulary using the GloVe embeddings\n",
    "    NER_closest_ingredients = []\n",
    "\n",
    "    for i, row in tokenized_ngl_df.iterrows():\n",
    "        NER_closest_list = []\n",
    "        for ingredient_tokens in row.token_NER:\n",
    "            NER_closest_list.append( get_closest_ingredient(ingredient_tokens, ingredients_df) )\n",
    "        NER_closest_ingredients.append(NER_closest_list)\n",
    "\n",
    "    print(NER_closest_ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    processed_ngl_df = tokenized_ngl_df.copy()\n",
    "    processed_ngl_df['closest_ingredients'] = NER_closest_ingredients\n",
    "    processed_ngl_df.to_csv('data/processed_ngl.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ngl_df = pd.read_csv('data/processed_ngl.csv', index_col=0,\n",
    "                               converters={'ingredients':pd.eval, 'directions':pd.eval, 'NER':pd.eval, \n",
    "                                           'token_title':pd.eval, 'token_ingredients':pd.eval, \n",
    "                                           'token_directions':pd.eval,' token_NER':pd.eval, 'closest_ingredients':pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_NER</th>\n",
       "      <th>closest_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[['brown', 'sugar'], ['milk'], ['vanilla'], ['...</td>\n",
       "      <td>[brown sugar, milk, vanilla, soy nut, butter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[['beef'], ['chicken', 'breasts'], ['cream', '...</td>\n",
       "      <td>[beef, chicken, passion fruit, sour cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[['frozen', 'corn'], ['cream', 'cheese'], ['bu...</td>\n",
       "      <td>[corn, cheese, butter, garlic, salt, pepper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[['chicken'], ['chicken', 'gravy'], ['cream', ...</td>\n",
       "      <td>[chicken, chicken, passion fruit, cheese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[['peanut', 'butter'], ['graham', 'cracker', '...</td>\n",
       "      <td>[peanut butter, graham cracker, butter, sugar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[['baking', 'potatoes'], ['extra', 'lean', 'gr...</td>\n",
       "      <td>[baking mix, crescent roll, butter, milk, salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[['sugar'], ['butter'], ['egg'], ['buttermilk'...</td>\n",
       "      <td>[sugar, butter, egg, buttermilk, flour, salt, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           token_NER  \\\n",
       "0  [['brown', 'sugar'], ['milk'], ['vanilla'], ['...   \n",
       "1  [['beef'], ['chicken', 'breasts'], ['cream', '...   \n",
       "2  [['frozen', 'corn'], ['cream', 'cheese'], ['bu...   \n",
       "3  [['chicken'], ['chicken', 'gravy'], ['cream', ...   \n",
       "4  [['peanut', 'butter'], ['graham', 'cracker', '...   \n",
       "5  [['baking', 'potatoes'], ['extra', 'lean', 'gr...   \n",
       "6  [['sugar'], ['butter'], ['egg'], ['buttermilk'...   \n",
       "\n",
       "                                 closest_ingredients  \n",
       "0  [brown sugar, milk, vanilla, soy nut, butter, ...  \n",
       "1         [beef, chicken, passion fruit, sour cream]  \n",
       "2       [corn, cheese, butter, garlic, salt, pepper]  \n",
       "3          [chicken, chicken, passion fruit, cheese]  \n",
       "4  [peanut butter, graham cracker, butter, sugar,...  \n",
       "5  [baking mix, crescent roll, butter, milk, salt...  \n",
       "6  [sugar, butter, egg, buttermilk, flour, salt, ...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ngl_df[[\"token_NER\", \"closest_ingredients\"]][:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lettuce', 'black pepper', 'grape', 'garlic', 'pepper', 'onion', 'seasoning', 'lentils', 'gorgonzola']\n"
     ]
    }
   ],
   "source": [
    "# outputs the ingredient to closest ingredient pairings for What's Cooking data\n",
    "if pre_processing == True:\n",
    "    wc_ingredients, wc_closest_ingredients = [], []\n",
    "    \n",
    "    limit = 7500\n",
    "    for i in range(len(wc_train_data)):\n",
    "        wc_ingredients.append(wc_train_data[i]['ingredients'])\n",
    "        \n",
    "        item_closest_ingredients = []\n",
    "        for ingredient in wc_train_data[i]['ingredients']:\n",
    "            token_list = re.sub(r\"[^a-zA-Z ]+\", '', ingredient.lower()).split(' ')\n",
    "            closest_ingredient = get_closest_ingredient(token_list, ingredients_df)\n",
    "            item_closest_ingredients.append(closest_ingredient)\n",
    "        wc_closest_ingredients.append(item_closest_ingredients)\n",
    "        \n",
    "        if i >= limit:\n",
    "            break\n",
    "\n",
    "    print(wc_closest_ingredients[0])\n",
    "    \n",
    "    wc_ingredients_df = pd.DataFrame({'ingredients': wc_ingredients, 'closest_ingredients': wc_closest_ingredients})\n",
    "    wc_ingredients_df.to_csv('data/whats_cooking/closest_ingredients.csv', header=True, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredient Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    # get frequency based on appearences in RecipeNGL recipes\n",
    "    ingredient_frequency = torch.zeros(len(ingredients_df)).tolist()\n",
    "\n",
    "    for i in range(processed_ngl_df.shape[0]):\n",
    "        for closest_ingredient in processed_ngl_df[\"closest_ingredients\"][i]:\n",
    "            index = ingredient_index_dict.get(closest_ingredient)\n",
    "            if (index != None):\n",
    "                ingredient_frequency[index] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    # get frequency based on appearences in What's Cooking recipes\n",
    "    wc_ingredients_df = pd.read_csv('data/whats_cooking/closest_ingredients.csv', index_col=0,\n",
    "                                    converters={'ingredients':pd.eval, 'closest_ingredients':pd.eval})\n",
    "    \n",
    "    for i in range(wc_ingredients_df.shape[0]):\n",
    "        for closest_ingredient in wc_ingredients_df[\"closest_ingredients\"][i]:\n",
    "            index = ingredient_index_dict.get(closest_ingredient)\n",
    "            if (index != None):\n",
    "                ingredient_frequency[index] += 1\n",
    "    \"\"\"\n",
    "    wc_limit = 5000\n",
    "    for i in range(len(wc_train_data)):\n",
    "        for ingredient in wc_train_data[i]['ingredients']:\n",
    "            token_list = re.sub(r\"[^a-zA-Z ]+\", '', ingredient.lower()).split(' ')\n",
    "            closest_ingredient = get_closest_ingredient(token_list, ingredients_df)\n",
    "            index = ingredient_index_dict.get(closest_ingredient)\n",
    "            if (index != None):\n",
    "                ingredient_frequency[index] += 1\n",
    "                \n",
    "        if i > wc_limit:\n",
    "            break\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    ingredient_frequency = ( torch.FloatTensor(ingredient_frequency) / max(ingredient_frequency) ).tolist()\n",
    "    ingredients_frequency_df = (ingredients_df.copy()).drop('embedding', axis=1)\n",
    "    ingredients_frequency_df['frequency'] = ingredient_frequency\n",
    "    ingredients_frequency_df.to_csv('data/ingredients_frequency.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_frequency_df = pd.read_csv('data/ingredients_frequency.csv', header=None, names=[\"ingredient\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredient</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abalone</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absinthe</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acai</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acorn</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adobo</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>agar</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aioli</td>\n",
       "      <td>0.001170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albacore</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ale</td>\n",
       "      <td>0.007959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ingredient  frequency\n",
       "0    abalone   0.000234\n",
       "1   absinthe   0.000000\n",
       "2       acai   0.000000\n",
       "3      acorn   0.000234\n",
       "4      adobo   0.000000\n",
       "5       agar   0.000000\n",
       "6      aioli   0.001170\n",
       "7   albacore   0.000000\n",
       "8    alcohol   0.000468\n",
       "9        ale   0.007959"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_frequency_df[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ingredient frequency, we further filter the ingredients with less than 0.1% frequency. This yields 381 ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n"
     ]
    }
   ],
   "source": [
    "if pre_processing == True:\n",
    "    filtered_ingredients_frequency_df = ingredients_frequency_df[ingredients_frequency_df['frequency'] > 0.001]\n",
    "    print(filtered_ingredients_frequency_df.shape[0])\n",
    "    filtered_ingredients_frequency_df.drop('frequency', axis=1).to_csv('data/frequency_filtered_ingredients.csv', header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredient Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the graph, each time an ingredient appears in a recipe with another ingredient, their compatibility is increased. This is implemented with an adjacency matrix. The compatibilities will be normalized by each row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_processing == True:\n",
    "    ingredient_graph = torch.zeros(len(ingredients_df), len(ingredients_df)).tolist()\n",
    "\n",
    "    for i in range(processed_ngl_df.shape[0]):\n",
    "        for ingredient_1 in processed_ngl_df[\"closest_ingredients\"][i]:\n",
    "            index_1 = ingredient_index_dict[ingredient_1]\n",
    "            for ingredient_2 in processed_ngl_df[\"closest_ingredients\"][i]:\n",
    "                index_2 = ingredient_index_dict[ingredient_2]\n",
    "                \n",
    "                ingredient_graph[index_1][index_2] += 1\n",
    "    \n",
    "    ingredient_graph = ( torch.FloatTensor(ingredient_graph) / max(ingredient_graph) ).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Primary Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredient Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Baseline Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: A generative RNN trained using character-tokenized recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRecipeGenerator(nn.Moduel):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers =1 ):\n",
    "          super(RNNRecipeGenerator, self).__init__()\n",
    "          self.ident = torch.eye(vocab_size)\n",
    "          self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first = True)\n",
    "          self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inp, hidden = None):\n",
    "          inp = self.ident[inp]\n",
    "          output, hidden = self.rnn(inp, hidden)\n",
    "          output = self.decoder(output)\n",
    "          return output, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe Generation: Using probability distribution to predict next character. GPU Enabled to speed up generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence_cuda(model, max_len=1000, temperature=0.8):\n",
    "    generated_sequence = \"\n",
    "    \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().cuda()\n",
    "    hidden = None\n",
    "\n",
    "    for c in range(max_len):\n",
    "          output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "          output_dist = output.data.view(-1).div(temperature).exp().cpu()\n",
    "          top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "\n",
    "          predicted_char = vocab_itos[top_i]\n",
    "\n",
    "          if predicted_char == \"<EOS>\":\n",
    "              break\n",
    "\n",
    "          generated_sequence += predicted char\n",
    "          inp = torch.Tensor([top_i]).long().cuda()\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: GPU Enabled to speed up training. A sample is printed every print_freq iterations to see the model's progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cuda(model, data, batch_size=1, num_epochs=1, lr=1e-3, print_freq=200):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    iter = 0\n",
    "    data_iter = torchtext.legacy.data.BucketIterator(data, batch_size=batch_size, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "    losses, epochs = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        avg_loss = 0\n",
    "        for (recipe, lengths), label in data_iter:\n",
    "            target = recipe[:, 1:].cuda()\n",
    "            inp = recipe[:, :-1].cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            iter += 1\n",
    "            losses.append(float(loss))\n",
    "            epochs.append(e)\n",
    "\n",
    "            if iter % print_freq == 0:\n",
    "                  print(\"Iteration # %d: Loss %f\" % (it+1, float(avg_loss/print_freq)))\n",
    "                  print(\"Generated Recipe: \" + sample_sequence_cuda(model, 140, 0.8))\n",
    "                  avg_loss = 0\n",
    "\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, losses, label = \"Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = RNNRecipeGenerator(vocab_size, 64)\n",
    "baseline_model = baseline_model.cuda()\n",
    "baseline_model.ident = baseline_model.ident.cuda()\n",
    "train_cuda(baseline_model, recipes, batch_size=32, num_epochs=1, lr=1e-3, print_every=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Results and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f6b67828bcf6e6b24b24226b27e29c318445726d2dd849532926bd41a61ff72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
